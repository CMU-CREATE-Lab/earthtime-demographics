{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#From https://github.com/gabrielo/Allegheny-County-Property-Assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, re, sqlite3, thread, urllib, urllib2\n",
    "def exec_ipynb(filename_or_url):\n",
    "    nb = (urllib2.urlopen(filename_or_url) if re.match(r'https?:', filename_or_url) else open(filename_or_url)).read()\n",
    "    jsonNb = json.loads(nb)\n",
    "    #check for the modified formatting of Jupyter Notebook v4\n",
    "    if(jsonNb['nbformat'] == 4):\n",
    "        exec '\\n'.join([''.join(cell['source']) for cell in jsonNb['cells'] if cell['cell_type'] == 'code']) in globals()\n",
    "    else:\n",
    "        exec '\\n'.join([''.join(cell['input']) for cell in jsonNb['worksheets'][0]['cells'] if cell['cell_type'] == 'code']) in globals()\n",
    "\n",
    "exec_ipynb('timelapse-utilities.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# assessments/cd/AC Property Assessments_10012017.xls (which is actually TDF) from Amy Gottsegen and Randy Sargent buying a CD from the county assessors office\n",
    "# assessments/Allegheny_County_Parcel_Boundaries.geojson from https://data.wprdc.org/dataset/allegheny-county-parcel-boundaries\n",
    "import array, csv, datetime, json, math, numpy, os, random, re \n",
    "from dateutil.parser import parse\n",
    "from shapely.geometry import mapping, shape\n",
    "from PIL import Image\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas,numbers\n",
    "from operator import itemgetter, attrgetter\n",
    "import string\n",
    "import calendar,time\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# Use geopandas.  Boilerplate from https://docs.google.com/document/d/1utZuLHcKQEZNXTQLOysTNCxTHrqxczAUymmtplpn27Q/edit#heading=h.f50xoxwmcir\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoSeries, GeoDataFrame\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.max_colwidth = 300\n",
    "pd.options.display.max_rows = 100\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "# Built-in time handling fails for times before 1900.  Use arrow instead.  \n",
    "# See https://arrow.readthedocs.io/en/latest/ for info\n",
    "import arrow\n",
    "\n",
    "def LonLatToPixelXY(lonlat):\n",
    "    (lon, lat) = lonlat\n",
    "    x = (lon + 180.0) * 256.0 / 360.0\n",
    "    y = 128.0 - math.log(math.tan((lat + 90.0) * math.pi / 360.0)) * 128.0 / math.pi\n",
    "    return [x, y]\n",
    "\n",
    "# This does the same as above, but takes a GeoJSON point (which is what centroid returns)\n",
    "def PointToPixelXY(point):\n",
    "    lon=point.x\n",
    "    lat=point.y\n",
    "    x = (lon + 180.0) * 256.0 / 360.0\n",
    "    y = 128.0 - math.log(math.tan((lat + 90.0) * math.pi / 360.0)) * 128.0 / math.pi\n",
    "    return [x, y]\n",
    "\n",
    "def GetCentroid(geometry):\n",
    "    s = shape(geometry)\n",
    "    return (s.centroid.x, s.centroid.y)   \n",
    "\n",
    "def GetEpoch(date):\n",
    "    return (date - datetime.datetime(1970, 1, 1)).total_seconds()\n",
    "\n",
    "def HexToRgb(hex_string):\n",
    "    rgb = colors.hex2color(hex_string)\n",
    "    r,g,b = tuple([int(255*x) for x in rgb])\n",
    "    return (r,g,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "import collections\n",
    "\n",
    "\n",
    "# Google geocoding API to geocode an address\n",
    "# Assumes key is in google_api_key.txt\n",
    "# Use as second level lookup only for those which fail census lookup\n",
    "\n",
    "re_noalnum = re.compile('^([^\\w]+)$')\n",
    "\n",
    "def geocode_address_google(address):\n",
    "    if(pandas.isnull(address) or re_noalnum.match(address)):\n",
    "        #print \"Skipping address lookup for %r, no alphanumeric characters\" %(address)\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        geocode_address_google.conn\n",
    "    except:\n",
    "        geocode_address_google.conn = sqlite3.connect('geocoding_cache')\n",
    "        geocode_address_google.cur = geocode_address_google.conn.cursor()\n",
    "        geocode_address_google.cur.execute(('CREATE TABLE IF NOT EXISTS kvs'\n",
    "                                     ' (key PRIMARY KEY, value)'\n",
    "                                     ' WITHOUT ROWID;'))\n",
    "        geocode_address_google.conn.commit()\n",
    "\n",
    "    geocode_address_google.cur.execute('SELECT value FROM kvs WHERE key=?', (address,))\n",
    "    rows = geocode_address_google.cur.fetchall()\n",
    "    if rows:\n",
    "        #print \"Google: Found %r in cache\" % (address)\n",
    "        return json.loads(rows[0][0])\n",
    "        \n",
    "    #print \"Google: %r not found in cache, fetching\" % (address)\n",
    "\n",
    "    api_key = open('google-api-key-do-not-commit.txt').read()\n",
    "    payload = {'address':address, 'key':api_key}\n",
    "    result = json.load(urllib2.urlopen('https://maps.googleapis.com/maps/api/geocode/json?%s' % urllib.urlencode(payload)))\n",
    "    \n",
    "    geocode_address_google.cur.execute(('INSERT OR REPLACE INTO kvs (key, value)'\n",
    "                                 ' VALUES (?, ?);'),\n",
    "                                 (address, json.dumps(result)))\n",
    "    geocode_address_google.conn.commit()\n",
    "    \n",
    "    # If the result succeeded and generated a formatted address, cache under that key too\n",
    "    try:\n",
    "        if(result['status']=='OK'):\n",
    "            if(len(result['results'])== 1):\n",
    "                canonical = result['results'][0]['formatted_address']\n",
    "                #print \"Google: Storing canonical addr of %r for %r\" % (canonical, address)\n",
    "                geocode_address_google.cur.execute(('INSERT OR REPLACE INTO kvs (key, value)'\n",
    "                                                     ' VALUES (?, ?);'),\n",
    "                                                     (canonical, json.dumps(result)))\n",
    "                geocode_address_google.conn.commit()\n",
    "            else:\n",
    "                print \"Google: Fetch %r seemed to succeed, but results empty or non-unique\" % (address)\n",
    "\n",
    "        else:\n",
    "            print \"Google: Fetch %r failed\" % (address)\n",
    "\n",
    "    except:\n",
    "        #pass\n",
    "        raise\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_canonical_address_google(raw_addr):\n",
    "    if(pandas.isnull(raw_addr) or re_noalnum.match(raw_addr)):\n",
    "        #print \"Skipping address lookup for %r, no alphanumeric characters\" %(raw_addr)\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        result=geocode_address_google(raw_addr)\n",
    "        if(result['status']=='OK' and len(result['results'])== 1):\n",
    "            return(result['results'][0]['formatted_address'])\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_canonical_zip_google(addr):\n",
    "    if(pandas.isnull(addr) or re_noalnum.match(addr)):\n",
    "        #print \"Skipping address lookup for %r, no alphanumeric characters\" %(addr)\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        result=geocode_address_google(addr)\n",
    "        if(result['status']=='OK' and len(result['results'])== 1):\n",
    "            addr_comps = result['results'][0]['address_components']\n",
    "            for i in range(0, len(addr_comps)):\n",
    "                if('postal_code' in addr_comps[i]['types']):\n",
    "                    return addr_comps[i]['short_name']\n",
    "        \n",
    "        return None\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def get_canonical_coords_google(addr):\n",
    "    if(pandas.isnull(addr) or re_noalnum.match(addr)):\n",
    "        #print \"Skipping address lookup for %r, no alphanumeric characters\" %(addr)\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        result=geocode_address_google(addr)\n",
    "        if(result['status']=='OK' and len(result['results'])== 1):\n",
    "            latlon = result['results'][0]['geometry']['location']\n",
    "            if(latlon):\n",
    "                return(Point(latlon['lng'], latlon['lat']))\n",
    "        return None\n",
    "    except:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses Census geocoding API to geocode an address\n",
    "# Assumes key is in google_api_key.txt\n",
    "\n",
    "import sqlite3\n",
    "import urllib,urllib2\n",
    "import json\n",
    "from shapely.geometry import Point\n",
    "import collections\n",
    "\n",
    "# Match if the string contains no alphanumerics\n",
    "re_noalnum = re.compile('^([^\\w]+)$')\n",
    "\n",
    "def geocode_address_census(address):\n",
    "    if(pandas.isnull(address) or re_noalnum.match(address)):\n",
    "        #print \"Skipping address lookup for %r, no alphanumeric characters\" %(address)\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        geocode_address_census.sqlite\n",
    "    except:\n",
    "        geocode_address_census.sqlite = {}\n",
    "    \n",
    "    tpid = '%d.%s' % (os.getpid(), thread.get_ident())\n",
    "    if not tpid in geocode_address_census.sqlite:\n",
    "        conn = sqlite3.connect('geocoding_cache_census')\n",
    "        cur = conn.cursor()\n",
    "        geocode_address_census.sqlite[tpid] = { 'conn': conn, 'cur': cur }\n",
    "\n",
    "        cur.execute(('CREATE TABLE IF NOT EXISTS kvs'\n",
    "                     ' (key PRIMARY KEY, value)'\n",
    "                     ' WITHOUT ROWID;'))\n",
    "        conn.commit()\n",
    "    else:\n",
    "        conn = geocode_address_census.sqlite[tpid]['conn']\n",
    "        cur = geocode_address_census.sqlite[tpid]['cur']\n",
    "\n",
    "    cur.execute('SELECT value FROM kvs WHERE key=?', (address,))\n",
    "    rows = cur.fetchall()\n",
    "    if rows:\n",
    "        return json.loads(rows[0][0])\n",
    "    \n",
    "    payload = {'address':address, 'benchmark':4, 'format':'json'}\n",
    "    result = json.load(urllib2.urlopen('https://geocoding.geo.census.gov/geocoder/locations/onelineaddress?%s' % urllib.urlencode(payload)))\n",
    "    \n",
    "    cur.execute(('INSERT OR REPLACE INTO kvs (key, value)'\n",
    "                 ' VALUES (?, ?);'),\n",
    "                (address, json.dumps(result)))\n",
    "    conn.commit()\n",
    "    \n",
    "    if len(result['result']['addressMatches']) == 1:\n",
    "        canonical = result['result']['addressMatches'][0]['matchedAddress']\n",
    "        cur.execute(('INSERT OR REPLACE INTO kvs (key, value)'\n",
    "                     ' VALUES (?, ?);'),\n",
    "                     (canonical, json.dumps(result)))\n",
    "        conn.commit()\n",
    "    \n",
    "    return result\n",
    "\n",
    "def cache_canonical_addresses(addrs):\n",
    "    t = SimpleThreadPoolExecutor(25)\n",
    "    for addr in addrs:\n",
    "        if(not (pandas.isnull(addr) or re_noalnum.match(addr))):\n",
    "            t.submit(geocode_address_census, addr)\n",
    "        else:\n",
    "            #print \"Skipping address lookup for %r, no alphanumeric characters\" %(addr)\n",
    "            pass\n",
    "    t.shutdown()\n",
    "\n",
    "def get_canonical_address_census(raw_addr):\n",
    "    if(pandas.isnull(raw_addr) or re_noalnum.match(raw_addr)):\n",
    "        #print \"Skipping address lookup for %r, no alphanumeric characters\" %(raw_addr)\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        result=geocode_address_census(raw_addr)\n",
    "        if len(result['result']['addressMatches']) == 1:\n",
    "            return(result['result']['addressMatches'][0]['matchedAddress'])\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_canonical_zip_census(addr):\n",
    "    try:\n",
    "        result=geocode_address_census(addr)\n",
    "        if len(result['result']['addressMatches']) == 1:\n",
    "            return(result['result']['addressMatches'][0]['addressComponents']['zip'])\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    \n",
    "def get_canonical_coords_census(addr):\n",
    "    try:\n",
    "        result=geocode_address_census(addr)\n",
    "        if len(result['result']['addressMatches']) == 1:\n",
    "            coords = result['result']['addressMatches'][0]['coordinates']\n",
    "            return(Point(coords['x'], coords['y']))\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "################################################################################################################\n",
    "# These can handle being passed a map with either census or google in it and dispatch to the right service\n",
    "\n",
    "# TODO: Think if this is the right behavior: If handed a regular string do census lookup\n",
    "# If handed a map, return census if non-null, then google if non-null\n",
    "def get_canonical_address(addr):\n",
    "    if(pandas.isnull(addr) or ((not isinstance(addr, collections.Mapping) and re_noalnum.match(addr)))):\n",
    "        #print \"Skipping address lookup for %r, no alphanumeric characters\" %(addr)\n",
    "        return None\n",
    "\n",
    "    if(isinstance(addr, collections.Mapping)):\n",
    "        if('census' in addr and not pd.isnull(addr['census'])):\n",
    "            return addr['census']\n",
    "        elif('google' in addr and not pd.isnull(addr['google'])):\n",
    "            return addr['google']\n",
    "    else:\n",
    "        return(get_canonical_address_census(addr))\n",
    "\n",
    "def get_canonical_zip(addr):\n",
    "    if(pandas.isnull(addr) or ((not isinstance(addr, collections.Mapping) and re_noalnum.match(addr)))):\n",
    "        #print \"Skipping address lookup for %r, no alphanumeric characters\" %(addr)\n",
    "        return None\n",
    "    if(isinstance(addr, collections.Mapping)):\n",
    "        if('census' in addr and not pd.isnull(addr['census'])):\n",
    "            return get_canonical_zip_census(addr['census'])\n",
    "        elif('google' in addr and not pd.isnull(addr['google'])):\n",
    "            return get_canonical_zip_google(addr['google'])\n",
    "    else:\n",
    "        return(get_canonical_zip_census(addr))\n",
    "\n",
    "def get_canonical_coords(addr):\n",
    "    if(pandas.isnull(addr) or ((not isinstance(addr, collections.Mapping) and re_noalnum.match(addr)))):\n",
    "        #print \"Skipping address lookup for %r, no alphanumeric characters\" %(addr)\n",
    "        return None\n",
    "    \n",
    "    if(isinstance(addr, collections.Mapping)):\n",
    "        if('census' in addr and not pd.isnull(addr['census'])):\n",
    "            return get_canonical_coords_census(addr['census'])\n",
    "        elif('google' in addr and not pd.isnull(addr['google'])):\n",
    "            return get_canonical_coords_google(addr['google'])\n",
    "    else:\n",
    "        return(get_canonical_coords_census(addr))\n",
    "\n",
    "def canonicalize_addr_colset(df, col_arr, out_colname):\n",
    "    for  col_name in col_arr:\n",
    "        df[col_name]=df[col_name].astype(basestring)\n",
    "        df[col_name].fillna('', inplace=True)\n",
    "\n",
    "    df[out_colname] = df[col_arr].apply(lambda x: get_canonical_address_census(' '.join(x)), axis=1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_canonical_zip({'google':'4625 CORDAY WAY PITTSBURGH PA 15224'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_canonical_zip('4625 CORDAY WAY PITTSBURGH PA 15224')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create structures to populate with sale and ownership information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read in 2017 data to harvest list of residential parcel IDs\n",
    "# We need to do this before loading the older ADB files because they're missing some of the columns we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each residential PARID, we want to generate a sorted list of who bought it when.\n",
    "# For each owner name, we want to generate a sorted list of PARIDs they bought and when they bought it.\n",
    "# For each change address, we want to generate a sorted list of PARIDs they bought and when they bought it.\n",
    "# Each of the following is a map indexed on PARID, owner name, or change address.  \n",
    "\n",
    "# The map indexed on PARID builds up a history of a given property.\n",
    "#   Each record contains a a list of maps containing a list of date, event_type, owner name, and change address\n",
    "#   event_type is either PURCHASE or FORECLOSURE\n",
    "\n",
    "# Each map indexed on owner name or change address builds a history of a given owner/change address.\n",
    "#   Each record contains a list of maps containing date, event_type, and PARID\n",
    "#   event_type is either PURCHASE or SALE\n",
    "\n",
    "\n",
    "# To start with, clear the maps\n",
    "property_map = {}\n",
    "owner_map = {}\n",
    "changeaddr_map = {}\n",
    "\n",
    "# Also create a map to cache lookups for owner and changeaddress PARID sets at various times\n",
    "# to speed up runtime\n",
    "owned_parids_cache={}\n",
    "\n",
    "# TODO: Create a map of owner and changeaddress to inferred owner type\n",
    "# 'GOVERNMENT'\n",
    "# 'INVESTOR' \n",
    "\n",
    "# Create a global map of PARIDs to canonical property addresses.  Not all versions of the db have valid \n",
    "# property addresses, but at least one hopefully should!\n",
    "canonical_property_address_map={}\n",
    "\n",
    "# Create a global map of PARIDs to raw property addresses.  We might have conflicts, but at least we'll have something\n",
    "# to fall back on if canonical resolution fails\n",
    "raw_property_address_map={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create functions to update the above maps with info from a given row of an assessment spreadsheet/database.  \n",
    "# The first argument should be a pandas table, the second an index into the table.\n",
    "# We expect the index to be PARID\n",
    "# We expect the following columns to exist and be valid:\n",
    "#   PROPERTYOWNER\n",
    "#   CHANGENOTICEADDRESS1 - CHANGENOTICEADDRESS4\n",
    "#   SALEDATE\n",
    "\n",
    "\n",
    "# In 2017 the date format is MM-DD-YYYY\n",
    "# In 2009, an example is 09/16/96 00:00:00\n",
    "\n",
    "# Setup regular expression for parsing various sorts of dates\n",
    "re_MDY = re.compile('(\\d\\d)-(\\d\\d)-(\\d\\d\\d\\d)')\n",
    "re_YMD = re.compile('(\\d\\d\\d\\d)-(\\d\\d)-(\\d\\d)')\n",
    "re_MDY_HMS = re.compile('(\\d+)/(\\d+)/(\\d+) (\\d+):(\\d+):(\\d+)')\n",
    "\n",
    "# Setup this year to use for dealing with 2-digit dates\n",
    "this_year = datetime.datetime.now().year\n",
    "this_date = arrow.now().format('YYYY-MM-DD')\n",
    "\n",
    "# Utility function to normalize SALEDATE format\n",
    "# SALEDATE field is in '%m-%d-%Y' format (ex 10-26-2012) when present\n",
    "def SaledateToEpoch(datestr):\n",
    "    if(re_YMD.match(datestr)!=None):\n",
    "        return calendar.timegm(time.strptime(datestr, '%Y-%m-%d'))\n",
    "    elif(re_MDY.match(datestr)!=None):\n",
    "        return calendar.timegm(time.strptime(datestr, '%m-%d-%Y'))\n",
    "\n",
    "# Built-in time handling fails for times before 1900.  Use arrow instead.  \n",
    "# See https://arrow.readthedocs.io/en/latest/ for info\n",
    "def SaledateToYMD(datestr):\n",
    "    # Check which format and parse accordingly\n",
    "    if(re_YMD.match(datestr)!=None):\n",
    "        # Already like we want it\n",
    "        return datestr\n",
    "    if(re_MDY.match(datestr)!=None):\n",
    "        return arrow.get(datestr, 'MM-DD-YYYY').format('YYYY-MM-DD')\n",
    "    elif(re_MDY_HMS.match(datestr)!=None):\n",
    "        # Some of the 2-digit years are farther back than python's default pivot year of 1969\n",
    "        # If the year we parse is > this_year, set it back by 100 years\n",
    "        ad=arrow.get(datestr, 'MM/DD/YY HH:mm:ss')\n",
    "        if(ad.year>this_year):\n",
    "            ad = ad.replace(year=(ad.year-100))\n",
    "        return ad.format('YYYY-MM-DD')\n",
    "    else:\n",
    "        raise Exception('Unrecognized saledate format %r' % (datestr))\n",
    "\n",
    "def merge_dwelling_col(df):\n",
    "    r_df = pd.merge(df,dwelling_col.to_frame(),on='PARID', left_on=None, right_on=None,\n",
    "                    left_index=False, right_index=False, sort=False,\n",
    "                    suffixes=('_x', '_y'), copy=True, indicator=False,\n",
    "                    validate=None)\n",
    "    print \"After merge, size = %d\" % (len(r_df))\n",
    "\n",
    "    r_df = r_df[r_df.is_dwelling]\n",
    "    print \"After filtering to only include dwellings, size = %d\" % (len(r_df))\n",
    "    return(r_df)\n",
    "\n",
    "# find_event_by_date takes a list of maps containg 'date' fields and looks for one the the last one that's before\n",
    "# search_date or the first which is simultaneous (in case of multiple simultaneous records).  \n",
    "# Returns -1 if the search_date is earlier than any existing item.  It assumes that each map in the event list \n",
    "# contains a 'date' field and that they're in sorted order from earliest to latest.\n",
    "def find_event_index_by_date(event_list, search_date):\n",
    "    match_index=-1\n",
    "    for i in range(0,len(event_list)):\n",
    "        if(event_list[i]['date']<search_date):\n",
    "            # This happened before the date we're looking for, update match_index\n",
    "            match_index=i\n",
    "        elif(event_list[i]['date']==search_date):\n",
    "            # This is the first exact match, return i\n",
    "            return(i)\n",
    "        else:\n",
    "            # This happened after the date we're looking for, return match_index\n",
    "            return match_index\n",
    "    # If we get to here, there is no event after this date.  Return match_index\n",
    "    return match_index\n",
    "\n",
    "def find_event_index_by_date_and_asofdate(event_list, search_date, asofdate):\n",
    "    match_index=-1\n",
    "    for i in range(0,len(event_list)):\n",
    "        if(event_list[i]['date']<search_date):\n",
    "            # This happened before the date we're looking for, update match_index\n",
    "            match_index=i\n",
    "        elif(event_list[i]['date']==search_date):\n",
    "            if(event_list[i]['asofdate']==asofdate):\n",
    "                # This is an exact match, return i\n",
    "                return(i)\n",
    "            elif(event_list[i]['asofdate']<asofdate):\n",
    "                # This happened on the same search_date and before the asoifdate \n",
    "                # we're looking for, update match_index\n",
    "                match_index=i\n",
    "\n",
    "        else:\n",
    "            # This happened after the date we're looking for, return match_index\n",
    "            return match_index\n",
    "    # If we get to here, there is no event after this date.  Return match_index\n",
    "    return match_index\n",
    "\n",
    "# This takes an existing event list and either inserts a given event_map and returns True, or \n",
    "# decides that the event is a duplicate and returns false\n",
    "def insert_event(event_list, event_map):\n",
    "    # Find the index of the element already in the list which precedes the date in event_map.  \n",
    "    # If not found, match_index is -1, so we should insert it at the beginning of event_list and return True\n",
    "    # If found, check if the record matches what we already have (TODO: see if we need something\n",
    "    # more sophisticated than ==).  If it matches what we already have, return False and do not modify the list.\n",
    "    # If it doesn't match what we have insert it.  Note that list.insert takes the arg of the element to insert\n",
    "    # before, so we use match_index+1\n",
    "    match_index = find_event_index_by_date(event_list, event_map['date'])\n",
    "    if(match_index==-1):\n",
    "        #print \"No match, insert at beginning\"\n",
    "        # No match.  Insert at the beginning, return True\n",
    "        event_list.insert(0, event_map)\n",
    "        return True\n",
    "    # We've got a matching or preceeding event, compare it with this and any other potentially \n",
    "    # simultaneous events\n",
    "    #print \"Time match at %d\" % (match_index)\n",
    "    for i in range(match_index, len(event_list)):\n",
    "        if(event_map == event_list[i]):\n",
    "            # This is a duplicate, ignore it\n",
    "            #print \"Found duplicate match at %d (%r)\" % (i,event_list[i])\n",
    "            return False\n",
    "        elif(event_map['date'] == event_list[i]['date'] and 'asofdate' in event_map and 'asofdate' in event_list[i] and event_map['asofdate'] == event_list[i]['asofdate']):\n",
    "            # This is an event from the same sale date and asofdate, but it doesn't match exactly.\n",
    "            # Have the new event supercede the old one.  Return true.\n",
    "            #print \"Found superceded version at %d (%r -> %r)\" % (i,event_list[i], event_map)\n",
    "            event_list[i]=event_map\n",
    "            return True\n",
    "        elif(event_map['date']<event_list[i]['date'] or ((event_map['date'] == event_list[i]['date'] and 'asofdate' in event_map and 'asofdate' in event_list[i] and event_map['asofdate'] < event_list[i]['asofdate']))):\n",
    "            # We're past any simultaneous events that might have matched, continue\n",
    "            # Note that this version only works if the asofdates within a given sale date \n",
    "            # are always kept sorted\n",
    "            #print \"Hit a later item at %d (%r)\" % (i,match_index)\n",
    "            break\n",
    "        #print \"Didn't match at %d (%r), keep trying\" % (i,event_list[i])\n",
    "        match_index=i\n",
    "    # We've got a preceeding or simultaneous event that doesnt match.  Insert this \n",
    "    # new item just past match_index\n",
    "    #print \"Inserting after %d\" % (match_index)\n",
    "    event_list.insert(match_index+1, event_map)\n",
    "    return True\n",
    "\n",
    "# This takes an existing event list and either deletes a given event_map and returns True, or \n",
    "# decides that the event is not there and returns False\n",
    "def delete_event(event_list, event_map):\n",
    "    # Find the index of the element already in the list which matches or precedes the date in event_map.  \n",
    "    # If not found, match_index is -1, so we should return False\n",
    "    # If found, check if the record matches what we already have.  If it matches, delete and return True\n",
    "    match_index = find_event_index_by_date(event_list, event_map['date'])\n",
    "    if(match_index==-1 or event_map['date']>event_list[match_index]['date']):\n",
    "        #print \"No match\"        \n",
    "        return False\n",
    "    # We've got a matching date for this event, compare it with this and any other potentially \n",
    "    # simultaneous events\n",
    "    #print \"Time match at %d\" % (match_index)\n",
    "    for i in range(match_index, len(event_list)):\n",
    "        if(event_map == event_list[i]):\n",
    "            #print \"Found match at %d (%r), deleting\" % (i,event_list[i])\n",
    "            del event_list[i]\n",
    "            return True\n",
    "        elif(event_map['date'] == event_list[i]['date'] and 'asofdate' in event_map and 'asofdate' in event_list[i] and event_map['asofdate'] == event_list[i]['asofdate']):\n",
    "            # This is an event from the same sale date and asofdate, delete and return true.\n",
    "            #print \"Found matching date and asofdate at %d, deleting\" % (i)\n",
    "            del event_list[i]\n",
    "            return True\n",
    "        elif(event_map['date']<event_list[i]['date']):\n",
    "            # We're past any simultaneous events that might have matched, break and return False\n",
    "            break\n",
    "    #print \"No match\"\n",
    "    return True\n",
    "\n",
    "# Create 'property_address_raw' and 'owner_address_raw' columns.  Call once per dataframe before calling\n",
    "# canonicalize_addrs\n",
    "def merge_addr_colset(df, col_arr, out_colname):\n",
    "    for  col_name in col_arr:\n",
    "        df[col_name]=df[col_name].astype(basestring)\n",
    "        df[col_name].fillna('', inplace=True)\n",
    "\n",
    "    df[out_colname] = df[col_arr].apply(lambda x: ' '.join(x), axis=1)\n",
    "    # Fix up PGH to be PITTSBURGH\n",
    "    df[out_colname] = df[out_colname].str.replace('\\s*PGH\\s*,', ' PITTSBURGH,', regex=True)\n",
    "    # Fix up to make sure there aren't differences in whitespace that could be avoided\n",
    "    df[out_colname] = df[out_colname].str.replace('\\s\\s+', ' ', regex=True)\n",
    "    \n",
    "def merge_addrs(df):\n",
    "    property_addr_cols=['PROPERTYHOUSENUM', 'PROPERTYADDRESS', \n",
    "                        'PROPERTYCITY' ,'PROPERTYSTATE', 'PROPERTYZIP']\n",
    "    owner_addr_cols=['CHANGENOTICEADDRESS1', 'CHANGENOTICEADDRESS2',\n",
    "                     'CHANGENOTICEADDRESS3', 'CHANGENOTICEADDRESS4']\n",
    "    merge_addr_colset(df, property_addr_cols, 'property_address_raw')\n",
    "    merge_addr_colset(df, owner_addr_cols, 'owner_address_raw')\n",
    "\n",
    "# This assumes that merge_addrs has already been called to create the columns 'property_address_raw' and \n",
    "# 'owner_address_raw'\n",
    "def canonicalize_cache_addr_col(df, colname):\n",
    "    chunk_size=100\n",
    "    total_len = len(df)\n",
    "    next_i=0\n",
    "    chunk_start_time=arrow.now()\n",
    "    print_subset=10\n",
    "    print_count=0\n",
    "    retry_num=0\n",
    "    while(next_i<total_len):\n",
    "        u_addrs = df[colname][next_i:next_i+chunk_size].unique()\n",
    "        if((print_count % print_subset) == 0):\n",
    "            print \"Processing %s for rows %d-%d, %d unique addresses (p=%d)\" % (colname, next_i,next_i+chunk_size-1, len(u_addrs),print_count)\n",
    "        try:\n",
    "            cache_canonical_addresses(u_addrs)\n",
    "            if((print_count % print_subset) == 0):\n",
    "                print \"   %s time elapsed\" % (arrow.now()-chunk_start_time)\n",
    "            next_i=next_i+chunk_size\n",
    "            print_count=print_count+1\n",
    "            chunk_start_time=arrow.now()\n",
    "            retry_num = 0\n",
    "        except:\n",
    "            print \"Error while processing %s for rows %d-%d, %d unique addresses (p=%d), retry %d: %s\" % (colname, next_i,next_i+chunk_size-1, len(u_addrs),print_count, retry_num, sys.exc_info()[0])\n",
    "            if(retry_num>=5):\n",
    "                # Give up on that block\n",
    "                print \"   Giving up on that block\"\n",
    "                next_i=next_i+chunk_size\n",
    "                print_count=print_count+1\n",
    "                chunk_start_time=arrow.now()\n",
    "            else:\n",
    "                # Try again\n",
    "                retry_num = retry_num+1\n",
    "                \n",
    "  \n",
    "def canonicalize_addrs(df):\n",
    "    if(not 'property_address_raw' in list(df) or not 'owner_address_raw' in list(df)):\n",
    "        print \"ERROR: No 'property_address_raw' or 'owner_address_raw' column, make sure merge_addrs has been run on this dataframe\"\n",
    "        raise\n",
    "        \n",
    "    # First make sure all the addresses are in the cache\n",
    "    canonicalize_cache_addr_col(df, 'property_address_raw')\n",
    "    canonicalize_cache_addr_col(df, 'owner_address_raw')\n",
    "    print \"* Address canonicalization cacheing complete.  Harvesting property addresses\"\n",
    "    # Harvest results.  They should all be in the cache\n",
    "    df['property_address'] = df['property_address_raw'].apply(get_canonical_address_census)\n",
    "    print \"* Harvesting owner addresses\"\n",
    "    df['owner_address'] = df['owner_address_raw'].apply(get_canonical_address_census)\n",
    "    print \"* Done\"\n",
    "\n",
    "# Clean up multiple whitespace to a single space\n",
    "def cleanup_str(str_in):\n",
    "    str_out = re.sub( '\\s+', ' ', str_in).strip()\n",
    "    return(str_out)\n",
    "\n",
    "# Try to normalize changeaddr to be more likely to match\n",
    "re_zp4 = re.compile('(.+) (\\d\\d\\d\\d\\d)-(\\d\\d\\d\\d)?')\n",
    "re_cscz = re.compile('(.+) (\\w\\w), (\\d\\d\\d\\d\\d)')\n",
    "# Check for change address with no alphanumerics in it\n",
    "re_noalnum = re.compile('^([^\\w]+)$')\n",
    "\n",
    "# The raw_owner_changeaddr is the one that should be used for detecting multiple ownership since it\n",
    "# preserves unit number.  The canonical version loses it.\n",
    "def get_raw_owner_changeaddr(apd, i):\n",
    "    try:\n",
    "        return(apd['owner_address_raw'].iloc[i])\n",
    "    except:\n",
    "        print \"ERROR: No 'owner_address_raw' column, make sure merge_addrs has been run on this dataframe\"\n",
    "        raise\n",
    "\n",
    "def get_canonical_owner_changeaddr(apd, i):\n",
    "    try:\n",
    "        return(apd['owner_address'].iloc[i])\n",
    "    except:\n",
    "        print \"ERROR: No 'owner_address' column, make sure canonicalize_addrs has been run on this dataframe\"\n",
    "        raise\n",
    "\n",
    "def update_property_address_info(parid, raw_addr, property_address):\n",
    "    global canonical_property_address_map\n",
    "    global raw_property_address_map\n",
    "\n",
    "    # Check if the canonical property address is cached\n",
    "    if(parid in canonical_property_address_map and canonical_property_address_map[parid]):\n",
    "        return False\n",
    "    # Not cached, see if we should add it\n",
    "    if(property_address):\n",
    "        # If this is a valid property address, cache it for next time\n",
    "        canonical_property_address_map[parid]={'census':property_address}\n",
    "        # Also cache the raw address\n",
    "        raw_property_address_map[parid]=raw_addr\n",
    "        #print \"Cacheing raw address for %r: %r\" % (parid, raw_addr)\n",
    "    else:\n",
    "        # Don't have a valid canonical address, only cache the raw address if we don't have \n",
    "        # anything in there yet.  \n",
    "        if(raw_addr and parid not in raw_property_address_map):\n",
    "            raw_property_address_map[parid]=raw_addr\n",
    "            #print \"Cacheing raw address for %r (can't canonicalize): %r\" % (parid, raw_addr)\n",
    "    return True\n",
    "\n",
    "def update_property_address(apd, i):\n",
    "    parid=apd.index[i]\n",
    "    # Check if the canonical property address is cached\n",
    "    if(parid in canonical_property_address_map and canonical_property_address_map[parid]):\n",
    "        return canonical_property_address_map[parid]\n",
    "    # Not cached, try to retrieve from data frame\n",
    "    try:\n",
    "        ret=apd['property_address'].iloc[i]\n",
    "        raw_addr=apd['property_address_raw'].iloc[i]\n",
    "        if(ret):\n",
    "            # If this is a valid property address, cache it for next time\n",
    "            canonical_property_address_map[parid]=ret\n",
    "            # Also cache the raw address\n",
    "            if(raw_addr):\n",
    "                raw_property_address_map[parid]=raw_addr\n",
    "                #print \"Cacheing raw address for %r: %r\" % (parid, raw_addr)\n",
    "        else:\n",
    "            # Don't have a valid canonical address, only cache the raw address if we don't have \n",
    "            # anything in there yet.  Return None\n",
    "            if(raw_addr and parid not in raw_property_address_map):\n",
    "                raw_property_address_map[parid]=raw_addr\n",
    "                #print \"Cacheing raw address for %r (can't canonicalize): %r\" % (parid, raw_addr)\n",
    "            return None\n",
    "    except:\n",
    "        #print \"ERROR: No 'property_address' column, make sure canonicalize_addrs has been run on this dataframe\"\n",
    "        #raise\n",
    "        # It's ok to not have a property_address, as some of the old versions are hopelessly messed up for that\n",
    "        pass\n",
    "\n",
    "def get_canonical_property_addr(parid):\n",
    "    # Check if the canonical property address is cached\n",
    "    if(parid in canonical_property_address_map.keys()):\n",
    "        return canonical_property_address_map[parid]\n",
    "    # Nope.  Return None\n",
    "    return None\n",
    "\n",
    "# First arg is a pandas table, second is an index for what to process.\n",
    "# This updates property_map, owner_map, and changeaddr_map appropriately\n",
    "# Returns True on success, False on failure\n",
    "def process_assessment_record(apd, i, asofdate):\n",
    "    global canonical_property_address_map\n",
    "    global raw_property_address_map\n",
    "    global property_map\n",
    "    global owner_map\n",
    "    global changeaddr_map\n",
    "\n",
    "    # Generate set of interesting colnames we have in this data frame\n",
    "    interesting_colnames=set(['SALEDESC','SALEPRICE','HOMESTEADFLAG','HOMESTEADFLAG',\n",
    "                              'USEDESC','FAIRMARKETTOTAL','FAIRMARKETTOTAL',\n",
    "                              'is_vacant','property_address_raw','owner_address_raw',\n",
    "                              'property_address','owner_address'])\n",
    "    colnames = set(apd.columns) & interesting_colnames\n",
    "    #print \"This dataframe has interesting columns %r\" % (colnames)\n",
    "    \n",
    "    # Extract PARID, saledate, owner_name, and changeaddr from the record\n",
    "    par_id = apd.index[i]\n",
    "    \n",
    "    # Process saledate\n",
    "    saledate_raw = apd['SALEDATE'].iloc[i]\n",
    "    # If saledate is valid, keep it.  Otherwise return False\n",
    "    if(saledate_raw == '' or (isinstance(saledate_raw, numbers.Number) and math.isnan(saledate_raw))):\n",
    "        #print \"%s: Missing saledate\" % (par_id)\n",
    "        return False\n",
    "    \n",
    "    # Convert saledatet YMD so it sorts properly \n",
    "    saledate = SaledateToYMD(saledate_raw)\n",
    "    \n",
    "    owner_name = cleanup_str(apd['PROPERTYOWNER'].iloc[i])\n",
    "    # Use the raw changeaddr for computing multiple ownership since it preserves\n",
    "    # unit number.  Canonical doesn't \n",
    "    owner_changeaddr = get_raw_owner_changeaddr(apd,i)\n",
    "        \n",
    "    # If owner_changaddr is None or doesn't contain alphanumeric characters, skip \n",
    "    # TODO: Consider including entries with an empy changeaddr since it could help with accuracy \n",
    "    # of SALE calculations for the owners \n",
    "    if(pandas.isnull(owner_changeaddr) or re_noalnum.match(owner_changeaddr)):\n",
    "        #print \"%s: Missing owner_changeaddr\" % (par_id)\n",
    "        # skip this one\n",
    "        return False\n",
    "    \n",
    "    # We have valid par_id, owner_name, and owner_changeaddr.  Update the maps.\n",
    "    property_event={'date': saledate, 'asofdate': asofdate, \n",
    "                    'event_type':'PURCHASE', 'ownername':owner_name, 'changeaddr': owner_changeaddr}\n",
    "    owner_event={'date':saledate, 'event_type':'PURCHASE', 'parid':par_id}\n",
    "\n",
    "    # Check for nice-to-have columns, if valid value, add to property_event\n",
    "    try:\n",
    "        if('SALEDESC' in colnames):\n",
    "            saledesc = apd['SALEDESC'].iloc[i]\n",
    "            if(saledesc != '' and not (isinstance(saledesc, numbers.Number) and math.isnan(saledesc))):\n",
    "                property_event['saledesc']=saledesc\n",
    "                \n",
    "        if('SALEPRICE' in colnames):\n",
    "            saleprice = apd['SALEPRICE'].iloc[i]\n",
    "            if(saleprice != '' and not (isinstance(saleprice, numbers.Number) and math.isnan(saleprice))):\n",
    "                property_event['saleprice']=saleprice\n",
    "\n",
    "        if('HOMESTEADFLAG' in colnames):\n",
    "            homesteadflag = apd['HOMESTEADFLAG'].iloc[i]\n",
    "            if(homesteadflag != '' and not (isinstance(homesteadflag, numbers.Number) and math.isnan(homesteadflag))):\n",
    "                property_event['homesteadflag']=homesteadflag\n",
    "\n",
    "        if('OWNERDESC' in colnames):\n",
    "            ownerdesc = apd['OWNERDESC'].iloc[i]\n",
    "            if(ownerdesc != '' and not (isinstance(ownerdesc, numbers.Number) and math.isnan(ownerdesc))):\n",
    "                property_event['ownerdesc']=ownerdesc\n",
    "\n",
    "        if('USEDESC' in colnames):\n",
    "            usedesc = apd['USEDESC'].iloc[i]\n",
    "            if(usedesc != '' and not (isinstance(usedesc, numbers.Number) and math.isnan(usedesc))):\n",
    "                property_event['usedesc']=usedesc\n",
    "\n",
    "        if('FAIRMARKETTOTAL' in colnames):\n",
    "            fm_total = apd['FAIRMARKETTOTAL'].iloc[i]\n",
    "            if(fm_total != '' and not (isinstance(fm_total, numbers.Number) and math.isnan(fm_total))):\n",
    "                property_event['fm_total']=fm_total\n",
    "\n",
    "        if('is_vacant' in colnames):\n",
    "            is_vacant = apd['is_vacant'].iloc[i]\n",
    "            if(not math.isnan(fm_total)):\n",
    "                property_event['is_vacant']=is_vacant\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        # Cache the canonical address for this property if available\n",
    "        update_property_address(apd, i)\n",
    "        # Store the canonical owner address with this property record if available\n",
    "        canonical_owner_addr=get_canonical_owner_changeaddr(apd, i)\n",
    "        if(canonical_owner_addr):\n",
    "            property_event['canonical_owner_address']={'census':canonical_owner_addr}\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    #print \"-------\\nProcessing %s: %r %r %r\" % (par_id, saledate, owner_name, owner_changeaddr)\n",
    "\n",
    "    if(not property_map.has_key(par_id)):\n",
    "        # We don't have an entry for this property yet, create an empty list\n",
    "        property_map[par_id]=[]\n",
    "    \n",
    "    # Insert the property event\n",
    "    pret = insert_event(property_map[par_id], property_event)\n",
    " \n",
    "    if(not owner_map.has_key(owner_name)):\n",
    "        # We don't have an entry for this owner name yet, create an empty list\n",
    "        owner_map[owner_name]=[]\n",
    "    \n",
    "    # Insert the owner event to owner map\n",
    "    oret = insert_event(owner_map[owner_name], owner_event)\n",
    "   \n",
    "    if(not changeaddr_map.has_key(owner_changeaddr)):\n",
    "        # We don't have an entry for this changeaddr yet, create an empty list\n",
    "        changeaddr_map[owner_changeaddr]=[]\n",
    "    \n",
    "    # Insert the owner event\n",
    "    cret = insert_event(changeaddr_map[owner_changeaddr], owner_event)\n",
    "    \n",
    "    # Print diagnostics\n",
    "    #print \"%s: %r %r %r = %r %r %r\" % (par_id, saledate, owner_name, owner_changeaddr, pret, oret, cret)\n",
    "    return(pret or oret or cret)\n",
    "\n",
    "# Clear out the cache that stores what sets of properties are associated with each owner changeaddr at a given time.  \n",
    "# We need to do this whenever we process in a new dataset.\n",
    "def reset_cache():\n",
    "    owned_parids_cache={}\n",
    "    \n",
    "# For a common parcel id, check if two events are both for the same transaction.  \n",
    "# This happens when the dates are the same and/or when the set of owner names overlap\n",
    "def is_same_transaction(e1,e2):\n",
    "    if(e1['event_type']!='PURCHASE' or e2['event_type']!='PURCHASE'):\n",
    "        return False\n",
    "    if(e1['date']==e2['date']):\n",
    "        return True\n",
    "    if((e1['ownername'] in e2['ownername']) or (e2['ownername'] in e1['ownername'])):\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "def register_sale_dates(parid):\n",
    "    prop_ev_list = property_map[parid]\n",
    "    if(len(prop_ev_list)<2):\n",
    "        # No sales have happened that we know about, only purchases\n",
    "        return\n",
    "\n",
    "    # Keep a list of the equivalent transactions so any subsequent sale can \n",
    "    # be applied to all.  Seed it with the 0'th item in prop_ev_list\n",
    "    equiv_transactions=[prop_ev_list[0]]\n",
    "    #print \"Starting with %r\" %(equiv_transactions)\n",
    "    for i in range(1,len(prop_ev_list)):\n",
    "        # See if this new transaction is equivalent to the last item in the current list\n",
    "        if(is_same_transaction(prop_ev_list[i],equiv_transactions[-1])):\n",
    "            equiv_transactions.append(prop_ev_list[i])\n",
    "            #print \"  %d still equivalent %r\" %(i, equiv_transactions)\n",
    "        else:\n",
    "            #print \"  %d not equivalent %r, record sale date\" %(i,prop_ev_list[i])\n",
    "            sell_event={'date':prop_ev_list[i]['date'],'event_type':'SALE','parid':parid}\n",
    "            \n",
    "            for j in range(0,len(equiv_transactions)):\n",
    "                    dup_o = insert_event(owner_map[equiv_transactions[j]['ownername']], sell_event)\n",
    "                    dup_c = insert_event(changeaddr_map[equiv_transactions[j]['changeaddr']], sell_event)\n",
    "                    #print \"    %d %r: %r %r\" % (j, equiv_transactions[j], dup_o, dup_c)\n",
    "            # Reset equiv_transactions and keep going if any more transactions are left\n",
    "            equiv_transactions=[prop_ev_list[i]]\n",
    "        \n",
    "# This needs to be called once after imports are complete and before get_owned_parids\n",
    "def register_all_sales():\n",
    "    # Reset the owner and changeaddr caches since this will change the results\n",
    "    reset_cache()\n",
    "    \n",
    "    # Add SALE records each time a property goes into different hands\n",
    "    for parid in property_map.keys():\n",
    "        register_sale_dates(parid)\n",
    "   \n",
    "def get_next_sale_date(parid, start_date):\n",
    "    event_list = property_map[parid]\n",
    "    # Get the index of the first transaction at or before this date\n",
    "    start_index=find_event_index_by_date(event_list, start_date)\n",
    "\n",
    "    # Starting with the transaction at or before the initial start date, \n",
    "    # go past all the equivalent transactions to the next one that's a real transfer.\n",
    "    # If no future transfers, then return None\n",
    "    equiv_transactions=[event_list[start_index]]\n",
    "\n",
    "    for i in range(start_index,len(event_list)):\n",
    "        # See if this new transaction is equivalent to the last item in the current list\n",
    "        if(is_same_transaction(event_list[i],equiv_transactions[-1])):\n",
    "            equiv_transactions.append(event_list[i])\n",
    "            #print \"  %d still equivalent %r\" %(i, equiv_transactions)\n",
    "        else:\n",
    "            # This is a real tranfer, return the event date\n",
    "            return event_list[i]['date']\n",
    "    # If we get to here, there is no event after this date.  Return None\n",
    "    return None\n",
    "\n",
    "    \n",
    "def get_owned_parids(name, event_list, eval_date):\n",
    "    # Check if this name:date pair is in the cache.  If so, return the \n",
    "    # stored map of {'parids':owned_parids,'next_date':next_date}\n",
    "    name_date_str=u\"%s:%s\"%(name,eval_date)\n",
    "    if(name_date_str in owned_parids_cache):\n",
    "        return(owned_parids_cache[name_date_str])\n",
    "\n",
    "    # Return value wasn't in the cache, compute it\n",
    "    \n",
    "    # Use a set for accumulating parids since we don't want duplicates\n",
    "    owned_parids=set()\n",
    "    \n",
    "    # In case eval_date is at or after the last date in the list, \n",
    "    # default next_date to today\n",
    "    next_date = this_date\n",
    "    for i in range(0,len(event_list)):\n",
    "        if(event_list[i]['date']<= eval_date):\n",
    "            # This event happened on or before the date we're asking about, process the event\n",
    "            if(event_list[i]['event_type']=='PURCHASE'):\n",
    "                # Add purchased property to owned_parids\n",
    "                owned_parids.add(event_list[i]['parid'])\n",
    "            else:\n",
    "                # This must be a sale, remove it from owned_parids\n",
    "                # Note that this can potentially fail in the case where \n",
    "                # two subsequent sales of the same property involve the same \n",
    "                # change address if the transactions happen as (add, \n",
    "                # (add, remove) from the same day, remove).  This actually \n",
    "                # happens with PARID == '0104R00158000000'.  So, we put this\n",
    "                # in a try/catch block.  Bleh...\n",
    "                try:\n",
    "                    owned_parids.remove(event_list[i]['parid'])\n",
    "                except:\n",
    "                    pass\n",
    "        else:\n",
    "            # This and subsequent events happened after the date we're looking for.\n",
    "            # Set next_date and return owned_parids\n",
    "            next_date = event_list[i]['date']\n",
    "            break\n",
    "\n",
    "    ret_val = {'parids':owned_parids,'next_date':next_date}\n",
    "    # Store in cache for next time\n",
    "    owned_parids_cache[name_date_str]=ret_val\n",
    "    return ret_val\n",
    "\n",
    "# For a given parid and eval_date, get a map back containing:\n",
    "#   'parids' = the set of parids owned by the same owners/changeaddrs as of eval_date, and\n",
    "#   'next_date' = the next date that something changes within that set of owners/changeaddrs\n",
    "# When the last of the related events is complete, 'next_date'==this_date\n",
    "def get_related_parids(parid, eval_date):\n",
    "    prop_ev_list = property_map[parid]\n",
    "    parid_set = set()\n",
    "    start_date = eval_date\n",
    "    next_date = this_date\n",
    "\n",
    "    # Keep a list of the equivalent transactions so we can accumulate the \n",
    "    # parids for all.  Seed it with the 0'th item in prop_ev_list\n",
    "    equiv_transactions=[prop_ev_list[0]]\n",
    "    # If date of the first event is later than eval_date, set eval_date to \n",
    "    # the first event.  We don't want to iterate over the earlier purchases by\n",
    "    # the original owners we know about.  Just start at the first purchase\n",
    "    # of this parcel we know about.\n",
    "    if(eval_date<prop_ev_list[0]['date']):\n",
    "        eval_date=prop_ev_list[0]['date']\n",
    "        start_date=eval_date\n",
    "    #print \"%s: starting with %r\" %(parid, equiv_transactions)\n",
    "    for i in range(1,len(prop_ev_list)):\n",
    "        # See if this new transaction is equivalent to the last item in the current list\n",
    "        if(is_same_transaction(prop_ev_list[i],equiv_transactions[-1]) and \n",
    "           prop_ev_list[i]['date']<=eval_date):\n",
    "            equiv_transactions.append(prop_ev_list[i])\n",
    "            #print \"  %d still equivalent %r\" %(i, equiv_transactions)\n",
    "        elif(prop_ev_list[i]['date']>eval_date):\n",
    "            #print \"  %d after timespan (%r), process current set\" %(i,prop_ev_list[i])\n",
    "            break\n",
    "        else:\n",
    "            # We haven't hit the end of the timespan yet, but the ownership has changed.\n",
    "            # Reset equiv_transactions starting from the current event\n",
    "            equiv_transactions=[prop_ev_list[i]]\n",
    "            \n",
    "    # We've got all the equivalent transactions, process them\n",
    "    changeaddr_set=set()\n",
    "    owner_set=set()\n",
    "    for j in range(0,len(equiv_transactions)):\n",
    "        owner_set.add(equiv_transactions[j]['ownername'])\n",
    "        changeaddr_set.add(equiv_transactions[j]['changeaddr'])\n",
    "\n",
    "    #print \"    Owner set= %r\\n    Changeaddr set= %r\" % (owner_set, changeaddr_set)\n",
    "    \n",
    "    for owner in owner_set:\n",
    "        ret_map=get_owned_parids(owner, owner_map[owner],eval_date)\n",
    "        parid_set = parid_set.union(ret_map['parids'])\n",
    "        if(ret_map['next_date']<next_date):\n",
    "            next_date = ret_map['next_date']\n",
    "    for changeaddr in changeaddr_set:\n",
    "        ret_map=get_owned_parids(changeaddr, changeaddr_map[changeaddr],eval_date)\n",
    "        parid_set = parid_set.union(ret_map['parids'])\n",
    "        if(ret_map['next_date']<next_date):\n",
    "            next_date = ret_map['next_date']\n",
    "\n",
    "    ret_val={\"parids\":parid_set, \"parcount\":len(parid_set), \"start_date\":start_date, \"next_date\":next_date}\n",
    "    #print \"    Returning %r\" % (ret_val)\n",
    "    return ret_val\n",
    "\n",
    "def get_simultaneous_property_events(prop_ev_list, start_index):\n",
    "    eval_date=prop_ev_list[start_index]['date']\n",
    "    start_date = eval_date\n",
    "    next_date = this_date\n",
    "\n",
    "    # Keep a list of the equivalent transactions.  Seed it with the start_index'th item in prop_ev_list\n",
    "    equiv_transactions=[prop_ev_list[start_index]]\n",
    "    # Check if this is the last transaction in prop_ev_list\n",
    "    if(start_index+1>=len(prop_ev_list)):\n",
    "        return equiv_transactions\n",
    "    \n",
    "    # At least one left\n",
    "    for i in range(start_index+1,len(prop_ev_list)):\n",
    "        # See if this new transaction is equivalent to the last item in the current list\n",
    "        if(is_same_transaction(prop_ev_list[i],equiv_transactions[-1]) and \n",
    "           prop_ev_list[i]['date']==eval_date):\n",
    "            equiv_transactions.append(prop_ev_list[i])\n",
    "            #print \"  %d still equivalent %r\" %(i, equiv_transactions)\n",
    "        elif(prop_ev_list[i]['date']>eval_date):\n",
    "            #print \"  %d after timespan (%r), process current set\" %(i,prop_ev_list[i])\n",
    "            break\n",
    "        else:\n",
    "            # We haven't hit the end of the timespan yet, but the ownership has changed.\n",
    "            # Reset equiv_transactions starting from the current event\n",
    "            break\n",
    "            \n",
    "    # We've got all the equivalent transactions, sort them according to asofdate\n",
    "    return sorted(equiv_transactions, key=lambda ev: ev['asofdate'])\n",
    "\n",
    "# Call this once for each assessment spreadsheet/database file.  It adds PURCHASE records if they don't already exist.\n",
    "# It's safe to call multiple times on the same input file if necessary, but takes a lot of processing time\n",
    "# asofdate, in format YYYY-MM-DD, is used to disambiguate earlier vs later versions of addresses or values\n",
    "def process_all_assessment_records(apd, asofdate):\n",
    "    start=arrow.now()\n",
    "    addcnt=0\n",
    "    chunk_start_time=arrow.now()\n",
    "    chunk_size=10000\n",
    "\n",
    "    # Clear the owner and changeaddr caches as this will cause changes in the ownership profiles\n",
    "    reset_cache()\n",
    "    \n",
    "    # Iterate over each record in the data frame and add to the maps\n",
    "    for i in range(0,len(apd)):\n",
    "        if(process_assessment_record(apd,i, asofdate)):\n",
    "            addcnt=addcnt+1\n",
    "        if((i%chunk_size)==0 and i>0):\n",
    "            print \"%d-%d: %d added, %s time elapsed\" %(i-(chunk_size-1), i, addcnt, arrow.now()-chunk_start_time)\n",
    "            #break\n",
    "            addcnt=0\n",
    "            chunk_start_time=arrow.now()\n",
    "    end=arrow.now()\n",
    "    print \"Processing took %s\" % (str(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support save/restore for assessment maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to save out property_map before we start messsing with it\n",
    "import pickle\n",
    "pickle_file_t = 'assessments/{0}_{1}.pickle'\n",
    "\n",
    "# Save this data frame using a suffix to get it back with.  \n",
    "# For example, '99_04_05_09_17'\n",
    "def save_assessment_maps(suffix):\n",
    "    global property_map\n",
    "    global owner_map\n",
    "    global changeaddr_map\n",
    "    global canonical_property_address_map\n",
    "    global raw_property_address_map\n",
    "\n",
    "    print \"** Saving assessment maps with suffix %s\" % (suffix)\n",
    "    \n",
    "    with open(pickle_file_t.format('property_map',suffix), 'wb') as handle:\n",
    "        pickle.dump(property_map, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open(pickle_file_t.format('owner_map',suffix), 'wb') as handle:\n",
    "        pickle.dump(owner_map, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open(pickle_file_t.format('changeaddr_map',suffix), 'wb') as handle:\n",
    "        pickle.dump(changeaddr_map, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open(pickle_file_t.format('canonical_property_address_map',suffix), 'wb') as handle:\n",
    "        pickle.dump(canonical_property_address_map, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open(pickle_file_t.format('raw_property_address_map',suffix), 'wb') as handle:\n",
    "        pickle.dump(raw_property_address_map, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    with open(pickle_file_t.format('parid2centroid',suffix), 'wb') as handle:\n",
    "        pickle.dump(parid2centroid, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print \"** Done\"\n",
    "    \n",
    "def restore_assessment_maps(suffix):\n",
    "    global property_map\n",
    "    global owner_map\n",
    "    global changeaddr_map\n",
    "    global canonical_property_address_map\n",
    "    global raw_property_address_map\n",
    "    \n",
    "    print \"** Restoring assessment maps with suffix %s\" % (suffix)\n",
    "\n",
    "    with open(pickle_file_t.format('property_map',suffix), 'rb') as handle:\n",
    "        property_map = pickle.load(handle)\n",
    "    print \"    Loaded %d records from %s\" % (len(property_map), pickle_file_t.format('property_map',suffix))\n",
    "\n",
    "    with open(pickle_file_t.format('owner_map',suffix), 'rb') as handle:\n",
    "        owner_map = pickle.load(handle)\n",
    "    print \"    Loaded %d records from %s\" % (len(owner_map), pickle_file_t.format('owner_map',suffix))\n",
    "    \n",
    "    with open(pickle_file_t.format('changeaddr_map',suffix), 'rb') as handle:\n",
    "        changeaddr_map = pickle.load(handle)\n",
    "    print \"    Loaded %d records from %s\" % (len(changeaddr_map), pickle_file_t.format('changeaddr_map',suffix))\n",
    "    \n",
    "    with open(pickle_file_t.format('canonical_property_address_map',suffix), 'rb') as handle:\n",
    "        canonical_property_address_map = pickle.load(handle)\n",
    "    print \"    Loaded %d records from %s\" % (len(canonical_property_address_map), pickle_file_t.format('canonical_property_address_map',suffix))\n",
    "    \n",
    "    with open(pickle_file_t.format('raw_property_address_map',suffix), 'rb') as handle:\n",
    "        raw_property_address_map = pickle.load(handle)\n",
    "    print \"    Loaded %d records from %s\" % (len(raw_property_address_map), pickle_file_t.format('raw_property_address_map',suffix))\n",
    "    print \"** Done\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restore_assessment_maps('pms_cac_99b_04d_05_09_10_11_17_18b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apd_15217_2017 = apd_2017[apd_2017.PROPERTYZIP=='15217'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_addrs(apd_15213_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canonicalize_addrs(apd_15213_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_all_assessment_records(apd_15213_2017,'2017-10-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge property and owner addresses\n",
    "merge_addrs(apd_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canonicalize_addrs(apd_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Process entries from apd_2017 into the property, owner, and changeaddr maps\n",
    "process_all_assessment_records(apd_2017, '2017-10-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(apd_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out apd_2017 to mdf\n",
    "save_adf(apd_2017, 'apd_2017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following are historical fixups I had to do for the 2017 data set, but which shouldn't need to be done again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apd_2017.loc[['0174E00236000000','0174E00234000000','0029H00025000000']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacant_col1_2017 = (apd_2017.FAIRMARKETBUILDING<2.0)\n",
    "vacant_col2_2017 = ((apd_2017['USEDESC'].fillna('').str.contains('VACANT')) | (apd_2017['USEDESC'].fillna('').str.contains('UNLOCATED PARCEL')) | (apd_2017['USEDESC'].fillna('').str.contains('CONDEMNED')))\n",
    "vacant_col_2017=(vacant_col1_2017 | vacant_col2_2017)\n",
    "apd_2017[vacant_col_2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacant_col_2017.name='is_vacant'\n",
    "apd_2017 = pd.concat([apd_2017, vacant_col_2017], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore apd_2018 from mdf if we don't want to reprocess from scratch\n",
    "apd_2018 = restore_adf('apd_2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in assessments data as TDF, set index to PARID\n",
    "path = \"assessments/2018-01-cd/AC Property Assessments_01222018.xls\"\n",
    "raw_pd_2018 = pd.read_csv(path,sep='\\t', index_col='PARID',dtype={'PROPERTYHOUSENUM':numpy.str,'CHANGENOTICEADDRESS4':numpy.str, 'PROPERTYZIP':numpy.str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider a parcel a dwelling if either residential or apartments (usedesc includes APART)\n",
    "# dwelling_col is indexed on PARID, has TRUE for dwellings, and FALSE for non-dwellings\n",
    "dwelling_col_2018 = ((raw_pd_2018.CLASSDESC == 'RESIDENTIAL') | (raw_pd_2018.USEDESC.str.contains('APART')))\n",
    "dwelling_col_2018.name='is_dwelling'\n",
    "\n",
    "# Only keep the subset that are dwellings\n",
    "apd_2018 = pd.concat([raw_pd_2018, dwelling_col_2018], axis=1)[dwelling_col_2018]\n",
    "len(apd_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apd_2018.loc[['0174E00236000000','0174E00234000000','0029H00025000000']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacant_col1_2018 = (apd_2018.FAIRMARKETBUILDING<2.0)\n",
    "vacant_col2_2018 = ((apd_2018['USEDESC'].fillna('').str.contains('VACANT')) | (apd_2018['USEDESC'].fillna('').str.contains('UNLOCATED PARCEL')) | (apd_2018['USEDESC'].fillna('').str.contains('CONDEMNED')))\n",
    "vacant_col_2018=(vacant_col1_2018 | vacant_col2_2018)\n",
    "apd_2018[vacant_col_2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacant_col_2018.name='is_vacant'\n",
    "apd_2018 = pd.concat([apd_2018, vacant_col_2018], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge property and owner addresses\n",
    "merge_addrs(apd_2018)\n",
    "canonicalize_addrs(apd_2018)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apd_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_map['0002M00197000000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_assessment_record(apd_2018, 0, '2018-01-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = ['owner_address', 'is_vacant', 'HOMESTEADFLAG', 'USEDESC', 'property_address_raw', 'SALEPRICE', 'FAIRMARKETTOTAL', 'SALEDESC', 'owner_address_raw', 'property_address']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "'is_vacant' in colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out apd_2018 to mdf\n",
    "save_adf(apd_2018, 'apd_2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process entries from apd_2018 into the property, owner, and changeaddr maps\n",
    "process_all_assessment_records(apd_2018, '2018-01-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deal with older versions of the assessment database from Kristin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original processing of 1999 data is in\n",
    "#   http://localhost:8815/notebooks/uwsgi/dotmaptiles/data-visualization-tools/examples/lodes/Import%20DBF%20Test.ipynb\n",
    "#\n",
    "# It was then pulled into a copy of this notebook and processed:\n",
    "#  apd_1999 = merge_dwelling_col(apd_1999)\n",
    "#  pa_2017 = apd_2017[['property_address_raw','property_address']]\n",
    "#  # Merge property_address_raw and property_address into 1999\n",
    "#  apd_1999 = pd.merge(adf_1999,pa_2017,on='PARID', left_on=None, right_on=None,\n",
    "#                left_index=False, right_index=False, sort=False,\n",
    "#                suffixes=('_x', '_y'), copy=True, indicator=False,\n",
    "#                validate=None)\n",
    "#  # After canonicalization cacheing completes\n",
    "#  apd_1999['owner_address'] = apd_1999['owner_address_raw'].apply(get_canonical_address)\n",
    "# \n",
    "#  Manual fixup of bad addresses and sale dates:\n",
    "#  apd_1999.set_value('0162H00002000000','property_address_raw','124 MONTVILLE ST PITTSBURGH PA 15214')\n",
    "#  apd_1999.set_value('0162H00002000000','property_address',get_canonical_address('124 MONTVILLE ST PITTSBURGH PA 15214'))\n",
    "# \n",
    "#  apd_1999.set_value('0232C00201000000','SALEDATE','1970-11-01')\n",
    "#  The result was saved out using:\n",
    "#  save_adf(apd_1999, 'apd_1999')\n",
    "#\n",
    "apd_1999 = restore_adf('apd_1999')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process assessment records from apd_1999.  We don't know when it's from exactly, but the most recent \n",
    "# SALEDATE in there is '1997-12-01' (apd_1999['SALEDATE'].max()), so '1999-01-01' seems safe\n",
    "process_all_assessment_records(apd_1999, '1999-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following are historical fixups I had to do for the 1999 data set, but which shouldn't need to be done again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do fixup to create FAIRMARKETTOTAL column.  This only needs to be done once.\n",
    "col_rename_1999_augment = {'PRICELASTS':'SALEPRICE',\n",
    "'TLANDASMT':'FAIRMARKETLAND',\n",
    "'TBLDGASMT':'FAIRMARKETBUILDING'}\n",
    "\n",
    "apd_1999= apd_1999.rename(index=str, columns=col_rename_1999_augment)\n",
    "\n",
    "# Construct a 'FAIRMARKETTOTAL' column\n",
    "apd_1999['FAIRMARKETTOTAL']=apd_1999[['FAIRMARKETLAND','FAIRMARKETBUILDING']].apply(sum, axis=1)\n",
    "\n",
    "\n",
    "vacant_col_1999 = (apd_1999.FAIRMARKETBUILDING==0.0)\n",
    "vacant_col_1999.name='is_vacant'\n",
    "apd_1999 = pd.concat([apd_1999, vacant_col_1999], axis=1)\n",
    "apd_1999\n",
    "\n",
    "# Save apd_1999 back out\n",
    "save_adf(apd_1999, 'apd_1999')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter()\n",
    "    \n",
    "def change_property_event_date(parid, orig_date, asofdate, new_date):\n",
    "    event_list = property_map[parid]\n",
    "    # Find the index of the element already in the list which matches or precedes orig_date.  \n",
    "    # If not found, match_index is -1, so we should return False\n",
    "    # If found, check if the record matches what we already have.  If it matches, delete and return True\n",
    "    match_index = find_event_index_by_date(event_list, orig_date)\n",
    "    if(match_index==-1 or orig_date>event_list[match_index]['date']):\n",
    "        print \"%s: No match\" % (parid)\n",
    "        return False\n",
    "    # We've got a matching date for this event, compare it with this and any other potentially \n",
    "    # simultaneous events\n",
    "    print \"%s: Time match at %d\" % (parid, match_index)\n",
    "    for i in range(match_index, len(event_list)):\n",
    "        if(orig_date == event_list[i]['date'] and 'asofdate' in event_list[i] and asofdate == event_list[i]['asofdate']):\n",
    "            # This is an event from the same sale date and asofdate, delete, modify, reinsert, \n",
    "            # Modify changeaddr_map and owner_map, and return true.\n",
    "            print \"%s: Found matching date and asofdate in property_map at %d, modifying\" % (parid, i)\n",
    "            event_map = event_list[i]\n",
    "            del event_list[i]\n",
    "            event_map['date']=new_date\n",
    "            res_eins = insert_event(event_list, event_map)\n",
    "                            \n",
    "            old_owner_event={'date':orig_date, 'event_type':'PURCHASE', 'parid':parid}\n",
    "            new_owner_event={'date':new_date, 'event_type':'PURCHASE', 'parid':parid}\n",
    "            res_cdel = delete_event(changeaddr_map[event_map['changeaddr']], old_owner_event)\n",
    "            res_cins = insert_event(changeaddr_map[event_map['changeaddr']], new_owner_event)\n",
    "            \n",
    "            res_odel = delete_event(owner_map[event_map['ownername']], old_owner_event)\n",
    "            res_oins = insert_event(owner_map[event_map['ownername']], new_owner_event)\n",
    "\n",
    "            print \"property_map insert %r:\\n   property_map[%r]=\" % (\"SUCCEEDED\" if res_eins else 'FAILED', parid)\n",
    "            pp.pprint(property_map[parid])\n",
    "            print \"changeaddr_map del/ins %r\\n   changeaddr_map[%r]=\" % (\"SUCCEEDED\" if (res_cdel and res_cins) else 'FAILED', event_map['changeaddr'])\n",
    "            pp.pprint(changeaddr_map[event_map['changeaddr']])\n",
    "            print \"owner_map del/ins %r\\n   owner_map[%r]=\" % (\"SUCCEEDED\" if (res_odel and res_oins) else 'FAILED', event_map['ownername'])\n",
    "            pp.pprint(owner_map[event_map['ownername']])\n",
    "            return True\n",
    "        elif(orig_date<event_list[i]['date']):\n",
    "            # We're past any simultaneous events that might have matched, break and return False\n",
    "            break\n",
    "    #print \"No match\"\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apd_1999[apd_1999.SALEDATE.str.contains('-00-')][['PROPERTYOWNER','SALEDATE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_map['0062G00184000000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_property_event_date('0062G00184000000','1950-05-05', '2009-09-01','1956-05-21')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_property_event_date('0062G00184000000','1950-05-05', '1999-01-01','1956-05-21')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_property_event_date('0062G00184000000','1950-05-05', '2010-06-01','1956-05-21')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_property_event_date('0127N00014000000','1995-00-01', '1999-01-01', '1995-05-18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_property_event_date('0120L00200000000','1955-00-01', '1999-01-01', '1955-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(changeaddr_map[u'539 S 4TH AVE LOUISVILLE KY 40202'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_property_event_date('0009M00087000000', '1992-00-01', '1999-01-01', '1950-05-05')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_property_event_date('0009M00087000000', '1950-05-05', '1999-01-01', '1992-00-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do fixup on bad addresses.  This should only have to be done once\n",
    "apd_1999[apd_1999.SALEDATE.str.contains('-00-')][['PROPERTYOWNER','SALEDATE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixup_changeaddrs_from_asofdate(apd_1999.index, '1999-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixup_canonical_owner_address_from_asofdate(apd_1999.index, '1999-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " len(changeaddr_map[u'539 S 4TH AVE LOUISVILLE KY 40202'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changeaddr_map[u'539 S 4TH AVE LOUISVILLE KY 40202'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_map['0087R00101000000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to save out property_map before we start messsing with it\n",
    "import pickle\n",
    "\n",
    "with open('assessments/property_map_99_09_17.pickle', 'wb') as handle:\n",
    "    pickle.dump(property_map, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apd_2004 = restore_adf('apd_2004')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(apd_2004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process assessment records from apd_2004.  We don't know when it's from exactly, but the most recent \n",
    "# SALEDATE in there is '2001-02-15' (apd_2004['SALEDATE'].max()), so '2004-01-01' seems safe.\n",
    "# Actually, comparing the 2004 to 2005 sets, we really need the asofdate to be '2001-02-15'\n",
    "# instead of '2004-01-01'\n",
    "process_all_assessment_records(apd_2004, '2001-02-15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following are historical fixups I had to do for the 2004 data set, but which shouldn't need to be done again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewrite property_map entries with asofdate of '2004-01-01' to '2001-02-15'\n",
    "bad_asofdate='2004-01-01'\n",
    "replacement_asofdate='2001-02-15'\n",
    "count=0\n",
    "for parid in apd_2004.index:\n",
    "    for ev in property_map[parid]:\n",
    "        if(ev['asofdate']==bad_asofdate):\n",
    "            ev['asofdate']=replacement_asofdate\n",
    "            if(count % 1000==0):\n",
    "                print \"%s: %s [%s] -> [%s]\" % (parid, ev['date'], bad_asofdate, replacement_asofdate)\n",
    "            count = count+1            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(apd_2004)):\n",
    "    update_property_address(apd_2004, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for parid in noprop_parids_2004:\n",
    "    if (not parid in canonical_property_address_map):\n",
    "        print \"%s missing from canonical_property_address_map (raw = %s)\" % (parid, raw_property_address_map[parid])\n",
    "    else:\n",
    "        if((count % 100)==0):\n",
    "            print \"%d: Ok\" % (count)\n",
    "        count = count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacant_col_2004 = apd_2004.FAIRMARKETBUILDING==0.0\n",
    "vacant_col_2004.name='is_vacant'\n",
    "apd_2004 = pd.concat([apd_2004, vacant_col_2004], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with potential quality issue for owner addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apd_2004.loc[['0174E00236000000','0174E00234000000','0029H00025000000']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apd_2004[apd_2004.owner_address_raw=='PO BOX 8469 CANTON OH 44711']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixup_changeaddrs_from_asofdate(apd_2004.index, '2001-02-15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixup_canonical_owner_address_from_asofdate(apd_2004.index, '2001-02-15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(changeaddr_map['PO BOX 8469 CANTON OH 44711'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_map['9946X83377000000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_event_index_by_date(changeaddr_map['133 JEFFERSON RD PITTSBURGH PA 15235'], '1981-08-19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owner_event={'date':'1981-08-19', 'event_type':'PURCHASE', 'parid':'0001C01662004200'}\n",
    "delete_event(changeaddr_map['133 JEFFERSON RD PITTSBURGH PA 15235'],owner_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changeaddr_map['PO BOX 22552 PITTSBURGH PA 15222']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_map[apd_2004.index[4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_adf(apd_2004,'apd_2004')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to save out property_map before we start messsing with it\n",
    "import pickle\n",
    "\n",
    "with open('assessments/property_map_99_04_09_17.pickle', 'wb') as handle:\n",
    "    pickle.dump(property_map, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apd_2005 = restore_adf('apd_2005')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process assessment records from apd_2005.  We don't know when it's from exactly, but the most recent \n",
    "# valid-looking SALEDATE in there is '2005-08-28 (apd_2005['SALEDATE'].max())\n",
    "process_all_assessment_records(apd_2005, '2005-08-28')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following are historical fixups I had to do for the 2005 data set, but which shouldn't need to be done again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apd_2005.loc[['0174E00236000000','0174E00234000000','0029H00025000000']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacant_col1_2005 = (apd_2005.FAIRMARKETBUILDING<2.0)\n",
    "vacant_col2_2005 = ((apd_2005.USEDESC.fillna('').str.contains('VACANT')) | (apd_2005.USEDESC.fillna('').str.contains('UNLOCATED PARCEL')) | (apd_2005.USEDESC.fillna('').str.contains('CONDEMNED')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacant_diff_col_2005=((vacant_col1_2005 & (~vacant_col2_2005)) | ((~vacant_col1_2005) & vacant_col2_2005))\n",
    "apd_2005[vacant_diff_col_2005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacant_col_2005=(vacant_col1_2005 | vacant_col2_2005)\n",
    "vacant_col_2005.name='is_vacant'\n",
    "apd_2005 = pd.concat([apd_2005, vacant_col_2005], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deal with older versions of the assessment database from the ACCDB files from Bob Gradeck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# From https://stackoverflow.com/questions/17123550/extract-and-sort-data-from-mdb-file-using-mdbtools-in-python?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa\n",
    "import sys, subprocess, os, numpy\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import pandas\n",
    "VERBOSE = True\n",
    "def mdb_to_pandas(database_path,dtype_map):\n",
    "    subprocess.call([\"mdb-schema\", database_path, \"mysql\"])\n",
    "    # Get the list of table names with \"mdb-tables\"\n",
    "    table_names = subprocess.Popen([\"mdb-tables\", \"-1\", database_path],\n",
    "                                   stdout=subprocess.PIPE).communicate()[0]\n",
    "    tables = table_names.splitlines()\n",
    "    sys.stdout.flush()\n",
    "    # Dump each table as a stringio using \"mdb-export\",\n",
    "    out_tables = {}\n",
    "    for rtable in tables:\n",
    "        table = rtable.decode()\n",
    "        if VERBOSE: print('running table:',table)\n",
    "        if table != '':\n",
    "            if VERBOSE: print(\"Dumping \" + table)\n",
    "            contents = subprocess.Popen([\"mdb-export\", database_path, table],\n",
    "                                        stdout=subprocess.PIPE).communicate()[0]\n",
    "            temp_io = StringIO(contents.decode('utf8'))\n",
    "            print(table, temp_io)\n",
    "            out_tables[table] = pd.read_csv(temp_io,dtype=dtype_map)\n",
    "    return out_tables\n",
    "pd.options.display.max_colwidth = 300\n",
    "pd.set_option('display.max_columns', 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ACCDB files are from Bob Gradeck\n",
    "# 2009 is missing YEARBLT, CLASSDESC, and CLASS\n",
    "\n",
    "# A number of files after 2009 seem to have a common set of column names\n",
    "post_2009_colmap = {'mapblolot':'PARID',\n",
    "                    'PropertyHouseNum'    :'PROPERTYHOUSENUM', \n",
    "                    'PropertyFraction'    :'PROPERTYFRACTION', \n",
    "                    'PropertyAddress'     :'PROPERTYADDRESS',  \n",
    "                    'PropertyUnit'        :'PROPERTYUNIT',     \n",
    "                    'PropertyCityState'   :'PROPERTYCITY',     \n",
    "                    'PropertyLocation2'   :'PROPERTYSTATE',    \n",
    "                    'PropertyZip'         :'PROPERTYZIP',       \n",
    "                    'FairMarketTotal':'FAIRMARKETTOTAL',\n",
    "                    'UseDesc':'USEDESC',\n",
    "                    'PropertyZip':'PROPERTYZIP',\n",
    "                    'PropertyOwner2':'PROPERTYOWNER',\n",
    "                    'ChangeNoticeFullAddress1':'CHANGENOTICEADDRESS1',\n",
    "                    'ChangeNoticeFullAddress2':'CHANGENOTICEADDRESS2',\n",
    "                    'ChangeNoticeFullAddress3':'CHANGENOTICEADDRESS3',\n",
    "                    'ChangeNoticeFullAddress4':'CHANGENOTICEADDRESS4',\n",
    "                    'HomesteadFlag':'HOMESTEADFLAG',\n",
    "                    'OwnerDesc':'OWNERDESC',\n",
    "                    'SaleDate':'SALEDATE',\n",
    "                    'SalePrice':'SALEPRICE',\n",
    "                    'SaleCode': 'SALECODE',\n",
    "                    'SaleDesc': 'SALEDESC'\n",
    "                    }\n",
    "post_2009_dtype = {'PropertyHouseNum':numpy.str,'PropertyFraction':numpy.str, 'PropertyZip':numpy.str,\n",
    "                    'ChangeNoticeFullAddress1':numpy.str,'ChangeNoticeFullAddress2':numpy.str, \n",
    "                    'ChangeNoticeFullAddress3':numpy.str,'ChangeNoticeFullAddress4':numpy.str, \n",
    "                   }\n",
    "\n",
    "accdb_info = {'2004':{'fname': 'assessments/kristin/ParcelData_RandySargent/2004/db1.mdb',\n",
    "                      'tname': 'LandFileAug2001',\n",
    "                      'col_remap': {'MapBloLot':'PARID',\n",
    "                                    'CurOwnerName':'PROPERTYOWNER',\n",
    "                                    'Homestead':'HOMESTEADFLAG',\n",
    "                                    'SalePrice':'SALEPRICE',\n",
    "                                    'TotalLandAssess':'FAIRMARKETLAND',\n",
    "                                    'TotalBuildingAssess':'FAIRMARKETBUILDING'\n",
    "                        },\n",
    "                      'dtype':{'MailStreetNumber':numpy.str,'MailZipCode':numpy.str, \n",
    "                               'ST_NUMBER':numpy.str, 'ZIPCODE':numpy.str\n",
    "                              }\n",
    "                     },\n",
    "              '2005':{'fname': 'assessments/kristin/ParcelData_RandySargent/2005/AC db 10.20.05.mdb',\n",
    "                      'tname': 'DS09-21-05',\n",
    "                      'col_remap': {'Field1':'PARID',\n",
    "                                    'Field2':'PROPERTYOWNER',\n",
    "                                    'Field23':'OWNERDESC',\n",
    "                                    'Field27':'USEDESC',\n",
    "                                    'Field29':'HOMESTEADFLAG',\n",
    "                                    'Field31':'SALEDATE_raw',\n",
    "                                    'Field32':'SALEPRICE',\n",
    "                                    'Field36':'FAIRMARKETBUILDING',\n",
    "                                    'Field37':'FAIRMARKETLAND',\n",
    "                                    'Field38':'FAIRMARKETTOTAL',\n",
    "                                    'Field57':'SALECODE',\n",
    "                                    'Field58':'SALEDESC',\n",
    "                                    'Field62':'YEARRBLT'\n",
    "                        },\n",
    "                      'dtype':{'Field45':numpy.str,'Field31':numpy.str, \n",
    "                               'Field4' :numpy.str, 'Field5' :numpy.str, 'Field6' :numpy.str, \n",
    "                               'Field7' :numpy.str, 'Field8' :numpy.str, 'Field9' :numpy.str, \n",
    "                               'Field10':numpy.str, 'Field11':numpy.str\n",
    "                              }\n",
    "                     },\n",
    "              '2009':{'fname':'assessments/gradeck/AssessmentSep09.mdb',\n",
    "                      'tname':'pncis',\n",
    "                     'col_remap': {'PIN':'PARID',\n",
    "                                    'PROPERTYHO':'PROPERTYHOUSENUM', \n",
    "                                    'PROPERTYAD':'PROPERTYFRACTION', \n",
    "                                    'PROPERTY_1':'PROPERTYADDRESS',  \n",
    "                                    'PROPERTYUN':'PROPERTYUNIT',     \n",
    "                                    'PROPERTYLO':'PROPERTYCITY',     \n",
    "                                    'PROPERTYCI':'PROPERTYSTATE',    \n",
    "                                    'PROPERTYZI':'PROPERTYZIP',       \n",
    "                                    'FAIRMARK_2':'FAIRMARKETTOTAL',\n",
    "                                    'USEDESC':'USEDESC',\n",
    "                                    'PROPERTYZI':'PROPERTYZIP',\n",
    "                                    'PROPERTYOW':'PROPERTYOWNER',\n",
    "                                    'CHANGENOTI':'CHANGENOTICEADDRESS1',\n",
    "                                    'CHANGENO_1':'CHANGENOTICEADDRESS2',\n",
    "                                    'CHANGENO_2':'CHANGENOTICEADDRESS3',\n",
    "                                    'CHANGENO_3':'CHANGENOTICEADDRESS4',\n",
    "                                    'HOMESTEADF':'HOMESTEADFLAG',\n",
    "                                    'OWNERDESC':'OWNERDESC',\n",
    "                                    'SALEDATE':'SALEDATE',\n",
    "                                    'SALEPRICE':'SALEPRICE'\n",
    "                                  },\n",
    "                      'dtype':{'PROPERTYHO':numpy.str,'PROPERTY_1':numpy.str,'PROPERTYUN':numpy.str,\n",
    "                                'PROPERTYLO':numpy.str,'PROPERTYCI':numpy.str,'PROPERTYZI':numpy.str}\n",
    "                     },\n",
    "               '2010':{'fname':'assessments/gradeck/June2010AssessNew.accdb',\n",
    "                       'tname':'RawDataAssessment',\n",
    "                       'col_remap': post_2009_colmap,\n",
    "                       'dtype': post_2009_dtype\n",
    "                      },\n",
    "               '2011':{'fname':'assessments/gradeck/AssessMarch2011Data3.mdb.accdb',\n",
    "                       'tname':'AssessMarch2011',\n",
    "                       'col_remap': {'PIN':'PARID',\n",
    "                                    'PropertyOwnerNew':'PROPERTYOWNER',\n",
    "                                    'CHANGENOTICEFULLADDRESS1':'CHANGENOTICEADDRESS1',\n",
    "                                    'CHANGENOTICEFULLADDRESS2':'CHANGENOTICEADDRESS2',\n",
    "                                    'CHANGENOTICEFULLADDRESS3':'CHANGENOTICEADDRESS3',\n",
    "                                    'CHANGENOTICEFULLADDRESS4':'CHANGENOTICEADDRESS4',\n",
    "                                    'PROPERTYHOUSENUM'    :'PROPERTYHOUSENUM',\n",
    "                                    'PROPERTYADDNUMPREFIX':'PROPERTYFRACTION',\n",
    "                                    'PROPERTYADDRESS'     :'PROPERTYADDRESS',\n",
    "                                    'PROPERTYUNIT'        :'PROPERTYUNIT',\n",
    "                                    'PROPERTYLOCATION2'   :'PROPERTYCITY',\n",
    "                                    'PROPERTYCITYSTATE'   :'PROPERTYSTATE',\n",
    "                                    'PROPERTYZIP'         :'PROPERTYZIP'       \n",
    "\n",
    "                                  },\n",
    "                        'dtype':{'PROPERTYHOUSENUM':numpy.str,'PROPERTYADDRESS':numpy.str,'PROPERTYUNIT':numpy.str,\n",
    "                                'PROPERTYLOCATION2':numpy.str,'PROPERTYCITYSTATE':numpy.str,'PROPERTYZIP':numpy.str}\n",
    "                      },\n",
    "               '2012':{'fname':'assessments/gradeck/AssessNov2012.mdb',\n",
    "                       'tname':'AssessmentOct2_2012',                       \n",
    "                       'col_remap': {'SALEDATE':'SALEDATE_raw'},\n",
    "                       'dtype':{'PROPERTYHOUSENUM':numpy.str,'PROPERTYADDRESS':numpy.str,'PROPERTYUNIT':numpy.str,\n",
    "                                'PROPERTYLOCATION2':numpy.str,'PROPERTYCITYSTATE':numpy.str,'PROPERTYZIP':numpy.str,\n",
    "                                'SALEDATE':numpy.str, 'CHANGENOTICEADDRESS1':numpy.str,\n",
    "                                'CHANGENOTICEADDRESS2':numpy.str, 'CHANGENOTICEADDRESS3':numpy.str,\n",
    "                                'CHANGENOTICEADDRESS4':numpy.str,}\n",
    "                      },\n",
    "               '2013':{'fname':'assessments/gradeck/Dec2013Assess.accdb',\n",
    "                       'tname':'RawDataAssessment',\n",
    "                       'col_remap': post_2009_colmap,\n",
    "                       'dtype': post_2009_dtype\n",
    "                      },\n",
    "               '2014':{'fname':'assessments/gradeck/June2014Assess.accdb',\n",
    "                       'tname':'RawDataAssessment',\n",
    "                       'col_remap': post_2009_colmap,\n",
    "                       'dtype': post_2009_dtype\n",
    "                      },\n",
    "               '2015':{'fname':'assessments/gradeck/May2015Assess.accdb',\n",
    "                       'tname':'RawDataAssessment',\n",
    "                       'col_remap': post_2009_colmap,\n",
    "                       'dtype': post_2009_dtype\n",
    "                      }\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Uses accdb_info map to import a file for a given year and return a dataframe.  \n",
    "# This doesn't remap the column names (we might not know them yet)\n",
    "def import_accdb_file(year):\n",
    "    # Import the accdb file for this year.  The returned value is a map of table names to tables\n",
    "    accdb_table_map = mdb_to_pandas(accdb_info[year]['fname'], accdb_info[year]['dtype'])     \n",
    "    apd_df = accdb_table_map[accdb_info[year]['tname']]\n",
    "    return apd_df\n",
    "\n",
    "# Takes a dataframe imported by import_accdb_file, renames columns, sets the index to PARID, \n",
    "# filters out rows that aren't dwellings, sets zipcode to be type string\n",
    "skip_dwelling_col=False\n",
    "def cleanup_accdb_import(df, year):\n",
    "    r_df = df.rename(index=str, columns=accdb_info[year]['col_remap']).set_index('PARID')\n",
    "    if(not skip_dwelling_col):\n",
    "        r_df = pd.merge(r_df,dwelling_col.to_frame(),on='PARID', left_on=None, right_on=None,\n",
    "                         left_index=False, right_index=False, sort=False,\n",
    "                         suffixes=('_x', '_y'), copy=True, indicator=False,\n",
    "                         validate=None)\n",
    "        print \"After merge, size = %d\" % (len(r_df))\n",
    "\n",
    "        r_df = r_df[r_df.is_dwelling]\n",
    "        print \"After filtering to only include dwellings, size = %d\" % (len(r_df))\n",
    "\n",
    "    # Change ',  -' in CHANGENOTICEADDRESS3 to nan (that's a common pattern in these accdb files)\n",
    "    bad_changeaddr = r_df.CHANGENOTICEADDRESS3==',  -'\n",
    "    r_df.loc[bad_changeaddr,'CHANGENOTICEADDRESS3']= numpy.nan\n",
    "\n",
    "    # Filter out empty owner names\n",
    "    empty_owner = r_df.PROPERTYOWNER.isna()\n",
    "\n",
    "    # Filter out empty change addresses\n",
    "    p1=pandas.isna(r_df.CHANGENOTICEADDRESS1) \n",
    "    p2=pandas.isna(r_df.CHANGENOTICEADDRESS2) \n",
    "    p3=pandas.isna(r_df.CHANGENOTICEADDRESS3)\n",
    "    p4=pandas.isna(r_df.CHANGENOTICEADDRESS4)\n",
    "    r_df = r_df[~((p1&p2&p3&p4)|empty_owner)]\n",
    "    print \"After filtering to remove empty owner names and change addresses, size = %d\" % (len(r_df))\n",
    "\n",
    "    # Fix up <BR> to be &\n",
    "    r_df.PROPERTYOWNER = r_df.PROPERTYOWNER.str.replace('\\s*<BR>\\s*', ' & ', regex=True)\n",
    "    return r_df\n",
    "\n",
    "# More cleanup after merge\n",
    "def cleanup_merged_accdb_import(df, year):\n",
    "    # A common pattern of bad addresses is ' , -0000 ', get rid of ones like this\n",
    "    bad_owner_addr = df.owner_address_raw.str.contains('^[\\s,]*-\\d\\d\\d\\d\\s*$',regex=True)\n",
    "    r_df = df[~(bad_owner_addr)]\n",
    "    print \"After filtering to remove malformed owner_address_raw addresses, size = %d\" % (len(r_df))\n",
    "    return r_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "apd_2015_raw = import_accdb_file('2015') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apd_2015 = cleanup_accdb_import(apd_2015_raw, '2015')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_addrs(apd_2015)\n",
    "canonicalize_addrs(apd_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out apd_2015 to mdf\n",
    "save_adf(apd_2015, 'apd_2015')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "process_all_assessment_records(apd_2015, '2015-05-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "apd_2014_raw = import_accdb_file('2014') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "apd_2014 = cleanup_accdb_import(apd_2014_raw, '2014')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_addrs(apd_2014)\n",
    "canonicalize_addrs(apd_2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "process_all_assessment_records(apd_2014, '2014-06-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "apd_2013_raw = import_accdb_file('2013') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "apd_2013 = cleanup_accdb_import(apd_2013_raw, '2013')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_addrs(apd_2013)\n",
    "canonicalize_addrs(apd_2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out apd_2013 to mdf\n",
    "save_adf(apd_2013, 'apd_2013')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "process_all_assessment_records(apd_2013, '2013-12-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  SALEDATE field is MMDDYYYY, so we need to do some more work than usual:\n",
    "#    9042007  = '2007-09-04'\n",
    "\n",
    "# Setup regular expression for parsing mmddyyyy date\n",
    "mmddyyyy_re = re.compile('(\\d\\d)(\\d\\d)(\\d\\d\\d\\d)')\n",
    "\n",
    "def mmddyyyy_to_yyyymmdd(datestr_raw):\n",
    "    # Parse the sections of the date string\n",
    "    m = mmddyyyy_re.match(str(datestr_raw))\n",
    "    # The month will be in the first group, the day in the second, the year in the third\n",
    "    return '%s-%s-%s'%(m.group(3),m.group(1),m.group(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we've already processed before and haven't changed process, just reload\n",
    "apd_2012 = restore_adf('apd_2012')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Otherwise, process afresh\n",
    "apd_2012_raw = import_accdb_file('2012') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "apd_2012 = cleanup_accdb_import(apd_2012_raw, '2012')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of empty sale dates and convert to yyyy-mm-dd format\n",
    "apd_2012=apd_2012[~pandas.isna(apd_2012.SALEDATE_raw)].copy()\n",
    "apd_2012['SALEDATE'] = apd_2012['SALEDATE_raw'].map(mmddyyyy_to_yyyymmdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_addrs(apd_2012)\n",
    "canonicalize_addrs(apd_2012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apd_2012=cleanup_merged_accdb_import(apd_2012, '2012')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "process_all_assessment_records(apd_2012, '2012-11-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process 2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore apd_2011 from mdf if we don't want to reprocess from scratch\n",
    "apd_2011 = restore_adf('apd_2011')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "apd_2011_raw = import_accdb_file('2011') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apd_2011 = cleanup_accdb_import(apd_2011_raw, '2011')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_addrs(apd_2011)\n",
    "canonicalize_addrs(apd_2011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out apd_2011 to mdf\n",
    "save_adf(apd_2011, 'apd_2011')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "process_all_assessment_records(apd_2011, '2011-03-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apd_2011.loc[['0174E00236000000','0174E00234000000','0029H00025000000']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacant_col1_2011 = (apd_2011.FAIRMARKETBUILDING<2.0)\n",
    "vacant_col2_2011 = ((apd_2011['USEDESC'].fillna('').str.contains('VACANT')) | (apd_2011['USEDESC'].fillna('').str.contains('UNLOCATED PARCEL')) | (apd_2011['USEDESC'].fillna('').str.contains('CONDEMNED')))\n",
    "vacant_col_2011=(vacant_col1_2011 | vacant_col2_2011)\n",
    "apd_2011[vacant_col_2011]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacant_col_2011.name='is_vacant'\n",
    "apd_2011 = pd.concat([apd_2011, vacant_col_2011], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = apd_2011\n",
    "fixup_cols = ['property_address_raw', 'owner_address_raw', 'property_address', \n",
    "              'owner_address','USEDESC','SALEDESC','HOMESTEADFLAG']\n",
    "for col in fixup_cols:\n",
    "    # Fix up to make sure there aren't differences in whitespace that could be avoided\n",
    "    df[col] = df[col].str.replace('\\s\\s+', ' ', regex=True)\n",
    "    df[col] = df[col].str.replace('^\\s+', '', regex=True)\n",
    "    df[col] = df[col].str.replace('\\s+$', '', regex=True)\n",
    "\n",
    "apd_2011=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace zip+4 with zip\n",
    "zip_p4_re = re.compile('(.*)\\s(\\d\\d\\d\\d\\d)-?(\\d\\d\\d\\d)?$')\n",
    "\n",
    "def zip_p4_to_zip(zipstr):\n",
    "    # Parse the sections of the zipcode\n",
    "    m = zip_p4_re.match(str(zipstr))\n",
    "    if(m):\n",
    "        # The zip will be in the second group, the +4 in the third\n",
    "        return \"%s %s\"%(m.group(1), m.group(2))\n",
    "    else:\n",
    "        return(zipstr)\n",
    "\n",
    "apd_2011['owner_address_raw'] = apd_2011['owner_address_raw'].map(zip_p4_to_zip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of empty sale dates\n",
    "apd_2011=apd_2011[~pd.isna(apd_2011.SALEDATE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apd_2011[apd_2011.SALEDATE.fillna('').str.contains('^[^01]', regex=True)][['SALEDATE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apd_2011[~apd_2011.SALEDATE.str.contains('^\\d\\d/[0123]\\d/\\d\\d', regex=True)][['SALEDATE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apd_2011[apd_2011.SALEDATE.str.contains('^00/\\d\\d/\\d\\d', regex=True)][['SALEDATE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apd_2011[apd_2011.SALEDATE.str.contains('^\\d\\d/00/\\d\\d', regex=True)][['SALEDATE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_map[apd_2011.iloc[0].name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_map[apd_2011.iloc[0].name]=[{'asofdate': '2001-02-15',\n",
    "      'canonical_owner_address': u'362 INDIAN RIDGE DR, CORAOPOLIS, PA, 15108',\n",
    "      'changeaddr': '362 INDIAN RIDGE DR CORAOPOLIS PA 15108',\n",
    "      'date': '1994-12-12',\n",
    "      'event_type': 'PURCHASE',\n",
    "      'fm_total': 207800,\n",
    "      'is_vacant': False,\n",
    "      'ownername': 'IMBROGNO PATRICK M & DENISE E (W)',\n",
    "      'saleprice': 210025.0},\n",
    "     {'asofdate': '2005-08-28',\n",
    "      'canonical_owner_address': u'362 INDIAN RIDGE DR, CORAOPOLIS, PA, 15108',\n",
    "      'changeaddr': '362 INDIAN RIDGE DR CORAOPOLIS PA 15108',\n",
    "      'date': '1994-12-12',\n",
    "      'event_type': 'PURCHASE',\n",
    "      'fm_total': 247700,\n",
    "      'is_vacant': False,\n",
    "      'ownerdesc': 'REGULAR-ETUX OR ET VIR',\n",
    "      'ownername': 'IMBROGNO PATRICK M & DENISE E (W)',\n",
    "      'saledesc': 'VALID SALE',\n",
    "      'saleprice': 210025.0},\n",
    "     {'asofdate': '2009-09-01',\n",
    "      'canonical_owner_address': u'362 INDIAN RIDGE DR, CORAOPOLIS, PA, 15108',\n",
    "      'changeaddr': ' 362 INDIAN RIDGE DR CORAOPOLIS, PA 15108-1374 ',\n",
    "      'date': u'1994-12-12',\n",
    "      'event_type': 'PURCHASE',\n",
    "      'fm_total': 247700.0,\n",
    "      'homesteadflag': 'C',\n",
    "      'is_vacant': False,\n",
    "      'ownerdesc': 'Regular',\n",
    "      'ownername': 'IMBROGNO PATRICK M & DENISE E (W)',\n",
    "      'saledesc': 'VALID SALE',\n",
    "      'saleprice': 210025.0},\n",
    "     {'asofdate': '2010-06-01',\n",
    "      'canonical_owner_address': u'362 INDIAN RIDGE DR, CORAOPOLIS, PA, 15108',\n",
    "      'changeaddr': ' 362 INDIAN RIDGE DR CORAOPOLIS, PA 15108-1374 ',\n",
    "      'date': u'1994-12-12',\n",
    "      'event_type': 'PURCHASE',\n",
    "      'fm_total': 247700.0,\n",
    "      'homesteadflag': 'C',\n",
    "      'is_vacant': False,\n",
    "      'ownerdesc': 'Regular',\n",
    "      'ownername': 'IMBROGNO PATRICK M & DENISE E (W)',\n",
    "      'saledesc': 'VALID SALE',\n",
    "      'saleprice': 210025.0},\n",
    "     {'asofdate': '2017-10-01',\n",
    "      'canonical_owner_address': u'362 INDIAN RIDGE DR, CORAOPOLIS, PA, 15108',\n",
    "      'changeaddr': '362 INDIAN RIDGE DR CORAOPOLIS PA 15108',\n",
    "      'date': u'1994-12-12',\n",
    "      'event_type': 'PURCHASE',\n",
    "      'fm_total': 345300,\n",
    "      'homesteadflag': 'HOM',\n",
    "      'is_vacant': False,\n",
    "      'ownerdesc': 'REGULAR-ETUX OR ET VIR',\n",
    "      'ownername': 'IMBROGNO PATRICK M & DENISE E (W)',\n",
    "      'saledesc': 'VALID SALE',\n",
    "      'saleprice': 210025.0},\n",
    "     {'asofdate': '2018-01-22',\n",
    "      'canonical_owner_address': u'362 INDIAN RIDGE DR, CORAOPOLIS, PA, 15108',\n",
    "      'changeaddr': '362 INDIAN RIDGE DR CORAOPOLIS PA 15108',\n",
    "      'date': u'1994-12-12',\n",
    "      'event_type': 'PURCHASE',\n",
    "      'fm_total': 345300,\n",
    "      'homesteadflag': 'HOM',\n",
    "      'is_vacant': False,\n",
    "      'ownername': 'IMBROGNO PATRICK M & DENISE E (W)',\n",
    "      'saledesc': 'VALID SALE',\n",
    "      'saleprice': 210025.0,\n",
    "      'usedesc': 'SINGLE FAMILY'}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_assessment_record(apd_2011, 0, '2011-03-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore apd_2010 from mdf if we don't want to reprocess from scratch\n",
    "apd_2010 = restore_adf('apd_2010')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "apd_2010_raw = import_accdb_file('2010') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "apd_2010 = cleanup_accdb_import(apd_2010_raw, '2010')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_addrs(apd_2010)\n",
    "canonicalize_addrs(apd_2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, load apd_2010 from storage\n",
    "apd_2010 = restore_adf('apd_2010')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to save out property_map before we start messsing with it\n",
    "import pickle\n",
    "\n",
    "with open('assessments/property_map_99_04_05_09_17.pickle', 'wb') as handle:\n",
    "    pickle.dump(property_map, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('assessments/owner_map_99_04_05_09_17.pickle', 'wb') as handle:\n",
    "    pickle.dump(owner_map, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('assessments/changeaddr_map_99_04_05_09_17.pickle', 'wb') as handle:\n",
    "    pickle.dump(changeaddr_map, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('assessments/canonical_property_address_map_99_04_05_09_17.pickle', 'wb') as handle:\n",
    "    pickle.dump(canonical_property_address_map, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('assessments/raw_property_address_map_99_04_05_09_17.pickle', 'wb') as handle:\n",
    "    pickle.dump(raw_property_address_map, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out apd_2010 to mdf\n",
    "save_adf(apd_2010, 'apd_2010')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "process_all_assessment_records(apd_2010, '2010-06-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following are historical fixups I had to do for the 2010 data set, but which shouldn't need to be done again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_rename_2010_augment = {'FairMarketLand':'FAIRMARKETLAND',\n",
    "'FairMarketBuilding':'FAIRMARKETBUILDING'}\n",
    "apd_2010= apd_2010.rename(index=str, columns=col_rename_2010_augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apd_2010.loc[['0174E00236000000','0174E00234000000','0029H00025000000']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacant_col1_2010 = (apd_2010.FAIRMARKETBUILDING<2.0)\n",
    "vacant_col2_2010 = ((apd_2010['USEDESC'].fillna('').str.contains('VACANT')) | (apd_2010['USEDESC'].fillna('').str.contains('UNLOCATED PARCEL')) | (apd_2010['USEDESC'].fillna('').str.contains('CONDEMNED')))\n",
    "vacant_col_2010=(vacant_col1_2010 | vacant_col2_2010)\n",
    "apd_2010[vacant_col_2010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacant_col_2010.name='is_vacant'\n",
    "apd_2010 = pd.concat([apd_2010, vacant_col_2010], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process 2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore apd_2009 from mdf if we don't want to reprocess from scratch\n",
    "apd_2009 = restore_adf('apd_2009')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "apd_2009_raw = import_accdb_file('2009') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apd_2009 = cleanup_accdb_import(apd_2009_raw, '2009')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_addrs(apd_2009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apd_2009=cleanup_merged_accdb_import(apd_2009, '2009')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canonicalize_addrs(apd_2009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "process_all_assessment_records(apd_2009, '2009-09-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(apd_2009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out apd_2009 to mdf\n",
    "save_adf(apd_2009, 'apd_2009')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following are historical fixups I had to do for the 2009 data set, but which shouldn't need to be done again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_rename_2009_augment = {'FAIRMARK_1':'FAIRMARKETLAND',\n",
    "'FAIRMARKET':'FAIRMARKETBUILDING'}\n",
    "apd_2009= apd_2009.rename(index=str, columns=col_rename_2009_augment).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apd_2009.loc[['0174E00236000000','0174E00234000000','0029H00025000000']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacant_col1_2009 = (apd_2009.FAIRMARKETBUILDING<2.0)\n",
    "vacant_col2_2009 = ((apd_2009['USEDESC'].fillna('').str.contains('VACANT')) | (apd_2009['USEDESC'].fillna('').str.contains('UNLOCATED PARCEL')) | (apd_2009['USEDESC'].fillna('').str.contains('CONDEMNED')))\n",
    "vacant_col_2009=(vacant_col1_2009 | vacant_col2_2009)\n",
    "apd_2009[vacant_col_2009]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacant_col_2009.name='is_vacant'\n",
    "apd_2009 = pd.concat([apd_2009, vacant_col_2009], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After input processing is done, register all sales so we know when owners stop owning given parcels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start=arrow.now()\n",
    "register_all_sales()\n",
    "end=arrow.now()\n",
    "print \"Processing took %s\" % (str(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(property_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(owner_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_map['9946X83377000000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owner_map['ANDREW ALAN S & SUSAN B (W)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(changeaddr_map['PO BOX 8469 CANTON OH 44711'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "property_map['0029C00050000000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "property_map['2018H00310000000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_map['0028H00014000000']=[{'changeaddr': '3535 BLVD OF THE ALLIES, PITTSBURGH, PA 15213',\n",
    "  'date': u'1986-08-25',\n",
    "  'event_type': 'PURCHASE',\n",
    "  'ownername': 'KELLY ROBERT E JR & KELLY ROBERT E JR WILLIAM W RIELLY ROBERT B NELL JR & HOMER E STOTLER'},\n",
    " {'changeaddr': '3535 BLVD OF THE ALLIES, PITTSBURGH, PA 15213',\n",
    "  'date': u'1986-08-25',\n",
    "  'event_type': 'PURCHASE',\n",
    "  'ownername': 'KELLY ROBERT E JR & RIELLY WILLIAM W & NELL ROBE'},\n",
    " {'changeaddr': '3535 BLVD OF THE ALLIES, PITTSBURGH, PA 15213',\n",
    "  'date': u'2012-12-18',\n",
    "  'event_type': 'PURCHASE',\n",
    "  'ownername': 'ROBERT E KELLY JR REVOCABLE TRUST'},\n",
    " {'changeaddr': '3535 BLVD OF THE ALLIES, PITTSBURGH, PA 15213',\n",
    "  'date': u'2012-12-18',\n",
    "  'event_type': 'PURCHASE',\n",
    "  'ownername': 'ROBERT E KELLY JR REVOCABLE TRUST & WILLIAM W RIELLY REVOCABLE TRU'},\n",
    " {'changeaddr': '3535 BLVD OF THE ALLIES, PITTSBURGH, PA 15213',\n",
    "  'date': u'2012-12-18',\n",
    "  'event_type': 'PURCHASE',\n",
    "  'ownername': 'ROBERT E KELLY JR REVOCABLE TRUST ROBERT E KELLY JR TRUSTEE & WIL'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owner_map['ROBERT E KELLY JR REVOCABLE TRUST ROBERT E KELLY JR TRUSTEE & WIL']=owner_map['ROBERT E KELLY JR REVOCABLE TRUST ROBERT E KELLY JR \\xce\\x93\\xc3\\x87\\xc3\\xb4 TRUSTEE & WIL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "changeaddr_map['8 WATSON ST, CARNEGIE, PA 15106']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "owner_map['STANDARD REALTY GROUP LP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sanity check it's working\n",
    "next_date = '1970-01-01'\n",
    "parid='0084J00135000000'\n",
    "print property_map[parid]\n",
    "while(next_date<this_date):\n",
    "    ret=get_related_parids(parid, next_date)\n",
    "    print \"%s: %s - %s = %d\" % (parid, ret['start_date'], ret['next_date'], ret['parcount'])\n",
    "    next_date=ret['next_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TORKEO ROBERT V\n",
    "next_date = '1970-01-01'\n",
    "parid='0029H00175000000'\n",
    "print property_map[parid]\n",
    "while(next_date<this_date):\n",
    "    ret=get_related_parids(parid, next_date)\n",
    "    print \"%s: %s - %s = %d\" % (parid, ret['start_date'], ret['next_date'], ret['parcount'])\n",
    "    next_date=ret['next_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print property_map.keys()[1]\n",
    "property_map[property_map.keys()[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apd_2009[apd_2009.PROPERTYOWNER.str.contains('WYDRENSKI ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apd_2009.loc['0163B00066000000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apd_2017.loc['0162D00020000000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apd_2009[~apd_2009.index.str.contains('000000$',regex=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apd_2009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process geometries to be able to record lat/lon for properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#with open(\"assessments/Allegheny_County_Parcel_Boundaries.geojson\") as f:\n",
    "#    parcel_json = json.load(f)\n",
    "#len(parcel_json['features'])\n",
    "g = gpd.read_file('assessments/Allegheny_County_Parcel_Boundaries.geojson')\n",
    "\n",
    "g=g.set_index('PIN')\n",
    "\n",
    "# Calculate the centroid for each row and store in a new column called centroid\n",
    "g['centroid']=g['geometry'].centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a map from PARID to centroid\n",
    "parid2centroid = {g.index[i]:g['centroid'].iloc[i] for i in range(0, len(g.index))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a list of parcel ids, add a centroid to parid2centroid from the property address if we have a good one\n",
    "def update_centroids(parid_arr):\n",
    "    global canonical_property_address_map\n",
    "    global raw_property_address_map\n",
    "    for parid in parid_arr:\n",
    "        # Check if we already know this centroid\n",
    "        if(parid in parid2centroid):\n",
    "            continue\n",
    "        # Don't know the centroid yet.  Look it up\n",
    "        if parid in canonical_property_address_map:\n",
    "            addr=canonical_property_address_map[parid]\n",
    "            coords = get_canonical_coords(addr)\n",
    "            if(coords):\n",
    "                print \"Updating %s from stored canonical addr: %r (%f, %f)\" % (parid, addr, coords.x, coords.y)\n",
    "                parid2centroid[parid]=coords\n",
    "                continue\n",
    "        if parid in raw_property_address_map:\n",
    "            raw_addr = raw_property_address_map[parid]\n",
    "            cp_addr = update_canonical_address_from_raw(None, raw_addr, False)\n",
    "            # Cache for next time\n",
    "            canonical_property_address_map[parid]=cp_addr\n",
    "            coords = get_canonical_coords(cp_addr)\n",
    "            if(coords):\n",
    "                print \"Updating %s from raw: %r -> %r (%f, %f)\" % (parid, raw_addr, cp_addr, coords.x, coords.y)\n",
    "                parid2centroid[parid]=coords\n",
    "                continue\n",
    "            print \"Skipping %s: Raw property addr = %r, %r\" % (parid, raw_addr, cp_addr)\n",
    "        else:\n",
    "            print \"Skipping %s: No raw property addr\" % (parid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do fixup on centroids\n",
    "redo_propaddr_parids=['0023B00228000000','0083R00164000000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_property_address_map['0023B00228000000']='1641 PERRYSVILLE AVE PITTSBURGH PA 15212'\n",
    "raw_property_address_map['0083R00164000000']='340 N SHERIDAN AVE PITTSBURGH PA 15206'\n",
    "for parid in redo_propaddr_parids:\n",
    "    ca = update_canonical_address_from_raw(None, raw_property_address_map[parid], False)\n",
    "    if(not canonical_addresses_empty(ca)):\n",
    "        canonical_property_address_map[parid]=ca\n",
    "    elif(parid in canonical_property_address_map):\n",
    "        del canonical_property_address_map[parid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parid in redo_propaddr_parids:\n",
    "    print \"%s: %r\" % (parid,canonical_property_address_map[parid] if parid in canonical_property_address_map else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_centroids(redo_propaddr_parids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_centroid_parids = [line.rstrip('\\n') for line in open('assessments/ownerprox_99_04_05_09_17_missing.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parid in bad_centroid_parids:\n",
    "    print \"%s: %r\" % (parid,canonical_property_address_map[parid] if parid in canonical_property_address_map else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_centroids(bad_centroid_parids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New volume animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def color_from_floats(r,g,b):\n",
    "    return r + g * 256.0 + b * 256.0 * 256.0\n",
    "\n",
    "def pack_color(color):\n",
    "    return color['r'] + color['g'] * 256.0 + color['b'] * 256.0 * 256.0;\n",
    "\n",
    "def parse_color(color):\n",
    "    color = color.strip()\n",
    "    c = color\n",
    "    try:\n",
    "        if c[0] == '#':\n",
    "            c = c[1:]\n",
    "        if len(c) == 3:\n",
    "            return pack_color({'r': 17 * int(c[0:1], 16),\n",
    "                               'g': 17 * int(c[1:2], 16),\n",
    "                               'b': 17 * int(c[2:3], 16)})\n",
    "        if len(c) == 6:\n",
    "            return pack_color({'r': int(c[0:2], 16),\n",
    "                               'g': int(c[2:4], 16),\n",
    "                               'b': int(c[4:6], 16)})\n",
    "    except:\n",
    "        pass\n",
    "    raise InvalidUsage('Cannot parse color <code><b>%s</b></code> from spreadsheet.<br><br>Color must be in standard web form, <code><b>#RRGGBB</b></code>, where RR, GG, and BB are each two-digit hexadecimal numbers between 00 and FF.<br><br>See <a href=\"https://www.w3schools.com/colors/colors_picker.asp\">HTML Color Picker</a>' % color)\n",
    "\n",
    "def parse_colors(colors):\n",
    "    packed = [parse_color(color) for color in colors]\n",
    "    return numpy.array(packed, dtype = numpy.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#out_suffix=\"_09_10_11_13_14_15_17\"\n",
    "#out_suffix=\"_15213_17\"\n",
    "#out_suffix=\"_99_09_17\"\n",
    "#out_suffix=\"_99_04b_09_17\"\n",
    "#out_suffix=\"_99b_04_05_09_17\"\n",
    "#out_suffix=\"_99_04_05_09_10_17\"\n",
    "#out_suffix=\"99_04c_05_09_10_17\"\n",
    "out_suffix=\"99b_04d_05_09_10_11_17_18b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start=arrow.now()\n",
    "\n",
    "# Use the property_map, owner_map, and changeaddr_map to generate a new type of volume animation\n",
    "# Write out a binary file with the volume colors\n",
    "vol_colors = ['#a50026','#cd2827','#e75436','#f7804b','#fdad61','#fed788','#ffffbf','#b9e0ed','#8dc0db','#699fca','#4d7db9','#3e5aa7','#313695']\n",
    "def volume_to_color(volume):\n",
    "    if (volume < 2):\n",
    "        return parse_color(vol_colors[0]) \n",
    "    #elif (volume < 3):\n",
    "    #    return parse_color(vol_colors[1]) \n",
    "    #elif (volume < 4):\n",
    "    #    return parse_color(vol_colors[2]) \n",
    "    elif (volume < 5):\n",
    "        return parse_color(vol_colors[3]) \n",
    "    elif (volume < 10):\n",
    "        return parse_color(vol_colors[4]) \n",
    "    elif (volume < 20):\n",
    "        return parse_color(vol_colors[5]) \n",
    "    elif (volume < 40):\n",
    "        return parse_color(vol_colors[6]) \n",
    "    elif (volume < 60):\n",
    "        return parse_color(vol_colors[7]) \n",
    "    elif (volume < 80):\n",
    "        return parse_color(vol_colors[8]) \n",
    "    elif (volume < 150):\n",
    "        return parse_color(vol_colors[9]) \n",
    "    elif (volume < 300):\n",
    "        return parse_color(vol_colors[10]) \n",
    "    elif (volume < 500):\n",
    "        return parse_color(vol_colors[11]) \n",
    "    else:\n",
    "        return parse_color(vol_colors[12]) \n",
    "\n",
    "def output_volume_dots():\n",
    "    # Write out volume of ownership for each residential non-vacant land property\n",
    "    points = []\n",
    "    i=0\n",
    "    start=arrow.now()\n",
    "    chunk_start_time=arrow.now()\n",
    "    chunk_size=1000\n",
    "\n",
    "    for parid in property_map.keys():\n",
    "        centroid=None\n",
    "        try:\n",
    "            centroid = parid2centroid[parid]\n",
    "        except:\n",
    "            print \"%s is missing from centroids, skipping\" % (parid)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Handle periodic debug message\n",
    "            if((i%chunk_size)==0 and i>0):\n",
    "                print \"%d-%d: processing %r, %s time elapsed\" %(i-(chunk_size-1), i, parid, arrow.now()-chunk_start_time)\n",
    "                addcnt=0\n",
    "                chunk_start_time=arrow.now()\n",
    "\n",
    "            # Nominally start at 1950\n",
    "            next_date = '1950-01-01'\n",
    "\n",
    "            while(next_date<this_date):\n",
    "                volume_map=get_related_parids(parid, next_date)\n",
    "\n",
    "                next_date=volume_map['next_date']\n",
    "                color = volume_to_color(volume_map['parcount'])\n",
    "                if((i%chunk_size)==0):\n",
    "                    print \"  %s: %s - %s = %d\" % (parid, volume_map['start_date'], volume_map['next_date'], volume_map['parcount'])\n",
    "                saledate = SaledateToEpoch(volume_map['start_date'])\n",
    "                enddate = SaledateToEpoch(next_date)\n",
    "                if(color != None):\n",
    "                    points += PointToPixelXY(centroid)     \n",
    "                    points.append(color)\n",
    "                    # Put epoch time for SALEDATE as start valid time, and next_date as end valid time\n",
    "                    points.append(float(saledate))\n",
    "                    points.append(float(enddate))\n",
    "                else:\n",
    "                    print \"Color of \" + str(volume_map['parcount']) + \" is None\"\n",
    "        except:\n",
    "            print \"Unexpected error processing %s, next_date=%s:\" % (parid, next_date), sys.exc_info()[0]\n",
    "            #raise\n",
    "\n",
    "        #Increment debug message counter\n",
    "        i=i+1\n",
    "\n",
    "    array.array('f', points).tofile(open(('assessments/res_volume_color_m_epoch%s.bin'%out_suffix), 'wb'))\n",
    "\n",
    "    end=arrow.now()\n",
    "    print \"Processing took %s\" % (str(end-start))\n",
    "    \n",
    "#import cProfile\n",
    "#cProfile.run('output_volume_dots()')\n",
    "output_volume_dots()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call this version if you're ok with potentially extra computation \n",
    "# It checks if there's a census entry. If not, it generates one.\n",
    "# If that fails, it checks if there's a google entry.  If not, it generates one.\n",
    "# it returns the resulting map\n",
    "def update_canonical_address_from_raw(ca_in, raw_addr, force_google):\n",
    "    # Check if we have a valid ca_in, if not create with census from raw_addr\n",
    "    if(pd.isnull(ca_in) or ('census' not in ca_in)):\n",
    "       # We don't have anything yet, call census on raw_addr\n",
    "       ca_in = {'census':get_canonical_address(raw_addr)}\n",
    "    # Check for value of census field\n",
    "    if(not force_google and ca_in['census']):\n",
    "       # Census is good, not forcing computation of google, stop there\n",
    "       return ca_in\n",
    "    # No census canonicalization, or force_google is True, check for value in google field\n",
    "    if('google' not in ca_in):\n",
    "        ca_in['google']=get_canonical_address_google(raw_addr)\n",
    "        print \"Checked google for %r: %r\" % (raw_addr, ca_in)\n",
    "    return(ca_in)\n",
    "\n",
    "# This version doesn't try to go deeper.  Call this if you don't want more computation to happen\n",
    "def canonical_addresses_match(ca1,ca2):\n",
    "    # Check if both are non-null\n",
    "    #print \"Comparing %r and %r\" % (ca1, ca2)\n",
    "    if(pd.isnull(ca1) or pd.isnull(ca2)):\n",
    "        # We don't know all we could know here in case there's a raw address available\n",
    "        #print \"   Incomplete: %r (%r vs %r)\" % (None, ca1, ca2)\n",
    "        return None\n",
    "    # Check if they both have census entries\n",
    "    if('census' in ca1 and 'census' in ca2 and ca1['census'] and ca2['census']):\n",
    "        ret= ca1['census']==ca2['census']\n",
    "        #print \"   Census: %r\" % (ret)\n",
    "        return (ret)\n",
    "    # One or both are missing census, try google\n",
    "    if('google' in ca1 and 'google' in ca2):\n",
    "        # Both have been computed, return false if either is empty or both are non-empty and they don't match\n",
    "        if(ca1['google'] and ca1['google']):\n",
    "            ret = ca1['google']==ca2['google']\n",
    "            #print \"   Google: %r\" % (ret)\n",
    "            return (ret)\n",
    "        else:\n",
    "            #print \"   Mismatch: %r\" % (False)\n",
    "            return False\n",
    "    # Can't determine equality, more updating might be needed return None\n",
    "    #print \"   Incomplete: %r (%r vs %r)\" % (None, ca1, ca2)\n",
    "    return None\n",
    "       \n",
    "# Returns True if both census and google have been tried and failed\n",
    "def canonical_addresses_empty(ca):\n",
    "    if(pd.isnull(ca) or (('census' in ca) and pd.isnull(ca['census'])) and (('google' in ca) and (pd.isnull(ca['google'])))):\n",
    "        return True\n",
    "    elif(('census' in ca) and not pd.isnull(ca['census'])):\n",
    "        return False\n",
    "    elif(('google' in ca) and not pd.isnull(ca['google'])):\n",
    "        return False\n",
    "    # More could be done here, return Null\n",
    "    return None                                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=arrow.now()\n",
    "\n",
    "single_property_color = '#a50026'\n",
    "corporate_color = '#ffffbf'\n",
    "unknown_color = '#303030'\n",
    "\n",
    "def ownertype_to_color(parid, property_event):\n",
    "    # Check if property and owner canonical addresses are both known\n",
    "    property_canonical_addr=get_canonical_property_addr(parid)\n",
    "    owner_canonical_addr=None\n",
    "    if('canonical_owner_address' in property_event.keys()):\n",
    "        owner_canonical_addr=property_event['canonical_owner_address']\n",
    "    # Check if both addresses are identical.  If so, use single_property_color\n",
    "    # since it's owner occupied, even if the owner is a multiple or corporate owner\n",
    "    # in general\n",
    "    if(property_canonical_addr and owner_canonical_addr and canonical_addresses_match(property_canonical_addr,owner_canonical_addr)):\n",
    "        return single_property_color\n",
    "\n",
    "    ownerdesc = 'REGULAR'\n",
    "    if('ownerdesc' in property_event.keys()):\n",
    "        ownerdesc = property_event['ownerdesc']\n",
    "    volume_map=get_related_parids(parid, property_event['date'])\n",
    "    volume = volume_map['parcount']\n",
    "    if ('CORPORATION' in ownerdesc or 'Corporation' in ownerdesc):\n",
    "        # Ivory \n",
    "        return corporate_color\n",
    "    elif ('homesteadflag' in property_event.keys() and \n",
    "          (property_event['homesteadflag'].strip()=='HOM' or property_event['homesteadflag'].strip()=='C')):\n",
    "        # If homestead flag set, use single property color regardless of volume\n",
    "        return single_property_color\n",
    "    elif ('REGULAR' in ownerdesc or 'Regular' in ownerdesc):\n",
    "        # Regular owner, what volume?\n",
    "        if(volume == 1):\n",
    "            # Same color as 1 in volume view\n",
    "            return single_property_color\n",
    "        else:\n",
    "            # Same color as other in class view\n",
    "            return '#02ca75'\n",
    "    else:\n",
    "        print \"Unrecognized owner type: %s, %d\" % (ownerdesc,volume)\n",
    "        return unknown_color\n",
    "\n",
    "def output_ownertype_dots(parid_arr, suffix):\n",
    "    # If parid_arr not specified, do all of the keys in property_map\n",
    "    if(len(parid_arr)==0):\n",
    "        parid_arr=property_map.keys()\n",
    "        \n",
    "    # Write out volume of ownership for each residential non-vacant land property\n",
    "    points = []\n",
    "    start=arrow.now()\n",
    "    chunk_start_time=arrow.now()\n",
    "    chunk_size=1000\n",
    "    chunk_cnt=0\n",
    "    \n",
    "    for parid in parid_arr:\n",
    "        centroid=None\n",
    "        did_output_dot=False\n",
    "        try:\n",
    "            centroid = parid2centroid[parid]\n",
    "        except:\n",
    "            print \"%s is missing from centroids, skipping\" % (parid)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Handle periodic debug message\n",
    "            if((chunk_cnt%chunk_size)==0 and chunk_cnt>0):\n",
    "                print \"%d-%d: processing %r, %s time elapsed\" %(chunk_cnt-(chunk_size-1), chunk_cnt, parid, arrow.now()-chunk_start_time)\n",
    "                addcnt=0\n",
    "                chunk_start_time=arrow.now()\n",
    "\n",
    "            # Get list of property events or this property\n",
    "            property_events = property_map[parid]\n",
    "\n",
    "            # Keep track of the date of the last datapoint. \n",
    "            last_date = '1900-01-01'\n",
    "            last_color=0\n",
    "            \n",
    "            for i in range(0,len(property_events)):\n",
    "                event_date = property_events[i]['date']\n",
    "                # Get color for this property event\n",
    "                color = ownertype_to_color(parid, property_events[i])\n",
    "                # Check if the date and color haven't changed, if so skip to the next event\n",
    "                if(color == last_color and last_date == event_date):\n",
    "                    # Nothing new here, move along\n",
    "                    continue\n",
    "                elif(last_date==event_date and i>0):\n",
    "                    # Color changed without the sale date changing, possibly flag this as an issue\n",
    "                    # If this new color is unknown, skip it\n",
    "                    # If the last color was unknown, delete it and use this one\n",
    "                    # Otherwise, this might be a change in homesteadflag.  If so, single_property_color takes precedence.\n",
    "                    # Corporate takes priority between multi and corporate\n",
    "                    # If neither has single_property_color, raise an exception\n",
    "                    if(color==unknown_color):\n",
    "                        # Ok, just skip this one\n",
    "                        continue\n",
    "                    elif(last_color==unknown_color):\n",
    "                        # Pop the previous point off (5 floats) and use this new one\n",
    "                        for j in range(0,5):\n",
    "                            del points[-1]\n",
    "                        #print \"  REMOVED %s: %s = %s (len %d)\" % (parid, event_date,last_color, len(points))\n",
    "                    elif(last_color==single_property_color):\n",
    "                        # Ok, just skip this one\n",
    "                        continue\n",
    "                    elif(color == single_property_color):\n",
    "                        # Pop the previous point off (5 floats) and use this new one\n",
    "                        for j in range(0,5):\n",
    "                            del points[-1]\n",
    "                        #print \"  REMOVED %s: %s = %s (len %d)\" % (parid, event_date,last_color, len(points))\n",
    "                    elif(last_color==corporate_color):\n",
    "                        # Ok, just skip this one\n",
    "                        continue\n",
    "                    elif(color==corporate_color):\n",
    "                        # Pop the previous point off (5 floats) and use this new one\n",
    "                        for j in range(0,5):\n",
    "                            del points[-1]\n",
    "                        #print \"  REMOVED %s: %s = %s (len %d)\" % (parid, event_date,last_color, len(points))\n",
    "                    else:\n",
    "                        print \" PROBLEM %s: %s changed color on same date %s->%s\" % (parid, event_date, last_color, color)\n",
    "                        raise\n",
    "                        \n",
    "                if((chunk_cnt%chunk_size)==0):\n",
    "                    print \"  %s: %s = %s\" % (parid, event_date, color)\n",
    "                \n",
    "                # Get range of dates for this color\n",
    "                saledate = SaledateToEpoch(event_date)\n",
    "                next_date = get_next_sale_date(parid, event_date)\n",
    "                if(next_date == None):\n",
    "                    # No new owner after this, set end date to the end of time\n",
    "                    enddate = float(1e38)\n",
    "                    next_date=this_date\n",
    "                else:\n",
    "                    enddate = SaledateToEpoch(next_date)\n",
    "                if(color != None):\n",
    "                    points += PointToPixelXY(centroid)     \n",
    "                    points.append(parse_color(color))\n",
    "                    # Put epoch time for SALEDATE as start valid time, and next_date as end valid time\n",
    "                    points.append(float(saledate))\n",
    "                    points.append(float(enddate))\n",
    "                    #print \"  %s: %s - %s = %s (len %d)\" % (parid, event_date, next_date, color, len(points))\n",
    "                else:\n",
    "                    print \"Color of \" + str(volume_map['parcount']) + \" is None\"\n",
    "\n",
    "                # Set last_date and last_color for next loop\n",
    "                last_date = event_date\n",
    "                last_color= color\n",
    "                did_output_dot=True\n",
    "        except:\n",
    "            print \"Unexpected error processing %s:\" % (parid), sys.exc_info()[0]\n",
    "            raise\n",
    "\n",
    "        #Increment debug message counter\n",
    "        if(did_output_dot):\n",
    "            chunk_cnt = chunk_cnt+1\n",
    "\n",
    "    array.array('f', points).tofile(open(('assessments/ownertype_color_m_epoch%s.bin'%suffix), 'wb'))\n",
    "\n",
    "    end=arrow.now()\n",
    "    print \"Processing took %s for %d dots\" % (str(end-start), chunk_cnt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cProfile\n",
    "#cProfile.run('output_volume_dots()')\n",
    "output_ownertype_dots([],out_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parids_15213=apd_2017[apd_2017.PROPERTYZIP=='15213'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_parids_15213=list(set(parids_15213) & set(property_map.keys()) & set(parid2centroid.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid_parids_15213)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_parids_15217=list(set(apd_15217_2017.index) & set(property_map.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid_parids_15217)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ownertype_dots(valid_parids_15213,\"_d4_15213\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=arrow.now()\n",
    "\n",
    "single_property_color = '#a50026'\n",
    "corporate_color = '#ffffbf'\n",
    "unknown_color = '#303030'\n",
    "\n",
    "# Distressed sales are:\n",
    "distressed_saledescs=['SHERIFF SALE', 'CITY TREASURER SALE','CITY TREASURER',\n",
    "                      'PREV FORECLOSE','GOVT SALE','BANK SALE']\n",
    "#affection_saledescs=['LOVE AND AFFECTION SALE','LOVE&AFFECTION']\n",
    "affection_saledescs=[]\n",
    "\n",
    "#distressed_colors=[\"#ff0000\",\"#00ff00\"]\n",
    "#affection_colors=[\"#FA00D4\",\"#15FA00\"]\n",
    "distressed_colors=[\"#FA00D4\",\"#15FA00\"]\n",
    "affection_colors=[]\n",
    "\n",
    "single_property_colors=[single_property_color, \"#ff0000\", \"#FA00D4\"]\n",
    "def saletype_to_color(parid, property_event):\n",
    "    try:\n",
    "        ownerdesc = 'REGULAR'\n",
    "        if('ownerdesc' in property_event.keys()):\n",
    "            ownerdesc = property_event['ownerdesc']\n",
    "        saledesc = 'VALID'\n",
    "        if('saledesc' in property_event.keys()):\n",
    "            saledesc = property_event['saledesc']\n",
    "        volume_map=get_related_parids(parid, property_event['date'])\n",
    "        volume = volume_map['parcount']\n",
    "\n",
    "        # Check if homestead flag is set\n",
    "        homesteadflag=False\n",
    "        if('homesteadflag' in property_event.keys() and \n",
    "          (property_event['homesteadflag'].strip()=='HOM' or property_event['homesteadflag'].strip()=='C')):\n",
    "            homesteadflag=True\n",
    "        # Check if property went to a single owner or not\n",
    "        single_owner = True\n",
    "        if ('CORPORATION' in ownerdesc or 'Corporation' in ownerdesc or (volume>1 and not homesteadflag)):\n",
    "            single_owner=False\n",
    "        \n",
    "        \n",
    "        # Check for distressed sale\n",
    "        if(saledesc in distressed_saledescs):\n",
    "            # Is distressed sale, check if it went to a single owner or not\n",
    "            if(single_owner):\n",
    "                # Set to bright pink\n",
    "                return \"#FA00D4\"\n",
    "            else:\n",
    "                # Return fluorescent green\n",
    "                return \"#15FA00\"\n",
    "#            if(single_owner):\n",
    "#                # Set to bright red\n",
    "#                return \"#ff0000\"\n",
    "#            else:\n",
    "#                # Return bright green\n",
    "#                return \"#00ff00\"\n",
    "#        elif(saledesc in affection_saledescs):\n",
    "#            # Is affection sale, check if it went to a single owner or not\n",
    "#            if(single_owner):\n",
    "#                # Set to bright pink\n",
    "#                return \"#FA00D4\"\n",
    "#            else:\n",
    "#                # Return fluorescent green\n",
    "#                return \"#15FA00\"\n",
    "        else:\n",
    "            # Normal sale\n",
    "#            if ('CORPORATION' in ownerdesc or 'Corporation' in ownerdesc):\n",
    "#                # Ivory \n",
    "#                return '#ffffbf'\n",
    "            if (single_owner):\n",
    "                # Same color as 1 in volume view\n",
    "                return single_property_color\n",
    "            else:\n",
    "                # Non-corporate multi-owner, reguarl sale\n",
    "                # Same color as other in class view\n",
    "                return '#02ca75'\n",
    "    except:\n",
    "        print \"Exception processing saletype: %s\" % (parid), sys.exc_info()[0]\n",
    "        raise\n",
    "        return '#303030'\n",
    "\n",
    "def output_saletype_dots(parid_arr, suffix):\n",
    "    # If parid_arr not specified, do all of the keys in property_map\n",
    "    if(len(parid_arr)==0):\n",
    "        parid_arr=property_map.keys()\n",
    "    # Write out volume of ownership for each residential non-vacant land property\n",
    "    points = []\n",
    "    start=arrow.now()\n",
    "    chunk_start_time=arrow.now()\n",
    "    chunk_size=1000\n",
    "    chunk_cnt=0\n",
    "    \n",
    "    for parid in parid_arr:\n",
    "        centroid=None\n",
    "        did_output_dot=False\n",
    "        try:\n",
    "            centroid = parid2centroid[parid]\n",
    "        except:\n",
    "            print \"%s is missing from centroids, skipping\" % (parid)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Handle periodic debug message\n",
    "            if((chunk_cnt%chunk_size)==0 and chunk_cnt>0):\n",
    "                print \"%d-%d: processing %r, %s time elapsed\" %(chunk_cnt-(chunk_size-1), chunk_cnt, parid, arrow.now()-chunk_start_time)\n",
    "                addcnt=0\n",
    "                chunk_start_time=arrow.now()\n",
    "\n",
    "            # Get list of property events or this property\n",
    "            property_events = property_map[parid]\n",
    "\n",
    "            # Keep track of the date of the last datapoint. \n",
    "            last_date = '1900-01-01'\n",
    "            last_color=0\n",
    "            \n",
    "            for i in range(0,len(property_events)):\n",
    "                event_date = property_events[i]['date']\n",
    "                # Get color for this property event\n",
    "                color = saletype_to_color(parid, property_events[i])\n",
    "                # Check if the date and color haven't changed, if so skip to the next event\n",
    "                if(color == last_color and last_date == event_date):\n",
    "                    # Nothing new here, move along\n",
    "                    continue\n",
    "                elif(last_date==event_date and i>0):\n",
    "                    # Color changed without the sale date changing, possibly flag this as an issue\n",
    "                    # If this new color is unknown, skip it\n",
    "                    # If the last color was unknown, delete it and use this one\n",
    "                    # Otherwise, distressed_colors take precedence if present, \n",
    "                    # if not, this might be a change in homesteadflag.  If so, single_property_color takes precedence.\n",
    "                    # Corporate takes priority between multi and corporate\n",
    "                    # If neither has single_property_color, raise an exception\n",
    "                    if(color==unknown_color):\n",
    "                        # Ok, just skip this one\n",
    "                        continue\n",
    "                    elif(last_color==unknown_color):\n",
    "                        # Pop the previous point off (5 floats) and use this new one\n",
    "                        for j in range(0,5):\n",
    "                            del points[-1]\n",
    "                    elif(last_color in distressed_colors):\n",
    "                        # Ok, just skip this one\n",
    "                        continue\n",
    "                    elif(color in distressed_colors):\n",
    "                        # Pop the previous point off (5 floats) and use this new one\n",
    "                        for j in range(0,5):\n",
    "                            del points[-1]\n",
    "                    elif(last_color in affection_colors):\n",
    "                        # Ok, just skip this one\n",
    "                        continue\n",
    "                    elif(color in affection_colors):\n",
    "                        # Pop the previous point off (5 floats) and use this new one\n",
    "                        for j in range(0,5):\n",
    "                            del points[-1]\n",
    "                    elif(color in single_property_colors):\n",
    "                        # Pop the previous point off (5 floats) and use this new one\n",
    "                        for j in range(0,5):\n",
    "                            del points[-1]\n",
    "                    elif(last_color in single_property_colors):\n",
    "                        # Ok, just skip this one\n",
    "                        continue\n",
    "                    elif(last_color==corporate_color):\n",
    "                        # Ok, just skip this one\n",
    "                        continue\n",
    "                    elif(color==corporate_color):\n",
    "                        # Pop the previous point off (5 floats) and use this new one\n",
    "                        for j in range(0,5):\n",
    "                            del points[-1]\n",
    "                    else:\n",
    "                        print \" PROBLEM %s: %s changed color on same date %s->%s\" % (parid, event_date, last_color, color)\n",
    "                        raise\n",
    "\n",
    "                if((chunk_cnt%chunk_size)==0):\n",
    "                    print \"  %s: %s = %s\" % (parid, event_date, color)\n",
    "                \n",
    "                # Get range of dates for this color\n",
    "                saledate = SaledateToEpoch(event_date)\n",
    "                next_date = get_next_sale_date(parid, event_date)\n",
    "                if(next_date == None):\n",
    "                    # No new owner after this, set end date to the end of time\n",
    "                    enddate = float(1e38)\n",
    "                    next_date=this_date\n",
    "                else:\n",
    "                    enddate = SaledateToEpoch(next_date)\n",
    "                if(color != None):\n",
    "                    points += PointToPixelXY(centroid)     \n",
    "                    points.append(parse_color(color))\n",
    "                    # Put epoch time for SALEDATE as start valid time, and next_date as end valid time\n",
    "                    points.append(float(saledate))\n",
    "                    points.append(float(enddate))\n",
    "                    #print \"  %s: %s - %s = %s (len %d)\" % (parid, event_date, next_date, color, len(points))\n",
    "                else:\n",
    "                    print \"Color of \" + str(volume_map['parcount']) + \" is None\"\n",
    "\n",
    "                # Set last_date and last_color for next loop\n",
    "                last_date = event_date\n",
    "                last_color= color\n",
    "                did_output_dot=True\n",
    "        except:\n",
    "            print \"Unexpected error processing %s:\" % (parid), sys.exc_info()[0]\n",
    "            raise\n",
    "\n",
    "        #Increment debug message counter\n",
    "        if(did_output_dot):\n",
    "            chunk_cnt = chunk_cnt+1\n",
    "\n",
    "    array.array('f', points).tofile(open(('assessments/saletype_color_m_epoch%s.bin'%suffix), 'wb'))\n",
    "\n",
    "    end=arrow.now()\n",
    "    print \"Processing took %s for %d dots\" % (str(end-start), chunk_cnt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_saletype_dots([],out_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parid='0121K00245000000'\n",
    "print canonical_property_address_map[parid]\n",
    "print property_map[parid]\n",
    "for j in range(0,len(property_map[parid])):\n",
    "    print '%r: %s\\n' % (property_map[parid][j], ownerprox_to_color(parid, property_map[parid][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'#a50026'  in single_property_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_saletype_dots(['0028L00264000000'],\"_d1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_saletype_dots(valid_parids_15213,\"_15213\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Owner proximity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cache of zip code to proximity. \n",
    "# If zip is in PA, but not Allegheny county, value will be 'PA'\n",
    "# If zip is in Allegheny county, value will be 'Allegheny'\n",
    "zip_map={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in US zips, subset on PA\n",
    "us_zips = pd.read_csv(\"allegheny_county/zip/us-national-zips-2010.csv\",dtype={'zipcode':numpy.str})\n",
    "pa_zips = us_zips[us_zips.state=='PA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for zipcode in pa_zips['zipcode']:\n",
    "    zip_map[zipcode]='PA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Allegheny county zipcodes adapted from https://clicknathan.com/2012/10/10/csv-of-all-zip-codes-in-allegheny-county-and-pittsburgh/\n",
    "ac_zips = [15006,15007,15101, 15006,15102,15014,15104,15015,15017,15106,15024,15025,15026,15108,15030,15046,15034,15110,15035,15112,15037,15044,15045,15116,15120,15126,15056,15129,15136,15130,15131,15132,15133,15135,15146,15064,15065,15137,15071,15139,15140,15201,15202,15203,15204,15205,15206,15207,15208,15209,15210,15211,15212,15213,15214,15215,15216,15217,15218,15219,15220,15221,15222,15223,15224,15225,15226,15227,15228,15229,15232,15233,15234,15235,15236,15237,15238,15239,15241,15242,15243,15275,15276,15142,15076,15143,15144,15084,15145,15147,15086,15122,15123,15090,15148]\n",
    "for zipcode in ac_zips:\n",
    "    zip_map[str(zipcode)]='Allegheny'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_map['0029G00190000000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parid = '0062G00184000000'\n",
    "sevs = get_simultaneous_property_events(property_map[parid],0)\n",
    "colors = map(lambda ev: ownerprox_to_color(parid, ev), sevs)\n",
    "colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a set for accumulating parids for ownerprox to skip since we don't want duplicates\n",
    "# This is an optimization.  Re-execute if new entries are added to parid2centroid or property address related maps\n",
    "ownerprox_skip_parids=set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=arrow.now()\n",
    "\n",
    "vol_colors = ['#a50026','#cd2827','#e75436','#f7804b','#fdad61','#fed788','#ffffbf','#b9e0ed','#8dc0db','#699fca','#4d7db9','#3e5aa7','#313695']\n",
    "\n",
    "sameaddr_color = vol_colors[0]\n",
    "samezip_color = vol_colors[3]\n",
    "allegheny_co_color=vol_colors[6]\n",
    "in_state_color=vol_colors[8]\n",
    "out_of_state_color=vol_colors[11]\n",
    "\n",
    "unknown_color = '#303030'\n",
    "\n",
    "def ownerprox_to_color(parid, property_event):\n",
    "    global canonical_property_address_map\n",
    "    global raw_property_address_map\n",
    "    global property_map\n",
    "\n",
    "    # Record which sort of canonical address we have, as they can't be\n",
    "    # compared against each other\n",
    "    use_census_addrs=True\n",
    "    \n",
    "    # Check if property and owner canonical addresses are both known\n",
    "    # to census\n",
    "    property_canonical_addr=get_canonical_property_addr(parid)\n",
    "    owner_canonical_addr=None\n",
    "    if('canonical_owner_address' in property_event.keys()):\n",
    "        owner_canonical_addr=property_event['canonical_owner_address']\n",
    "    # if we already know enough to compare them.  If so, this returns True or False.\n",
    "    # If not, it returns None and we potentially need to do updating from raw\n",
    "    #print \"  %r: Comparing (%r, %r)\" % (parid, property_canonical_addr, owner_canonical_addr)\n",
    "    addrs_match = canonical_addresses_match(property_canonical_addr,owner_canonical_addr)\n",
    "    if(pd.isnull(addrs_match)):\n",
    "        # Don't know enough to answer, potentially update both\n",
    "        #print \"  %r: Canonical address comparison requires more processing (%r, %r)\" % (parid, property_canonical_addr, owner_canonical_addr)\n",
    "        # Missing something, try census first\n",
    "        property_canonical_addr = update_canonical_address_from_raw(property_canonical_addr, raw_property_address_map[parid], False)\n",
    "        owner_canonical_addr = update_canonical_address_from_raw(owner_canonical_addr, property_event['changeaddr'], False)\n",
    "        \n",
    "        # Check if it was enough to not force google processing\n",
    "        addrs_match = canonical_addresses_match(property_canonical_addr,owner_canonical_addr)\n",
    "        if(pd.isnull(addrs_match)):\n",
    "            print \"  %r: Forcing google address processing(%r, %r)\" % (parid, property_canonical_addr, owner_canonical_addr)\n",
    "            property_canonical_addr = update_canonical_address_from_raw(property_canonical_addr, raw_property_address_map[parid], True)\n",
    "            owner_canonical_addr = update_canonical_address_from_raw(owner_canonical_addr, property_event['changeaddr'], True)\n",
    "            # Check one more time after forcing google processing\n",
    "            addrs_match = canonical_addresses_match(property_canonical_addr,owner_canonical_addr)\n",
    "        \n",
    "        # Cache result for next time\n",
    "        canonical_property_address_map[parid] = property_canonical_addr\n",
    "        # This is messy bcause property_event is a copy and we need to modify the global version\n",
    "        pm_update_index = find_event_index_by_date_and_asofdate(property_map[parid], property_event['date'], property_event['asofdate'])\n",
    "        if(pm_update_index>=0 and property_map[parid][pm_update_index]['date']==property_event['date'] and property_map[parid][pm_update_index]['asofdate']==property_event['asofdate']):\n",
    "            property_map[parid][pm_update_index]['canonical_owner_address'] = owner_canonical_addr\n",
    "    \n",
    "        if(pd.isnull(addrs_match)):\n",
    "            print \"  %r: addrs_match is None after forcing google address processing(%r, %r)\" % (parid, property_canonical_addr, owner_canonical_addr)\n",
    "            raise\n",
    "        \n",
    "    # If property_canonical_addr is empty add parid to ownerprox_skip_parids\n",
    "    if(canonical_addresses_empty(property_canonical_addr)):\n",
    "        print \"%r: Empty property_canonical_addr, skipping for the future\" % (parid)\n",
    "        ownerprox_skip_parids.add(parid)\n",
    "        return unknown_color\n",
    "\n",
    "    # At this point we can trust addrs_match to be True or False and have complete info it needs\n",
    "    # Check if both addresses are identical use sameaddr_color\n",
    "    if(addrs_match):\n",
    "        return sameaddr_color\n",
    "    \n",
    "    # Do have both, check for matching zip code\n",
    "    property_zip=get_canonical_zip(property_canonical_addr)\n",
    "    owner_zip=get_canonical_zip(owner_canonical_addr)\n",
    "        \n",
    "    # Double check that owner_zip is valid\n",
    "    if(not owner_zip):\n",
    "        # Nope, return unknown\n",
    "        return unknown_color\n",
    "    \n",
    "    # Owner zip is valid, see if it matches property_zip\n",
    "    if(property_zip == owner_zip):\n",
    "        return samezip_color\n",
    "    \n",
    "    # Check for in county\n",
    "    zip_level = 'out-of-state'\n",
    "    if(owner_zip in zip_map.keys()):\n",
    "        zip_level = zip_map[owner_zip]\n",
    "    \n",
    "    if(zip_level=='Allegheny'):\n",
    "        return allegheny_co_color\n",
    "    elif(zip_level=='PA'):\n",
    "        return in_state_color\n",
    "    else:\n",
    "        return out_of_state_color\n",
    "\n",
    "def output_ownerprox_dots(parid_arr, suffix):\n",
    "    # If parid_arr not specified, do all of the keys in property_map\n",
    "    if(len(parid_arr)==0):\n",
    "        parid_arr=property_map.keys()\n",
    "        \n",
    "    # Write out volume of ownership for each residential non-vacant land property\n",
    "    points = []\n",
    "    start=arrow.now()\n",
    "    chunk_start_time=arrow.now()\n",
    "    chunk_size=1000\n",
    "    chunk_cnt=0\n",
    "    \n",
    "    print \"Before removing ownerprox_skip_parids, len(parid_arr) = %d\" % (len(parid_arr))\n",
    "    valid_parid_arr = list(set(parid_arr) - ownerprox_skip_parids)\n",
    "    print \"After removing ownerprox_skip_parids, len(parid_arr) = %d\" % (len(valid_parid_arr))\n",
    "   \n",
    "    for parid in valid_parid_arr:\n",
    "        centroid=None\n",
    "        did_output_dot=False\n",
    "        try:\n",
    "            centroid = parid2centroid[parid]\n",
    "        except:\n",
    "            print \"%s is missing from centroids, removing from future consideration\" % (parid)\n",
    "            ownerprox_skip_parids.add(parid)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Handle periodic debug message\n",
    "            if((chunk_cnt%chunk_size)==0 and chunk_cnt>0):\n",
    "                print \"%d-%d: processing %r, %s time elapsed\" %(chunk_cnt-(chunk_size-1), chunk_cnt, parid, arrow.now()-chunk_start_time)\n",
    "                addcnt=0\n",
    "                chunk_start_time=arrow.now()\n",
    "\n",
    "            # Get list of property events or this property\n",
    "            property_events = property_map[parid]\n",
    "\n",
    "            # Keep track of the date of the last datapoint. \n",
    "            last_date = '1900-01-01'\n",
    "            last_color=0\n",
    "            \n",
    "            # Iterate over the property events.  We need to deal with different instances of the same sale date\n",
    "            # as a batch because there might be multiple color changes before the next sale\n",
    "            i=0\n",
    "            while i < len(property_events):\n",
    "                event_date = property_events[i]['date']\n",
    "                # Get the set of simultaneous events in order of asofdate\n",
    "                sevs = get_simultaneous_property_events(property_events,i)\n",
    "                # Get color for each of these property events\n",
    "                scolors = map(lambda ev: ownerprox_to_color(parid, ev), sevs)\n",
    "                \n",
    "                # Compute the bookends of this set of events.  The start date of the first \n",
    "                # color dot will be the date of this sale.  The end date of the last color\n",
    "                # dot will be the date of the next sale.  We might have color changes in between\n",
    "                # due to modifications in change adresses without a sale.  The potential\n",
    "                # transition points are the asofdate of events where the status changes.\n",
    "                next_start_date = event_date\n",
    "                next_sell_date = None\n",
    "                # Check if there's another event after this batch.  If so, set next_sell_date to be the\n",
    "                # date of the next item\n",
    "                if(i+len(sevs)<len(property_events)):\n",
    "                    next_sell_date = property_events[i+len(sevs)]['date']\n",
    "\n",
    "                if((chunk_cnt%chunk_size)==0):\n",
    "                    print \"  %s (%d): %s = %s [next_sell_date = %s]\" % (parid, i, event_date, scolors, next_sell_date)\n",
    "\n",
    "                last_color = scolors[0]\n",
    "                output_color = True\n",
    "                \n",
    "                for j in range(0, len(sevs)):\n",
    "                    color = scolors[j]\n",
    "                    start_date = next_start_date\n",
    "                    end_date = next_sell_date\n",
    "                    # If we're not yet at the end, set end_date to the next asofdate\n",
    "                    if(j<len(sevs)-1):\n",
    "                        end_date = sevs[j+1]['asofdate']\n",
    "                        next_start_date = end_date\n",
    "                    # Calculate epoch time of start and end \n",
    "                    start_e = SaledateToEpoch(start_date)\n",
    "                    if(end_date == None):\n",
    "                        # No new owner after this, set end date to the end of time\n",
    "                        end_e = float(1e38)\n",
    "                    else:\n",
    "                        end_e = SaledateToEpoch(end_date)\n",
    "\n",
    "                    if(color != None):\n",
    "                        points += PointToPixelXY(centroid)     \n",
    "                        points.append(parse_color(color))\n",
    "                        # Put epoch time for SALEDATE as start valid time, and next_date as end valid time\n",
    "                        points.append(float(start_e))\n",
    "                        points.append(float(end_e))\n",
    "                        #print \"  %s (%d): %s - %s = %s (len %d)\" % (parid, i+j, start_date, end_date, color, len(points))\n",
    "                    else:\n",
    "                        print \"Color of \" + str(volume_map['parcount']) + \" is None\"\n",
    "\n",
    "\n",
    "                # Set i, last_date and last_color for next loop\n",
    "                i = i + len(sevs)\n",
    "                last_date = event_date\n",
    "                last_color= color\n",
    "                did_output_dot=True\n",
    "        except:\n",
    "            print \"Unexpected error processing %s:\" % (parid), sys.exc_info()[0]\n",
    "            raise\n",
    "\n",
    "        #Increment debug message counter\n",
    "        if(did_output_dot):\n",
    "            chunk_cnt = chunk_cnt+1\n",
    "\n",
    "    array.array('f', points).tofile(open(('assessments/ownerprox_color_m_epoch%s.bin'%suffix), 'wb'))\n",
    "\n",
    "    end=arrow.now()\n",
    "    print \"Processing took %s for %d dots\" % (str(end-start), chunk_cnt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_parids=['2018H00310000000','0026N00263000000', '0010H00190000000', '0428D00080000000',\n",
    "            '0072B00069000000', '0050P00138000000', '0195A00010000000',\n",
    "            '0233K00148000000', '0033G00182000000', '0067D00114000000',\n",
    "            '0079A00094000000', '0006G00201000000', '0555S00258000000',\n",
    "            '0368N00060000000', '0029M00270000000', '1081P00065000000',\n",
    "            '0130M00122000000', '1238H00385000000', '0381E00028000000',\n",
    "            '0235J00250000000', '0013S00060000000', '0019M00033000000',\n",
    "            '0042K00177000000', '0010D00089000000', '1064J00060000000',\n",
    "            '0596A00245113500', '0739L00354000000', '0738B00123000000',\n",
    "            '1099P00017000000', '0471D00291000000', '0632S00257000000',\n",
    "            '0456H00094000000', '0133L00049000000', '0029G00165000000',\n",
    "            '1223N00336000000', '0087C00130000000', '2018H00310000000']\n",
    "output_ownerprox_dots(test_parids,\"_test_99_04c_05_09_10_17\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ownerprox_dots(valid_parids_15217,\"_d1_15217_99b_04d_05_09_10_11_17_18b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ownerprox_skip_parids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the full set of properties\n",
    "output_ownerprox_dots([],out_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(property_map['0087D00043000000'], key=lambda ev: ''.join([ev['date'],ev['asofdate']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_map['0087D00043000000'][0]['date']='05-05-1950'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owner_map[u'SACKS AARON & SHIRLEY P'][0]['date']='05-05-1950'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changeaddr_map[u'5925 PHILLIPS AVE PITTSBURGH PA 15217'][0]['date']='05-05-1950'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apd_1999.loc['0086E00117000000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changeaddr_map['2300 E HOLIDAY CT LANSING IL 60438']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owner_map['HRANTICS STEPHEN A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ownerprox_to_color('0553N00123000000', property_map['0553N00123000000'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_property_address_map['0553N00123000000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_map['0232C00201000000'] = [{'asofdate': '1999-01-01',\n",
    "  'canonical_owner_address': u'6255 LIBRARY RD, BETHEL PARK, PA, 15102',\n",
    "  'changeaddr': u'6255 LIBRARY RD BETHEL PARK PA 1510',\n",
    "  'date': '1970-11-01',\n",
    "  'event_type': 'PURCHASE',\n",
    "  'ownername': u'SEACH CO INC'}]\n",
    "\n",
    "changeaddr_map['6255 LIBRARY RD BETHEL PARK PA 1510'] = [{'date': '1970-11-01', 'event_type': 'PURCHASE', 'parid': '0232C00201000000'}]\n",
    "\n",
    "owner_map['SEACH CO INC'] = [{'date': '1970-11-01', 'event_type': 'PURCHASE', 'parid': '0232C00202000000'},\n",
    " {'date': '1970-11-01', 'event_type': 'PURCHASE', 'parid': '0232C00201000000'}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ownerprox_dots(valid_parids_15213,\"_d3_15213\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ownerprox_dots(valid_parids_15213[0:100],\"_d3_100_15213\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Do new fair market colors based on values stored in property_map\n",
    "\n",
    "fm_colors = ['#a50026','#cd2827','#e75436','#f7804b','#fdad61','#fed788','#ffffbf','#b9e0ed','#8dc0db','#699fca','#4d7db9','#3e5aa7','#313695']\n",
    "unknown_color = '#303030'\n",
    "\n",
    "\n",
    "def fm_total_to_color(fm_total):\n",
    "    if (fm_total < 25000.):\n",
    "        return fm_colors[0] \n",
    "    elif (fm_total < 50000.):\n",
    "        return fm_colors[1] \n",
    "    elif (fm_total < 75000.):\n",
    "        return fm_colors[2] \n",
    "    elif (fm_total < 100000.):\n",
    "        return fm_colors[3] \n",
    "    elif (fm_total < 125000.):\n",
    "        return fm_colors[4] \n",
    "    elif (fm_total < 150000.):\n",
    "        return fm_colors[5] \n",
    "    elif (fm_total < 200000.):\n",
    "        return fm_colors[6] \n",
    "    elif (fm_total < 250000.):\n",
    "        return fm_colors[7] \n",
    "    elif (fm_total < 300000.):\n",
    "        return fm_colors[8] \n",
    "    elif (fm_total < 400000.):\n",
    "        return fm_colors[9] \n",
    "    elif (fm_total < 500000.):\n",
    "        return fm_colors[10] \n",
    "    elif (fm_total < 750000.):\n",
    "        return fm_colors[11] \n",
    "    else:\n",
    "        return fm_colors[12] \n",
    " \n",
    "def output_fairmarket_dots(parid_arr, suffix):\n",
    "    # If parid_arr not specified, do all of the keys in property_map\n",
    "    if(len(parid_arr)==0):\n",
    "        parid_arr=property_map.keys()\n",
    "        \n",
    "    # Write out fairmarket value for each residential land property\n",
    "    # TODO: get rid of vacant?\n",
    "    points = []\n",
    "    start=arrow.now()\n",
    "    chunk_start_time=arrow.now()\n",
    "    chunk_size=1000\n",
    "    chunk_cnt=0\n",
    "\n",
    "    for parid in parid_arr:\n",
    "        centroid=None\n",
    "        did_output_dot=False\n",
    "        try:\n",
    "            centroid = parid2centroid[parid]\n",
    "        except:\n",
    "            print \"%s is missing from centroids, skipping\" % (parid)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Handle periodic debug message\n",
    "            if((chunk_cnt%chunk_size)==0 and chunk_cnt>0):\n",
    "                print \"%d-%d: processing %r, %s time elapsed\" %(chunk_cnt-(chunk_size-1), chunk_cnt, parid, arrow.now()-chunk_start_time)\n",
    "                addcnt=0\n",
    "                chunk_start_time=arrow.now()\n",
    "\n",
    "            # Get list of property events or this property\n",
    "            property_events = property_map[parid]\n",
    "\n",
    "            # Keep track of the date of the last datapoint. \n",
    "            last_date = '1900-01-01'\n",
    "            last_color=0\n",
    "            \n",
    "            # Iterate over the property events.  We need to deal with different instances of the same sale date\n",
    "            # as a batch because there might be multiple color changes before the next sale\n",
    "            i=0\n",
    "            while i < len(property_events):\n",
    "                event_date = property_events[i]['date']\n",
    "                # Get the set of simultaneous events in order of asofdate\n",
    "                sevs = get_simultaneous_property_events(property_events,i)\n",
    "                # Get color for each of these property events\n",
    "                scolors = map(lambda ev: fm_total_to_color(ev['fm_total']) if 'fm_total' in ev else unknown_color, sevs)\n",
    "                \n",
    "                # Compute the bookends of this set of events.  The start date of the first \n",
    "                # color dot will be the date of this sale.  The end date of the last color\n",
    "                # dot will be the date of the next sale.  We might have color changes in between\n",
    "                # due to modifications in change adresses without a sale.  The potential\n",
    "                # transition points are the asofdate of events where the status changes.\n",
    "                next_start_date = event_date\n",
    "                next_sell_date = None\n",
    "                # Check if there's another event after this batch.  If so, set next_sell_date to be the\n",
    "                # date of the next item\n",
    "                if(i+len(sevs)<len(property_events)):\n",
    "                    next_sell_date = property_events[i+len(sevs)]['date']\n",
    "\n",
    "                if((chunk_cnt%chunk_size)==0):\n",
    "                    print \"  %s (%d): %s = %s [next_sell_date = %s]\" % (parid, i, event_date, scolors, next_sell_date)\n",
    "\n",
    "                last_color = scolors[0]\n",
    "                output_color = True\n",
    "                \n",
    "                for j in range(0, len(sevs)):\n",
    "                    color = scolors[j]\n",
    "                    start_date = next_start_date\n",
    "                    end_date = next_sell_date\n",
    "                    # If we're not yet at the end, set end_date to the next asofdate\n",
    "                    if(j<len(sevs)-1):\n",
    "                        end_date = sevs[j+1]['asofdate']\n",
    "                        next_start_date = end_date\n",
    "                    # Calculate epoch time of start and end \n",
    "                    start_e = SaledateToEpoch(start_date)\n",
    "                    if(end_date == None):\n",
    "                        # No new owner after this, set end date to the end of time\n",
    "                        end_e = float(1e38)\n",
    "                    else:\n",
    "                        end_e = SaledateToEpoch(end_date)\n",
    "\n",
    "                    # Skip if this is a vacant lot\n",
    "                    if('is_vacant' in sevs[j] and sevs[j]['is_vacant']):\n",
    "                        #print \"%s: Skipping vacant lot\" % (parid)\n",
    "                        continue\n",
    "\n",
    "                    if(color != None):\n",
    "                        points += PointToPixelXY(centroid)     \n",
    "                        points.append(parse_color(color))\n",
    "                        # Put epoch time for SALEDATE as start valid time, and next_date as end valid time\n",
    "                        points.append(float(start_e))\n",
    "                        points.append(float(end_e))\n",
    "                        #print \"  %s (%d): %s - %s = %s (len %d)\" % (parid, i+j, start_date, end_date, color, len(points))\n",
    "                    else:\n",
    "                        print \"Color of \" + str(volume_map['parcount']) + \" is None\"\n",
    "\n",
    "\n",
    "                # Set i, last_date and last_color for next loop\n",
    "                i = i + len(sevs)\n",
    "                last_date = event_date\n",
    "                last_color= color\n",
    "                did_output_dot=True\n",
    "        except:\n",
    "            print \"Unexpected error processing %s:\" % (parid), sys.exc_info()[0]\n",
    "            #raise\n",
    "\n",
    "        #Increment debug message counter\n",
    "        if(did_output_dot):\n",
    "            chunk_cnt = chunk_cnt+1\n",
    "\n",
    "    array.array('f', points).tofile(open(('assessments/parcels_fm_color_epoch%s.bin'%suffix), 'wb'))\n",
    "\n",
    "    end=arrow.now()\n",
    "    print \"Processing took %s for %d dots\" % (str(end-start), chunk_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_fairmarket_dots(valid_parids_15213,\"_d2_15213\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the full set of properties\n",
    "output_fairmarket_dots([],out_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_map['0086K00135000000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_map['0174E00234000000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vacancy_to_color(pev):\n",
    "    # default to grey\n",
    "    color = \"#333333\"\n",
    "    if('is_vacant' in pev and pev['is_vacant']):\n",
    "        # vacant lot, make it green\n",
    "        color = \"#02ca75\"\n",
    "    return color\n",
    " \n",
    "def output_vacancy_dots(parid_arr, suffix):\n",
    "    # If parid_arr not specified, do all of the keys in property_map\n",
    "    if(len(parid_arr)==0):\n",
    "        parid_arr=property_map.keys()\n",
    "        \n",
    "    # Write out whether or not land is vacant for each parcel\n",
    "    points = []\n",
    "    start=arrow.now()\n",
    "    chunk_start_time=arrow.now()\n",
    "    chunk_size=1000\n",
    "    chunk_cnt=0\n",
    "\n",
    "    for parid in parid_arr:\n",
    "        centroid=None\n",
    "        did_output_dot=False\n",
    "        try:\n",
    "            centroid = parid2centroid[parid]\n",
    "        except:\n",
    "            print \"%s is missing from centroids, skipping\" % (parid)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Handle periodic debug message\n",
    "            if((chunk_cnt%chunk_size)==0 and chunk_cnt>0):\n",
    "                print \"%d-%d: processing %r, %s time elapsed\" %(chunk_cnt-(chunk_size-1), chunk_cnt, parid, arrow.now()-chunk_start_time)\n",
    "                addcnt=0\n",
    "                chunk_start_time=arrow.now()\n",
    "\n",
    "            # Get list of property events or this property\n",
    "            property_events = property_map[parid]\n",
    "\n",
    "            # Keep track of the date of the last datapoint. \n",
    "            last_date = '1900-01-01'\n",
    "            last_color=0\n",
    "            \n",
    "            # Iterate over the property events.  We need to deal with different instances of the same sale date\n",
    "            # as a batch because there might be multiple color changes before the next sale\n",
    "            i=0\n",
    "            while i < len(property_events):\n",
    "                event_date = property_events[i]['date']\n",
    "                # Get the set of simultaneous events in order of asofdate\n",
    "                sevs = get_simultaneous_property_events(property_events,i)\n",
    "                # Get color for each of these property events\n",
    "                scolors = map(lambda ev: vacancy_to_color(ev), sevs)\n",
    "                \n",
    "                # Compute the bookends of this set of events.  The start date of the first \n",
    "                # color dot will be the date of this sale.  The end date of the last color\n",
    "                # dot will be the date of the next sale.  We might have color changes in between\n",
    "                # due to modifications in change adresses without a sale.  The potential\n",
    "                # transition points are the asofdate of events where the status changes.\n",
    "                next_start_date = event_date\n",
    "                next_sell_date = None\n",
    "                # Check if there's another event after this batch.  If so, set next_sell_date to be the\n",
    "                # date of the next item\n",
    "                if(i+len(sevs)<len(property_events)):\n",
    "                    next_sell_date = property_events[i+len(sevs)]['date']\n",
    "\n",
    "                if((chunk_cnt%chunk_size)==0):\n",
    "                    print \"  %s (%d): %s = %s [next_sell_date = %s]\" % (parid, i, event_date, scolors, next_sell_date)\n",
    "\n",
    "                last_color = scolors[0]\n",
    "                output_color = True\n",
    "                \n",
    "                for j in range(0, len(sevs)):\n",
    "                    color = scolors[j]\n",
    "                    start_date = next_start_date\n",
    "                    end_date = next_sell_date\n",
    "                    # If we're not yet at the end, set end_date to the next asofdate\n",
    "                    if(j<len(sevs)-1):\n",
    "                        end_date = sevs[j+1]['asofdate']\n",
    "                        next_start_date = end_date\n",
    "                    # Calculate epoch time of start and end \n",
    "                    start_e = SaledateToEpoch(start_date)\n",
    "                    if(end_date == None):\n",
    "                        # No new owner after this, set end date to the end of time\n",
    "                        end_e = float(1e38)\n",
    "                    else:\n",
    "                        end_e = SaledateToEpoch(end_date)\n",
    "\n",
    "                    if(color != None):\n",
    "                        points += PointToPixelXY(centroid)     \n",
    "                        points.append(parse_color(color))\n",
    "                        # Put epoch time for SALEDATE as start valid time, and next_date as end valid time\n",
    "                        points.append(float(start_e))\n",
    "                        points.append(float(end_e))\n",
    "                        #print \"  %s (%d): %s - %s = %s (len %d)\" % (parid, i+j, start_date, end_date, color, len(points))\n",
    "                    else:\n",
    "                        print \"Color of \" + str(volume_map['parcount']) + \" is None\"\n",
    "\n",
    "\n",
    "                # Set i, last_date and last_color for next loop\n",
    "                i = i + len(sevs)\n",
    "                last_date = event_date\n",
    "                last_color= color\n",
    "                did_output_dot=True\n",
    "        except:\n",
    "            print \"Unexpected error processing %s:\" % (parid), sys.exc_info()[0]\n",
    "            #raise\n",
    "\n",
    "        #Increment debug message counter\n",
    "        if(did_output_dot):\n",
    "            chunk_cnt = chunk_cnt+1\n",
    "\n",
    "    array.array('f', points).tofile(open(('assessments/vacant_lots_m_epoch%s.bin'%suffix), 'wb'))\n",
    "\n",
    "    end=arrow.now()\n",
    "    print \"Processing took %s for %d dots\" % (str(end-start), chunk_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_vacancy_dots(valid_parids_15213,\"_d1_15213\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the full set of properties\n",
    "output_vacancy_dots([],out_suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Below is Anne's attempt to put floating point representations of colors into the .bin file for a new \n",
    "# shader Randy's writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def incrementKey(adict,key):\n",
    "    if(key == '' or math.isnan(key)):\n",
    "       return\n",
    "    if not adict.has_key(key):\n",
    "        adict[key] = 1\n",
    "    else:\n",
    "        adict[key] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "year_built_colors = ['#ffffe0','#ffdaa3','#ffb27c','#fb8768','#eb5f5b','#d3394a','#b3152f','#8b0000']\n",
    "def year_built_to_color(year):\n",
    "    if (year <= 1900.0):\n",
    "        return color_from_floats(64,64,64)\n",
    "    elif (year < 1916.0):\n",
    "        return parse_color(year_built_colors[7])       \n",
    "    elif (year < 1932.0):\n",
    "        return parse_color(year_built_colors[6])\n",
    "    elif (year < 1948.0):\n",
    "        return parse_color(year_built_colors[5])\n",
    "    elif (year < 1964.0):\n",
    "        return parse_color(year_built_colors[4])\n",
    "    elif (year < 1980.0):\n",
    "        return parse_color(year_built_colors[3])\n",
    "    elif (year < 1996.0):\n",
    "        return parse_color(year_built_colors[2])\n",
    "    elif  (year < 2012.0):\n",
    "        return parse_color(year_built_colors[1])\n",
    "    elif  (year < 2016.0):\n",
    "        return parse_color(year_built_colors[0])\n",
    "    else:\n",
    "        return color_from_floats(255,255,255)\n",
    " \n",
    "   \n",
    "points = []\n",
    "for i in range(0, len(apd.index)):\n",
    "    par_id = apd['PARID'][i]\n",
    "    year_built = apd['YEARBLT'][i]\n",
    "    if year_built == '' or year_built == '0' or year_built == '0000' or math.isnan(year_built):\n",
    "        year_built = '0001' # Null Value, -62135596800.0 Epoch\n",
    "    elif isinstance(year_built, numbers.Number):\n",
    "        if(year_built < 1):\n",
    "            year_built = '0001' # Null Value, -62135596800.0 Epoch\n",
    "        else:\n",
    "            year_built = \"%04d\" % (year_built)\n",
    "    if centroids.has_key(par_id):\n",
    "        centroid = centroids[par_id]\n",
    "        points += LonLatToPixelXY(centroid)        \n",
    "        points.append(year_built_to_color(float(year_built)))\n",
    "        # Set start valid time as year_built, and max positive float as end valid time\n",
    "        date = datetime.datetime.strptime(year_built, '%Y')        \n",
    "        points.append(GetEpoch(date))\n",
    "        points.append(float(1e38))\n",
    "array.array('f', points).tofile(open(('assessments/parcels_bltyr_color_epoch%s.bin'%out_suffix), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Hightlight based on likelyhood of lead\n",
    "# <1950 = Probably lead\n",
    "# 1950-1978 = Maybe lead (not illegal, but being phased out)\n",
    "# >1978 = Probably not lead (illegal)\n",
    "year_built_colors = ['#ffffe0','#ffdaa3','#ffb27c','#fb8768','#eb5f5b','#d3394a','#b3152f','#8b0000']\n",
    "def year_built_to_lead_color(year):\n",
    "    if (year <= 1900.0):\n",
    "        return color_from_floats(64,64,64)\n",
    "    elif (year < 1950.0):\n",
    "        return parse_color(year_built_colors[7])       \n",
    "    elif (year < 1979.0):\n",
    "        return parse_color(year_built_colors[3])\n",
    "    else:\n",
    "        return parse_color(year_built_colors[0])\n",
    "    \n",
    "points = []\n",
    "for i in range(0, len(apd.index)):\n",
    "    par_id = apd['PARID'][i]\n",
    "    year_built = apd['YEARBLT'][i]\n",
    "    if year_built == '' or year_built == '0' or year_built == '0000' or math.isnan(year_built):\n",
    "        year_built = '0001' # Null Value, -62135596800.0 Epoch\n",
    "    elif isinstance(year_built, numbers.Number):\n",
    "        if(year_built < 1):\n",
    "            year_built = '0001' # Null Value, -62135596800.0 Epoch\n",
    "        else:\n",
    "            year_built = \"%04d\" % (year_built)\n",
    "    if centroids.has_key(par_id):\n",
    "        centroid = centroids[par_id]\n",
    "        points += LonLatToPixelXY(centroid)        \n",
    "        points.append(year_built_to_lead_color(float(year_built)))\n",
    "        # Set start valid time as year_built, and max positive float as end valid time\n",
    "        date = datetime.datetime.strptime(year_built, '%Y')        \n",
    "        points.append(GetEpoch(date))\n",
    "        points.append(float(1e38))\n",
    "array.array('f', points).tofile(open(('assessments/parcels_bltyr_lead_color_epoch%s.bin'%out_suffix), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parcel_colors = ['#fb3059','#fe6b2d','#d1947a','#c6a900','#02ca75','#00a2de','#9529b1']\n",
    "class_descriptions = sorted(class_descs.keys())\n",
    "\n",
    "def class_desc_to_color(class_description):\n",
    "    index = class_descriptions.index(class_description)\n",
    "    return(parse_color(parcel_colors[index]))\n",
    "\n",
    "points = []\n",
    "for i in range(0, len(apd.index)):\n",
    "    par_id = apd['PARID'][i]\n",
    "    class_description = apd['CLASSDESC'][i] \n",
    "    year_built = apd['YEARBLT'][i]\n",
    "\n",
    "    if year_built == '' or year_built == '0' or year_built == '0000' or math.isnan(year_built):\n",
    "        year_built = '0001' # Null Value, -62135596800.0 Epoch\n",
    "    elif isinstance(year_built, numbers.Number):\n",
    "        if(year_built < 1):\n",
    "            year_built = '0001' # Null Value, -62135596800.0 Epoch\n",
    "        else:\n",
    "            year_built = \"%04d\" % (year_built)\n",
    "\n",
    "    if centroids.has_key(par_id):\n",
    "        centroid = centroids[par_id]\n",
    "        points += LonLatToPixelXY(centroid)        \n",
    "        points.append(class_desc_to_color(class_description))\n",
    "        # Set start valid time as year_built, and max positive float as end valid time\n",
    "        date = datetime.datetime.strptime(year_built, '%Y')        \n",
    "        points.append(GetEpoch(date))\n",
    "        points.append(float(1e38))\n",
    "array.array('f', points).tofile(open(('assessments/parcels_class_color_epoch%s.bin'%out_suffix), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Highlight vacant lots\n",
    "points = []\n",
    "for i in range(0, len(apd.index)):\n",
    "    par_id = apd['PARID'][i]\n",
    "    class_description = apd['CLASSDESC'][i] \n",
    "    usedesc = apd['USEDESC'][i]\n",
    "\n",
    "    # default to grey\n",
    "    color = \"#555555\"\n",
    "    if(usedesc =='VACANT LAND'):\n",
    "        # vacant lot, make it green\n",
    "        color = \"#02ca75\"\n",
    "\n",
    "    if centroids.has_key(par_id):\n",
    "        centroid = centroids[par_id]\n",
    "        points += LonLatToPixelXY(centroid)        \n",
    "        points.append(parse_color(color))\n",
    "        # Set start valid time as forever, and max positive float as end valid time\n",
    "        points.append(0)\n",
    "        points.append(float(1e38))\n",
    "array.array('f', points).tofile(open(('assessments/vacant_lots%s.bin'%out_suffix), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Here are Anne's efforts using the version with owner names to generate volume of properties owned by current owner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# SALEDATE field is in '%m-%d-%Y' format (ex 10-26-2012) when present\n",
    "def SaledateToEpoch(datestr):\n",
    "    return calendar.timegm(time.strptime(datestr, '%m-%d-%Y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Parse apd table into dictionaries from owner names and change addresses to parid, and from parid to other info\n",
    "# about the property\n",
    "owner_names_to_parids = {}\n",
    "owner_changeaddrs_to_parids = {}\n",
    "parid_info = {}\n",
    "volume_counts = {}\n",
    "\n",
    "# For now, only look at residential properties with known centroids in 15213\n",
    "for i in range(0, len(apd.index)):\n",
    "    par_id = apd['PARID'][i]\n",
    "    zipcode = apd['PROPERTYZIP'][i]\n",
    "    class_description = apd['CLASSDESC'][i] \n",
    "    usedesc = apd['USEDESC'][i] \n",
    "    if (usedesc == 'VACANT LAND' or not centroids.has_key(par_id)):\n",
    "        # skip this one\n",
    "        continue\n",
    "    \n",
    "    # We want to include residential, apartments (usedesc includes APART)\n",
    "    if(class_description != 'RESIDENTIAL' and (isinstance(usedesc, numbers.Number) or 'APART' not in usedesc)):\n",
    "        # skip this one\n",
    "        continue\n",
    "        \n",
    "    # This one meets our criteria\n",
    "    owner_name = apd['PROPERTYOWNER'][i]\n",
    "    owner_changeaddr = string.strip(str(apd['CHANGENOTICEADDRESS1'][i]))\n",
    "    c2 = string.strip(str(apd['CHANGENOTICEADDRESS2'][i]))\n",
    "    c3 = string.strip(str(apd['CHANGENOTICEADDRESS3'][i]))\n",
    "    c4 = apd['CHANGENOTICEADDRESS4'][i]\n",
    "    if(c2 != ''):\n",
    "        owner_changeaddr+=\", \" + c2\n",
    "    if(c3 != ''):\n",
    "        owner_changeaddr+=\", \" + c3\n",
    "    if(c4 != ''):\n",
    "        if(isinstance(c4, numbers.Number)):\n",
    "            if(not math.isnan(float(c4))):\n",
    "                owner_changeaddr+=\", \" + str(int(c4))\n",
    "        else:\n",
    "            owner_changeaddr+=\", \" + c4\n",
    "        \n",
    "    if(owner_changeaddr == ''):\n",
    "        # skip this one\n",
    "        continue\n",
    "        \n",
    "    if(owner_names_to_parids.has_key(owner_name)):\n",
    "        owner_names_to_parids[owner_name].append(par_id)\n",
    "    else:\n",
    "        owner_names_to_parids[owner_name]=[par_id]\n",
    "        \n",
    "    if(owner_changeaddrs_to_parids.has_key(owner_changeaddr)):\n",
    "        owner_changeaddrs_to_parids[owner_changeaddr].append(par_id)\n",
    "    else:\n",
    "        owner_changeaddrs_to_parids[owner_changeaddr]=[par_id]\n",
    "\n",
    "    parid_info[par_id]={'owner_name':owner_name,\n",
    "                        'owner_changeaddr':owner_changeaddr,\n",
    "                        'centroid':centroids[par_id],\n",
    "                        'HOMESTEADFLAG':apd['HOMESTEADFLAG'][i],\n",
    "                        'USEDESC':apd['USEDESC'][i],\n",
    "                        'OWNERDESC':apd['OWNERDESC'][i],\n",
    "                        'SALEDATE':apd['SALEDATE'][i],\n",
    "                        'OWNERDESC':apd['OWNERDESC'][i],\n",
    "                        'CLASSDESC':apd['CLASSDESC'][i]\n",
    "                       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Add counts to parid_info based on max of owner_names_to_parids or owner_changeaddrs_to_parids\n",
    "# Special case properties with HOMESTEADFLAG set to HOM to be volume=1\n",
    "for par_id in parid_info.keys():\n",
    "    volume=1\n",
    "    if(parid_info[par_id]['HOMESTEADFLAG']!='HOM'):\n",
    "        ownername_num = len(owner_names_to_parids[parid_info[par_id]['owner_name']])\n",
    "        ownerchangeaddr_num = len(owner_changeaddrs_to_parids[parid_info[par_id]['owner_changeaddr']])\n",
    "        volume = max(ownername_num,ownerchangeaddr_num)\n",
    "    parid_info[par_id]['volume'] = volume\n",
    "    incrementKey(volume_counts,volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "volume_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ownername in owner_names_to_parids.keys():\n",
    "    ownername_num = len(owner_names_to_parids[ownername])\n",
    "    if(ownername_num>10):\n",
    "        print \"%s: %d\" % (ownername,ownername_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ownerchangeaddr in owner_changeaddrs_to_parids.keys():\n",
    "    ownerchangeaddr_num = len(owner_changeaddrs_to_parids[ownerchangeaddr])\n",
    "    if(ownerchangeaddr_num>10):\n",
    "        print \"%s: %d\" % (ownerchangeaddr,ownerchangeaddr_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for par_id in parid_info.keys():\n",
    "    if(parid_info[par_id]['volume']>10):\n",
    "        print parid_info[par_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Write out a binary file with the volume colors\n",
    "vol_colors = ['#a50026','#cd2827','#e75436','#f7804b','#fdad61','#fed788','#ffffbf','#b9e0ed','#8dc0db','#699fca','#4d7db9','#3e5aa7','#313695']\n",
    "def volume_to_color(volume):\n",
    "    if (volume < 2):\n",
    "        return parse_color(vol_colors[0]) \n",
    "    #elif (volume < 3):\n",
    "    #    return parse_color(vol_colors[1]) \n",
    "    #elif (volume < 4):\n",
    "    #    return parse_color(vol_colors[2]) \n",
    "    elif (volume < 5):\n",
    "        return parse_color(vol_colors[3]) \n",
    "    elif (volume < 10):\n",
    "        return parse_color(vol_colors[4]) \n",
    "    elif (volume < 20):\n",
    "        return parse_color(vol_colors[5]) \n",
    "    elif (volume < 40):\n",
    "        return parse_color(vol_colors[6]) \n",
    "    elif (volume < 60):\n",
    "        return parse_color(vol_colors[7]) \n",
    "    elif (volume < 80):\n",
    "        return parse_color(vol_colors[8]) \n",
    "    elif (volume < 150):\n",
    "        return parse_color(vol_colors[9]) \n",
    "    elif (volume < 300):\n",
    "        return parse_color(vol_colors[10]) \n",
    "    elif (volume < 500):\n",
    "        return parse_color(vol_colors[11]) \n",
    "    else:\n",
    "        return parse_color(vol_colors[12]) \n",
    "\n",
    "# Write out volume of ownership for each residential non-vacant land property\n",
    "points = []\n",
    "for par_id in parid_info.keys():\n",
    "    centroid = parid_info[par_id]['centroid']\n",
    "    color = volume_to_color(parid_info[par_id]['volume'])\n",
    "    saledate = SaledateToEpoch('01-01-1900')\n",
    "    saledate_raw = parid_info[par_id]['SALEDATE']\n",
    "    if(not(saledate_raw == '' or (isinstance(saledate_raw, numbers.Number) and math.isnan(saledate_raw)))):\n",
    "        # If not valid, leave as 1900, otherwise parse it into an epoch time\n",
    "        saledate = SaledateToEpoch(saledate_raw)\n",
    "        \n",
    "    if(color != None):\n",
    "        points += LonLatToPixelXY(centroid)        \n",
    "        points.append(color)\n",
    "        # Put epoch time for SALEDATE as start valid time, and max positive float as end valid time\n",
    "        points.append(float(saledate))\n",
    "        points.append(float(1e38))\n",
    "    else:\n",
    "        print \"Color of \" + str(parid_info[par_id]['volume']) + \" is None\"\n",
    "array.array('f', points).tofile(open(('assessments/res_volume_color_epoch%s.bin'%out_suffix), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#original version\n",
    "legend = '<svg width=\"400\" height=\"30\"><rect fill=\"#a50026\" x=\"0\" height=\"10\" width=\"30\"></rect><rect fill=\"#cd2827\" x=\"30\" height=\"10\" width=\"30\"></rect><rect fill=\"#e75436\" x=\"60\" height=\"10\" width=\"30\"></rect><rect fill=\"#f7804b\" x=\"90\" height=\"10\" width=\"30\"></rect><rect fill=\"#fdad61\" x=\"120\" height=\"10\" width=\"30\"></rect><rect fill=\"#fed788\" x=\"150\" height=\"10\" width=\"30\"></rect><rect fill=\"#ffffbf\" x=\"180\" height=\"10\" width=\"30\"></rect><rect fill=\"#b9e0ed\" x=\"210\" height=\"10\" width=\"30\"></rect><rect fill=\"#8dc0db\" x=\"240\" height=\"10\" width=\"30\"></rect><rect fill=\"#699fca\" x=\"270\" height=\"10\" width=\"30\"></rect><rect fill=\"#4d7db9\" x=\"300\" height=\"10\" width=\"30\"></rect><rect fill=\"#3e5aa7\" x=\"330\" height=\"10\" width=\"30\"></rect><text font-size=\"10.5px\" y=\"29\" x=\"0\" fill=\"#ffffff\">1</text><text font-size=\"10.5px\" y=\"29\" x=\"20\"  fill=\"#ffffff\">&nbsp;2</text><text font-size=\"10.5px\" y=\"29\" x=\"50\"  fill=\"#ffffff\">&nbsp;3</text><text font-size=\"10.5px\" y=\"29\" x=\"80\"  fill=\"#ffffff\"> 4</text><text font-size=\"10.5px\" y=\"29\" x=\"110\"  fill=\"#ffffff\"> 5</text><text font-size=\"10.5px\" y=\"29\" x=\"140\"  fill=\"#ffffff\">10</text><text font-size=\"10.5px\" y=\"29\" x=\"170\"  fill=\"#ffffff\">20</text><text font-size=\"10.5px\" y=\"29\" x=\"200\"  fill=\"#ffffff\">40</text><text font-size=\"10.5px\" y=\"29\" x=\"230\"  fill=\"#ffffff\">60</text><text font-size=\"10.5px\" y=\"29\" x=\"260\"  fill=\"#ffffff\">80</text><text font-size=\"10.5px\" y=\"29\" x=\"290\"  fill=\"#ffffff\">150</text><text font-size=\"10.5px\" y=\"29\" x=\"320\"  fill=\"#ffffff\">300</text><text font-size=\"10.5px\" y=\"29\" x=\"350\"  fill=\"#ffffff\">500</text>'\n",
    "HTML(legend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#modified version\n",
    "legend = '<svg width=\"400\" height=\"30\"><rect fill=\"#a50026\" x=\"0\" height=\"10\" width=\"30\"></rect><rect fill=\"#f7804b\" x=\"30\"  height=\"10\" width=\"30\"></rect><rect fill=\"#fdad61\" x=\"60\"  height=\"10\" width=\"30\"></rect><rect fill=\"#fed788\" x=\"90\"  height=\"10\" width=\"30\"></rect><rect fill=\"#ffffbf\" x=\"120\" height=\"10\" width=\"30\"></rect><rect fill=\"#b9e0ed\" x=\"150\" height=\"10\" width=\"30\"></rect><rect fill=\"#8dc0db\" x=\"180\" height=\"10\" width=\"30\"></rect><rect fill=\"#699fca\" x=\"210\" height=\"10\" width=\"30\"></rect><rect fill=\"#4d7db9\" x=\"240\" height=\"10\" width=\"30\"></rect><rect fill=\"#3e5aa7\" x=\"270\" height=\"10\" width=\"30\"></rect><text font-size=\"10.5px\" y=\"29\" x=\"0\" fill=\"#ffffff\">1</text><text font-size=\"10.5px\" y=\"29\" x=\"20\"  fill=\"#ffffff\">&nbsp;2</text><text Font-size=\"10.5px\" y=\"29\" x=\"50\"   fill=\"#ffffff\"> 5</text><text font-size=\"10.5px\" y=\"29\" x=\"80\"   fill=\"#ffffff\">10</text><text font-size=\"10.5px\" y=\"29\" x=\"110\"  fill=\"#ffffff\">20</text><text font-size=\"10.5px\" y=\"29\" x=\"140\"  fill=\"#ffffff\">40</text><text font-size=\"10.5px\" y=\"29\" x=\"170\"  fill=\"#ffffff\">60</text><text font-size=\"10.5px\" y=\"29\" x=\"200\"  fill=\"#ffffff\">80</text><text font-size=\"10.5px\" y=\"29\" x=\"230\"  fill=\"#ffffff\">150</text><text font-size=\"10.5px\" y=\"29\" x=\"260\"  fill=\"#ffffff\">300</text><text font-size=\"10.5px\" y=\"29\" x=\"290\"  fill=\"#ffffff\">500</text>'\n",
    "HTML(legend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Write out a binary file with the colors for residential owned by corporate, government, or regular owners\n",
    "def ownertype_to_color(par_id):\n",
    "    ownerdesc = parid_info[par_id]['OWNERDESC']\n",
    "    classdesc = parid_info[par_id]['CLASSDESC']\n",
    "    volume = parid_info[par_id]['volume']\n",
    "    if (classdesc == \"GOVERNMENT\"):\n",
    "        print \"Government: %s, %s, %d\" % (ownerdesc, classdesc,volume)\n",
    "        # Same color as government in property class view\n",
    "        return parse_color(\"#d1947a\") \n",
    "    elif ('CORPORATION' in ownerdesc):\n",
    "        # Same color as 500+ properties in volume view\n",
    "        #return parse_color('#313695') \n",
    "        return parse_color('#ffffbf')\n",
    "    elif ('REGULAR' in ownerdesc):\n",
    "        # Regular owner, what volume?\n",
    "        if(volume == 1):\n",
    "            # Same color as 1 in volume view\n",
    "            return parse_color('#a50026')\n",
    "        else:\n",
    "            # Same color as other in class view\n",
    "            return parse_color('#02ca75')\n",
    "    else:\n",
    "        print \"Unrecognized owner type: %s, %s, %d\" % (ownerdesc, classdesc,volume)\n",
    "        return parse_color('#303030') \n",
    "\n",
    "# Write out volume of ownership for each residential non-vacant land property\n",
    "points = []\n",
    "for par_id in parid_info.keys():\n",
    "    centroid = parid_info[par_id]['centroid']\n",
    "    color = ownertype_to_color(par_id)\n",
    "    saledate = SaledateToEpoch('01-01-1900')\n",
    "    saledate_raw = parid_info[par_id]['SALEDATE']\n",
    "    if(not(saledate_raw == '' or (isinstance(saledate_raw, numbers.Number) and math.isnan(saledate_raw)))):\n",
    "        # If not valid, leave as 1900, otherwise parse it into an epoch time\n",
    "        saledate = SaledateToEpoch(saledate_raw)\n",
    "        \n",
    "    if(color != None):\n",
    "        points += LonLatToPixelXY(centroid)        \n",
    "        points.append(color)\n",
    "        # Put epoch time for SALEDATE as start valid time, and max positive float as end valid time\n",
    "        points.append(float(saledate))\n",
    "        points.append(float(1e38))\n",
    "    else:\n",
    "        print \"Color of \" + str(parid_info[par_id]['volume']) + \" is None\"\n",
    "array.array('f', points).tofile(open(('assessments/ownertype_color_epoch%s.bin'%out_suffix), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parid_info['0002K00028000000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Foreclosures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# SALEDATE field is in '%m-%d-%Y' format (ex 10-26-2012) when present\n",
    "def FilingdateToEpoch(datestr):\n",
    "    return calendar.timegm(time.strptime(datestr, '%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read in foreclosure data \n",
    "fpath = \"assessments/foreclosures-180327.csv\"\n",
    "fapd = pandas.read_csv(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Write out volume of ownership for each residential non-vacant land property\n",
    "points = []\n",
    "missing_pin_c=0\n",
    "found_pin_c=0\n",
    "missing_pin_a=0\n",
    "found_pin_a=0\n",
    "foreclosure_color = '#ff00ff'\n",
    "\n",
    "for i in range(0, len(fapd.index)):\n",
    "    par_id = fapd['pin'][i]\n",
    "    filing_date_raw = fapd['filing_date'][i]\n",
    "    \n",
    "    if(filing_date_raw == '' or (isinstance(filing_date_raw, numbers.Number) and math.isnan(filing_date_raw))):\n",
    "        print \"%d: Can't read filing_date %s\" % (i, filing_date_raw)\n",
    "        continue\n",
    "    filing_date = FilingdateToEpoch(filing_date_raw)\n",
    "    \n",
    "    if par_id not in parid_info:\n",
    "        missing_pin_c+=1\n",
    "        continue\n",
    "    found_pin_c+= 1\n",
    "    \n",
    "    centroid = parid_info[par_id]['centroid']\n",
    "    \n",
    "    if par_id not in parid_info:\n",
    "        missing_pin_a+=1\n",
    "    found_pin_a+=1\n",
    "    \n",
    "    saledate = SaledateToEpoch('01-01-1900')\n",
    "    saledate_raw = parid_info[par_id]['SALEDATE']\n",
    "    if(not(saledate_raw == '' or (isinstance(saledate_raw, numbers.Number) and math.isnan(saledate_raw)))):\n",
    "        # If not valid, leave as 1900, otherwise parse it into an epoch time\n",
    "        saledate = SaledateToEpoch(saledate_raw)\n",
    "        \n",
    "    startdate = filing_date\n",
    "    enddate = saledate\n",
    "    \n",
    "    if(filing_date<saledate):\n",
    "        # Sold after foreclosure, who bought it?\n",
    "        points += LonLatToPixelXY(centroid)        \n",
    "        points.append(ownertype_to_color(par_id))\n",
    "        # Put epoch time for SALEDATE as start valid time, and max positive float as end valid time\n",
    "        points.append(float(filing_date))\n",
    "        points.append(float(1e38))\n",
    "    else:\n",
    "        # Sold before foreclosure, who owned it before?\n",
    "        points += LonLatToPixelXY(centroid)        \n",
    "        points.append(ownertype_to_color(par_id))\n",
    "        # Put epoch time for SALEDATE as start valid time, and max positive float as end valid time\n",
    "        points.append(float(saledate))\n",
    "        points.append(float(filing_date))\n",
    "        # Not sold after foreclosure, show ownership before sale and end foreclosure at end of time\n",
    "        enddate=float(1e38)\n",
    "        \n",
    "    # Output foreclosure dot\n",
    "    points += LonLatToPixelXY(centroid)        \n",
    "    points.append(parse_color(foreclosure_color))\n",
    "    # Put epoch time for SALEDATE as start valid time, and max positive float as end valid time\n",
    "    points.append(float(startdate))\n",
    "    points.append(float(enddate))\n",
    "\n",
    "array.array('f', points).tofile(open(('assessments/foreclosure_color_epoch%s.bin'%out_suffix), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Below here are modified versions of the blocks from Gabriel's original python notebook,\n",
    "# none of which are needed anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sale_prices = {}\n",
    "for i in range(0, len(apd.index)):\n",
    "    sale_price = apd['SALEPRICE'][i]\n",
    "    if not sale_prices.has_key(sale_price):\n",
    "        sale_prices[sale_price] = 1\n",
    "    else:\n",
    "        sale_prices[sale_price] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(sale_prices.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "years_built = {}\n",
    "sale_dates = {}\n",
    "fairmarket_totals = {}\n",
    "classes = {}\n",
    "class_descs = {}\n",
    "\n",
    "def incrementKey(adict,key):\n",
    "    if(key == '' or math.isnan(key)):\n",
    "       return\n",
    "    if not adict.has_key(key):\n",
    "        adict[key] = 1\n",
    "    else:\n",
    "        adict[key] += 1\n",
    "    \n",
    "for i in range(0, len(apd.index)):\n",
    "    year_built = apd['YEARBLT'][i]\n",
    "    incrementKey(years_built, year_built)\n",
    "    sale_date = apd['SALEDATE'][i]\n",
    "    incrementKey(sale_dates, sale_date)\n",
    "    fairmarket_total = apd['FAIRMARKETTOTAL'][i]\n",
    "    incrementKey(fairmarket_totals, fairmarket_total)\n",
    "    class_ = apd['CLASS'][i]\n",
    "    incrementKey(classes, class_)\n",
    "    class_desc = apd['CLASSDESC'][i]\n",
    "    incrementKey(class_descs, class_desc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(apd.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(years_built.keys())\n",
    "years_built['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "years_built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "points = []\n",
    "for i in range(0, len(apd.index)):\n",
    "    par_id = apd['PARID'][i]\n",
    "    year_built = apd['YEARBLT'][i]\n",
    "    if year_built == '' or year_built == '0' or year_built == '0000' or math.isnan(year_built):\n",
    "        year_built = '0001' # Null Value, -62135596800.0 Epoch\n",
    "    elif isinstance(year_built, numbers.Number):\n",
    "        if(year_built < 1):\n",
    "            year_built = '0001' # Null Value, -62135596800.0 Epoch\n",
    "        else:\n",
    "            year_built = \"%04d\" % (year_built)\n",
    "    if centroids.has_key(par_id):\n",
    "        centroid = centroids[par_id]\n",
    "        points += LonLatToPixelXY(centroid)\n",
    "        date = datetime.datetime.strptime(year_built, '%Y')        \n",
    "        points.append(GetEpoch(date))\n",
    "array.array('f', points).tofile(open(('assessments/parcels_yr%s.bin'%out_suffix), 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "year_built\n",
    "date = datetime.datetime.strptime('2000', '%Y')   \n",
    "GetEpoch(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fairmarket_totals['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class_descs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sorted(class_descs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "no_year_built = 0\n",
    "no_= 0\n",
    "for i in range(0, len(apd.index)):\n",
    "    year_built = apd['YEARBLT'][i]\n",
    "    class_ = apd['CLASS'][i]\n",
    "    if year_built == '' and class_ == 'R':\n",
    "        no_year_built += 1\n",
    "    elif year_built == '':\n",
    "        no_ += 1\n",
    "print no_year_built\n",
    "print no_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class_descriptions = sorted(class_descs.keys())\n",
    "points = []\n",
    "for i in range(0, len(apd.index)):\n",
    "    par_id = apd['PARID'][i]\n",
    "    year_built = apd['YEARBLT'][i]\n",
    "    class_description = apd['CLASSDESC'][i]  \n",
    "    if year_built == '' or year_built == '0' or year_built == '0000' or math.isnan(year_built):\n",
    "        year_built = '0001' # Null Value, -62135596800.0 Epoch\n",
    "    elif isinstance(year_built, numbers.Number):\n",
    "        if(year_built < 1):\n",
    "            year_built = '0001' # Null Value, -62135596800.0 Epoch\n",
    "        else:\n",
    "            year_built = \"%04d\" % (year_built)\n",
    "    if centroids.has_key(par_id):\n",
    "        centroid = centroids[par_id]\n",
    "        points += LonLatToPixelXY(centroid)\n",
    "        date = datetime.datetime.strptime(year_built, '%Y')        \n",
    "        points.append(GetEpoch(date))\n",
    "        points.append(class_descriptions.index(class_description))\n",
    "array.array('f', points).tofile(open(('assessments/parcels_yrblt%s.bin'%out_suffix), 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "foo = sorted(class_descs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parcel_colors = ['#e41a1c','#377eb8','#4daf4a','#984ea3','#ff7f00','#ffff33','#a65628']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for color in hex_colors:\n",
    "    rgb_color = HexToRgb(color)\n",
    "    print \"%s,%s,%s\" % (rgb_color[0]/255.0, rgb_color[1]/255.0, rgb_color[2]/255.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hex_colors = [\"#fb3059\",\n",
    "\"#fe6b2d\",\n",
    "\"#d1947a\",\n",
    "\"#c6a900\",\n",
    "\"#02ca75\",\n",
    "\"#00a2de\",\n",
    "\"#9529b1\"]\n",
    "for color in hex_colors:\n",
    "    rgb_color = HexToRgb(color)\n",
    "    print \"%.4f,%.4f,%.4f\" % (rgb_color[0]/255.0, rgb_color[1]/255.0, rgb_color[2]/255.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hex_colors = ['#ffffff','#fffffa','#fffdf5','#fffdf1','#fffcef','#fffbed','#fffbea','#fff9e6','#fff9e3','#fff7e0','#fff7dd','#fff6db','#fff5d8','#fff4d6','#fff3d4','#fff3d2','#fff2d0','#fff0cd','#fff0cb','#ffefc8','#ffeec6','#ffedc3','#ffecc1','#ffeabe','#ffeabc','#ffe9b9','#ffe7b7','#ffe7b4','#ffe6b2','#ffe4b0','#ffe4ad','#ffe3ab','#ffe1a8','#ffe1a6','#ffe0a4','#ffdea1','#ffde9f','#ffdc9d','#ffdc9b','#ffda98','#ffd996','#ffd994','#ffd891','#ffd68f','#ffd68d','#ffd58a','#ffd388','#ffd386','#ffd284','#ffd081','#ffcf7f','#ffcf7d','#ffcd7b','#ffcc78','#ffcc76','#ffca74','#ffc971','#ffc96f','#ffc76d','#ffc76b','#ffc568','#ffc566','#ffc464','#ffc361','#ffc15d','#ffc05a','#ffbf58','#ffbe56','#ffbc53','#ffbc51','#ffba4e','#ffba4c','#ffb849','#ffb747','#ffb644','#ffb53f','#ffb43d','#ffb23a','#ffb237','#ffb134','#ffaf31','#ffaf2e','#ffad2a','#ffab24','#ffab21','#ffa91c','#ffa818','#ffa813','#ffa60b','#ffa400','#ffa300','#ffa200','#ffa100','#ff9f00','#ff9e00','#ff9d00','#ff9c00','#ff9b00','#ff9900','#ff9800','#ff9700','#ff9500','#ff9500','#ff9300','#ff9100','#ff9000','#ff8f00','#ff8d00','#ff8c00','#ff8a00','#ff8a00','#fe8800','#fe8800','#fe8700','#fe8400','#fd8400','#fd8300','#fd8200','#fd8000','#fc7f00','#fc7d00','#fc7c00','#fc7b00','#fb7a00','#fb7900','#fb7800','#fa7700','#fa7400','#fa7400','#f97200','#f97100','#f87000','#f86e00','#f86e00','#f76d00','#f76c00','#f66a00','#f66900','#f56800','#f56600','#f46600','#f46400','#f36400','#f36200','#f36100','#f26000','#f15e00','#f15d00','#f05d00','#ef5b00','#ef5900','#ef5900','#ee5800','#ed5700','#ed5500','#ec5500','#eb5400','#ea5200','#ea5100','#e95000','#e84f00','#e84e00','#e74c00','#e74b00','#e64a00','#e54900','#e44800','#e44700','#e34600','#e34500','#e14400','#e14300','#e04100','#df4000','#de4000','#de3e00','#dd3d00','#dc3d00','#db3c00','#db3a00','#da3900','#d83800','#d83700','#d73600','#d63500','#d63401','#d43301','#d43201','#d23101','#d13001','#d12e01','#d02e01','#d02d01','#ce2c01','#ce2b01','#cd2a01','#cc2901','#ca2801','#c92701','#c92601','#c82501','#c72401','#c62301','#c52201','#c42102','#c32002','#c21f02','#c01e02','#bf1d02','#bf1c02','#be1b02','#bd1a02','#bc1902','#ba1902','#ba1802','#b81702','#b81602','#b71502','#b51402','#b51302','#b41202','#b21202','#b11102','#b01002','#af0e02','#ae0d02','#ac0d02','#ac0c02','#aa0b02','#a90a02','#a80902','#a80902','#a60802','#a50702','#a30602','#a30602','#a20502','#a00502','#a00402','#9e0302','#9c0302','#9b0302','#9b0201','#9a0201','#990201','#970101','#950101','#950101','#940101','#920001','#920001','#900001','#8f0000','#8d0000','#8d0000','#8b0000']\n",
    "\n",
    "img = Image.new( 'RGB', (255,255), \"white\") # create a new image\n",
    "pixels = img.load() # create the pixel map\n",
    "for i in range(img.size[0]):    # for every pixel:\n",
    "    for j in range(img.size[1]):\n",
    "        pixels[i,j] = hex_to_rgb(hex_colors[i]) # set the colour accordingly\n",
    "img.show\n",
    "img.save(\"assessments/year-built-color-map.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hex_stops =  ['#1d1e4e','#482045','#6f1f4b','#8d2747','#aa3f4c','#a9513f','#ba663a','#cf8139','#e59f44','#e9bb76']\n",
    "hex_colors = ['#1d1e4e','#201e4e','#241e4d','#261e4d','#291e4c','#291f4c','#2b1f4c','#2d1f4b','#301f4b','#331f4a','#341f4a','#371f49','#381f49','#3b1f48','#3d1f48','#3d1f48','#3f2047','#402047','#432046','#452046','#462046','#482045','#492045','#4b2045','#4d2046','#4d2046','#502046','#502046','#522047','#532047','#552047','#562047','#572047','#5a2048','#5b2048','#5c2048','#5d2048','#5e2048','#602049','#622049','#632049','#652049','#65204a','#67204a','#69204a','#6b1f4a','#6b1f4a','#6c1f4b','#6f1f4b','#701f4b','#711f4b','#72204b','#72204b','#75204a','#75214a','#76214a','#77214a','#78214a','#7a224a','#7a224a','#7d2249','#7d2249','#7e2349','#7f2349','#802449','#812449','#832448','#842548','#852548','#862548','#882548','#882648','#892648','#8b2647','#8b2747','#8d2747','#8e2847','#8e2847','#902947','#902948','#912a48','#922b48','#932c48','#932c48','#952d48','#952d48','#952e49','#972f49','#973049','#983049','#993149','#9a3249','#9a3249','#9c334a','#9c334a','#9d354a','#9d354a','#9f364a','#9f364a','#a0374a','#a1384a','#a2384b','#a2394b','#a33a4b','#a43a4b','#a43b4b','#a63c4b','#a73c4b','#a83d4c','#a83d4c','#a93e4c','#a93f4c','#aa414b','#aa424a','#aa4349','#aa4448','#aa4747','#aa4846','#a94b44','#a94b43','#a94d43','#a94e41','#a94f40','#a9513f','#a9523f','#aa523f','#ab533e','#ac543e','#ac543e','#ad563e','#ad563e','#ae573e','#ae583e','#af593d','#b05a3d','#b05a3d','#b15b3d','#b15b3d','#b25d3c','#b35d3c','#b35e3c','#b45f3c','#b55f3c','#b5603c','#b6623b','#b6623b','#b7623b','#b8643b','#b9643b','#b9653a','#ba653a','#ba673a','#bb683a','#bb683a','#bc683a','#bd693a','#bd6a3a','#bd6a3a','#be6c3a','#bf6c3a','#c06d3a','#c16e3a','#c16e3a','#c16f3a','#c2713a','#c2713a','#c3723a','#c4733a','#c5743a','#c5753a','#c5753a','#c6753a','#c7773a','#c8773a','#c87939','#c97939','#c97939','#ca7a39','#ca7b39','#cb7c39','#cc7d39','#cc7e39','#cd7e39','#ce7f39','#ce8039','#cf8139','#cf8139','#d08239','#d0833a','#d1843a','#d2843a','#d2853b','#d2863b','#d3873b','#d4883b','#d5883c','#d5893c','#d58a3c','#d68b3c','#d78c3d','#d78d3d','#d88d3d','#d88d3e','#d98f3e','#da8f3e','#da903f','#da913f','#db923f','#dc933f','#dc9340','#dd9340','#dd9440','#de9641','#df9641','#df9741','#e09741','#e09942','#e19942','#e19a42','#e29a42','#e29c43','#e39d43','#e49d43','#e59e44','#e59e44','#e59f45','#e5a047','#e6a149','#e6a24b','#e6a44c','#e6a54e','#e6a650','#e6a652','#e6a652','#e7a754','#e7a855','#e7aa57','#e7ab59','#e7ab5b','#e7ac5d','#e8ae5e','#e8af60','#e8af62','#e8b063','#e8b165','#e8b265','#e8b367','#e8b469','#e8b56b','#e9b66c','#e9b76e','#e9b870','#e9b872','#e9ba73','#e9ba75','#e9bb76']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hex_colors.index(hex_stops[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "years_built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hex_stops = ['#f7fbff','#deebf7','#c6dbef','#9ecae1','#6baed6','#4292c6','#2171b5','#08519c','#08306b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in hex_stops:\n",
    "  print i + \",\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hex_stops = ['#ffffe0','#dcfac5','#b8f4ab','#90ee90','#6dc88c','#46a386','#008080','#265a81','#233381','#000080']\n",
    "for color in hex_stops:\n",
    "    rgb_color = HexToRgb(color)\n",
    "    print \"%.4f,%.4f,%.4f\" % (rgb_color[0]/255.0, rgb_color[1]/255.0, rgb_color[2]/255.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hex_stops = ['#1d1e4e','#5f2049','#8c2747','#aa414b','#ba653a','#d0823a','#e5a045','#ebc07f','#f7e0af','#ffffe0']\n",
    "years = [1880,1895,1910,1925,1940,1955,1970,1985,2000,2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    print \"if (year < %s) {\" % (years[i])\n",
    "    rgb_color = HexToRgb(hex_stops[i])\n",
    "    print \"  color = vec4(%.4f,%.4f,%.4f,1.0)\" % (rgb_color[0]/255.0, rgb_color[1]/255.0, rgb_color[2]/255.0)\n",
    "    print \"}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hex_colors = ['#ffffe0','#ffeec1','#ffdea7','#ffcb91','#ffb880','#ffa474','#fe906a','#f87d64','#f06a5e','#e75758','#db4551','#cf3447','#c0223b','#b0122c','#9e051b','#8b0000']\n",
    "img = Image.new( 'RGB', (255,255), \"white\") # create a new image\n",
    "pixels = img.load() # create the pixel map\n",
    "for i in range(img.size[0]):    # for every pixel:\n",
    "    for j in range(img.size[1]):\n",
    "        pixels[i,j] = hex_to_rgb(hex_colors[i/16]) # set the colour accordingly\n",
    "img.show()\n",
    "img.save(\"assessments/year-built-color-map.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parcel_colors = ['#e41a1c','#377eb8','#4daf4a','#984ea3','#ff7f00','#ffff33','#a65628']\n",
    "parcel_colors = ['#fb3059','#fe6b2d','#d1947a','#c6a900','#02ca75','#00a2de','#9529b1']\n",
    "\n",
    "year_built_colors = ['#ffffe0','#ffeec1','#ffdea7','#ffcb91','#ffb880','#ffa474','#fe906a','#f87d64','#f06a5e','#e75758','#db4551','#cf3447','#c0223b','#b0122c','#9e051b','#8b0000']\n",
    "year_built_colors = ['#ffffe0','#ffdaa3','#ffb27c','#fb8768','#eb5f5b','#d3394a','#b3152f','#8b0000']\n",
    "img = Image.new( 'RGB', (255,255), \"white\") # create a new image\n",
    "pixels = img.load() # create the pixel map\n",
    "for i in range(0,len(parcel_colors)):\n",
    "    pixels[0,i] = HexToRgb(parcel_colors[i]) # set the colour accordingly\n",
    "j = 0\n",
    "for i in list(reversed(range(len(year_built_colors)))):\n",
    "    pixels[1,j] = HexToRgb(year_built_colors[i]) # set the colour accordingly\n",
    "    j += 1\n",
    "#img.show()\n",
    "img.save(\"assessments/color-map.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class_descriptions = sorted(class_descs.keys())\n",
    "points = []\n",
    "array.array('f', points).tofile(open('parcels_class.bin', 'wb'))\n",
    "    par_id = apd['PARID'][i]\n",
    "    year_built = apd['YEARBLT'][i]\n",
    "    class_description = apd['CLASSDESC'][i]   \n",
    "    if year_built == '' or year_built == '0':\n",
    "        year_built = '0001' # Null Value, -62135596800.0 Epoch\n",
    "    if centroids.has_key(par_id):\n",
    "        centroid = centroids[par_id]\n",
    "        points += LonLatToPixelXY(centroid)        \n",
    "        points.append(float(year_built))\n",
    "        points.append(class_descriptions.index(class_description)+1.0)\n",
    "array.array('f', points).tofile(open('assessments/parcels_class.bin', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sorted(class_descs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class_descriptions.index('RESIDENTIAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "points[10000:10005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def scale_year(year):\n",
    "    if year < 1800.0: \n",
    "        year = 1800.0\n",
    "    return int(((year - 1800.0) * 8.0) / (2016.0-1800.))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scale_year(1800.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(year_built_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "j = 0\n",
    "for i in list(reversed(range(len(year_built_colors)))):\n",
    "    print '<rect fill=\"%s\" x=\"%s\" height=\"10\" width=\"25\" stroke-width=\"1px\" stroke=\"#666\"></rect>' % (year_built_colors[i],j*25)\n",
    "    j += 1\n",
    "    \n",
    "j = 0\n",
    "for i in list(reversed(range(len(year_built_colors)))):\n",
    "    print '<text font-size=\"10.5px\" fill=\"%s\" y=\"29\" x=\"%s\">%s</text>' % (year_built_colors[i], j*25, 1800)\n",
    "    j += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list(reversed(range(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(fairmarket_totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fm_keys = sorted(map(int,fairmarket_totals.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "numpy.average(fm_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "numpy.mean(fm_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "numpy.std(fm_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fairmarket_totals['50000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parcel_colors = ['#fb3059','#fe6b2d','#d1947a','#c6a900','#02ca75','#00a2de','#9529b1']\n",
    "year_built_colors = ['#ffffe0','#ffdaa3','#ffb27c','#fb8768','#eb5f5b','#d3394a','#b3152f','#8b0000']\n",
    "fm_colors = ['#a50026','#cd2827','#e75436','#f7804b','#fdad61','#fed788','#ffffbf','#b9e0ed','#8dc0db','#699fca','#4d7db9','#3e5aa7','#313695'\n",
    "]\n",
    "\n",
    "img = Image.new( 'RGB', (255,255), \"white\") # create a new image\n",
    "pixels = img.load() # create the pixel map\n",
    "for i in range(0,len(parcel_colors)):\n",
    "    pixels[0,i] = HexToRgb(parcel_colors[i]) # set the colour accordingly\n",
    "j = 0\n",
    "for i in list(reversed(range(len(year_built_colors)))):\n",
    "    pixels[1,j] = HexToRgb(year_built_colors[i]) # set the colour accordingly\n",
    "    j += 1\n",
    "for i in range(0,len(fm_colors)):\n",
    "    pixels[2,i] = HexToRgb(fm_colors[i]) # set the colour accordingly\n",
    "    \n",
    "#img.show()\n",
    "img.save(\"assessments/color-map.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class_descriptions = sorted(class_descs.keys())\n",
    "points = []\n",
    "for i in range(0, len(apd.index)):\n",
    "    par_id = apd['PARID'][i]\n",
    "    year_built = apd['YEARBLT'][i]\n",
    "    class_description = apd['CLASSDESC'][i] \n",
    "    fm_total = float(apd['FAIRMARKETTOTAL'][i])    \n",
    "    if year_built == '' or year_built == '0' or year_built == '0000' or math.isnan(year_built):\n",
    "        year_built = '0001' # Null Value, -62135596800.0 Epoch\n",
    "    elif isinstance(year_built, numbers.Number):\n",
    "        if(year_built < 1):\n",
    "            year_built = '0001' # Null Value, -62135596800.0 Epoch\n",
    "        else:\n",
    "            year_built = \"%04d\" % (year_built)\n",
    "    if centroids.has_key(par_id):\n",
    "        centroid = centroids[par_id]\n",
    "        points += LonLatToPixelXY(centroid)        \n",
    "        points.append(float(year_built))\n",
    "        points.append(class_descriptions.index(class_description)+1.0)\n",
    "        points.append(fm_total)\n",
    "array.array('f', points).tofile(open(('assessments/parcels_all%s.bin'%out_suffix), 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sorted(class_descs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Below is Anne's attempt to put floating point representations of colors into the .bin file for a new \n",
    "# shader Randy's writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parcel_colors = ['#fb3059','#fe6b2d','#d1947a','#c6a900','#02ca75','#00a2de','#9529b1']\n",
    "class_descriptions = sorted(class_descs.keys())\n",
    "\n",
    "def class_desc_to_color(class_description):\n",
    "    index = class_descriptions.index(class_description)\n",
    "    return(parse_color(parcel_colors[index]))\n",
    "\n",
    "points = []\n",
    "for i in range(0, len(apd.index)):\n",
    "    par_id = apd['PARID'][i]\n",
    "    class_description = apd['CLASSDESC'][i] \n",
    "    if centroids.has_key(par_id):\n",
    "        centroid = centroids[par_id]\n",
    "        points += LonLatToPixelXY(centroid)        \n",
    "        points.append(class_desc_to_color(class_description))\n",
    "array.array('f', points).tofile(open(('assessments/parcels_class_color%s.bin'%out_suffix), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(fm_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fairmarket_totals['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(map(int,fairmarket_totals.keys()), fairmarket_totals.values(), 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fm_keys[len(fm_keys) - 100:len(fm_keys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fm_values = [\"0\", \"25K\", \"50K\", \"75K\", \"100K\", \"125K\", \"150K\", \"200K\", \"250K\", \"300K\", \"400K\", \"500K\", \"+500K\"]\n",
    "for i in range(len(fm_colors)):\n",
    "    print '<rect fill=\"%s\" x=\"%s\" height=\"10\" width=\"30\"></rect>' % (fm_colors[i],i*30)\n",
    "    \n",
    "for i in range(len(fm_values)):\n",
    "    print '<text font-size=\"10.5px\" y=\"29\" x=\"%s\">%s</text>' % (i*30, fm_values[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
