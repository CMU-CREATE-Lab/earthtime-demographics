{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the last version that used preservationdatabase/Active and Inconclusive Properties Pgh.xlsx\n",
    "# It's got a first cut at displaying REAC scores.\n",
    "# The next version is going to use Allegheny_County_All_Properties.csv, \n",
    "# the Allegheny county subset of the full preservation database\n",
    "# generated by all-properties-preservation-database.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#notebook-container { margin-left:-14px; width:calc(100% + 27px) !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Wide display\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>#notebook-container { margin-left:-14px; width:calc(100% + 27px) !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, json, os, math, numbers, pandas, re, scipy, scipy.sparse, shutil\n",
    "import subprocess, sys, threading, time, urllib2\n",
    "\n",
    "def exec_ipynb(filename_or_url):\n",
    "    nb = (urllib2.urlopen(filename_or_url) if re.match(r'https?:', filename_or_url) else open(filename_or_url)).read()\n",
    "    jsonNb = json.loads(nb)\n",
    "    #check for the modified formatting of Jupyter Notebook v4\n",
    "    if(jsonNb['nbformat'] == 4):\n",
    "        exec '\\n'.join([''.join(cell['source']) for cell in jsonNb['cells'] if cell['cell_type'] == 'code']) in globals()\n",
    "    else:\n",
    "        exec '\\n'.join([''.join(cell['input']) for cell in jsonNb['worksheets'][0]['cells'] if cell['cell_type'] == 'code']) in globals()\n",
    "\n",
    "exec_ipynb('timelapse-utilities.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.options.display.max_colwidth = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "earliest_year=2017\n",
    "latest_year=2051\n",
    "def process_preservationdatabase_xls(path, program_arr):\n",
    "    global earliest_year,latest_year\n",
    "    parsed_pd_data=[]\n",
    "    pd_data = pandas.read_excel(path)\n",
    "    \n",
    "    for i in range(0,len(pd_data.index)):\n",
    "        zip = pd_data['Zip'][i]\n",
    "        try:\n",
    "            rec = {'row':i}\n",
    "            rec['zip']= zip\n",
    "            rec['lat']= pd_data['Latitude'][i]\n",
    "            rec['lon']= pd_data['Longitude'][i]\n",
    "            rec['total_units'] = pd_data['TotalUnits'][i]\n",
    "            rec['TargetPopulation'] = str(pd_data['TargetPopulation'][i])\n",
    "            # Read in reac scores\n",
    "            reac_years = []\n",
    "            reac_vals = []\n",
    "            for j in range(3,0,-1):\n",
    "                reac_str = \"ReacScore%d\"%(j)\n",
    "                # Check if this reac score is missing\n",
    "                if(pandas.isnull(pd_data['%s'%(reac_str)][i]) or pandas.isnull(pd_data['%sDate'%(reac_str)][i])):\n",
    "                    continue\n",
    "                # Something non-null is there, hope it's valid!\n",
    "                reac_years.append(pandas.to_datetime(pd_data['%sDate'%(reac_str)][i]).date().year)\n",
    "                reac_vals.append(pd_data['%s'%(reac_str)][i])\n",
    "            rec['reac_years'] = reac_years\n",
    "            rec['reac_vals'] = reac_vals\n",
    "            if(len(program_arr)==0):\n",
    "                # There's no set of programs specified, just set year range to full range we've seen\n",
    "                rec['start_year']=earliest_year\n",
    "                rec['end_year']=latest_year\n",
    "                parsed_pd_data.append(rec)\n",
    "                continue\n",
    "            # If we get to here, then program_arr isn't empty\n",
    "            for program in program_arr:\n",
    "                asst_num = pd_data['%s_AssistedUnits'%(program)][i]\n",
    "                \n",
    "                # If this program doesn't have an _AssistedUnits value, only include start_year and end_year for all we've seen\n",
    "                if(asst_num == '' or math.isnan(asst_num) or asst_num<1):\n",
    "                    # There's no units for this program, skip it\n",
    "                    pass\n",
    "                else:\n",
    "                    rec['assisted_units']= asst_num\n",
    "                    rec['start_year'] = pandas.to_datetime(pd_data['%s_StartDate'%(program)][i]).date().year\n",
    "                    rec['end_year'] = pandas.to_datetime(pd_data['%s_EndDate'%(program)][i]).date().year\n",
    "                    if(rec['start_year']<earliest_year):\n",
    "                        earliest_year=rec['start_year']\n",
    "                    if(rec['end_year']>latest_year):\n",
    "                        latest_year=rec['end_year']\n",
    "                    parsed_pd_data.append(rec)\n",
    "                    #print \"Adding row %d, zip %s, %d %s units\" % (i,zip,asst_num,program)\n",
    "        except:\n",
    "            print \"%s: Skipping row %d, zip %s, due to error\" % (program_arr,i,zip)\n",
    "    return parsed_pd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgh_path = \"preservationdatabase/Active and Inconclusive Properties Pgh.xlsx\"\n",
    "parsed_pgh_data_s8 = process_preservationdatabase_xls(pgh_path,[\"S8_1\",\"S8_2\"])\n",
    "#parsed_pgh_data_s8_1 = process_preservationdatabase_xls(pgh_path,[\"S8_1\"])\n",
    "#parsed_pgh_data_s8_2 = process_preservationdatabase_xls(pgh_path,[\"S8_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parsed_pgh_data_s8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parsed_pgh_data_s8_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parsed_pgh_data_s8_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-4b9a93d8b268>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_pgh_data_s8_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'parsed_pgh_data_s8_2' is not defined"
     ]
    }
   ],
   "source": [
    "len(parsed_pgh_data_s8_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'write_pd_csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-b6e456f952d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwrite_pd_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_pgh_data_s8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2017\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2051\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"preservationdatabase/pgh_current_S8_m2_2018.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'current'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'write_pd_csv' is not defined"
     ]
    }
   ],
   "source": [
    "write_pd_csv(parsed_pgh_data_s8, 2017,2051,\"preservationdatabase/pgh_current_S8_m2_2018.csv\",'current')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'TargetPopulation': 'Family',\n",
       "  'assisted_units': 108.0,\n",
       "  'end_year': 2017,\n",
       "  'lat': 40.442909240722656,\n",
       "  'lon': -80.00183868408203,\n",
       "  'reac_vals': [u'45c*', u'69c*', u'69c*'],\n",
       "  'reac_years': [2015, 2016, 2016],\n",
       "  'row': 2,\n",
       "  'start_year': 2017,\n",
       "  'total_units': 191,\n",
       "  'zip': u'15222-3206'},\n",
       " {'TargetPopulation': 'Family',\n",
       "  'assisted_units': 64.0,\n",
       "  'end_year': 2034,\n",
       "  'lat': 40.44260025024414,\n",
       "  'lon': -80.00019836425781,\n",
       "  'reac_vals': [u'67c', u'89b', u'89b'],\n",
       "  'reac_years': [2014, 2015, 2015],\n",
       "  'row': 3,\n",
       "  'start_year': 2014,\n",
       "  'total_units': 94,\n",
       "  'zip': u'15222-3103'},\n",
       " {'TargetPopulation': 'Disabled',\n",
       "  'assisted_units': 14.0,\n",
       "  'end_year': 2018,\n",
       "  'lat': 40.43899154663086,\n",
       "  'lon': -79.98372650146484,\n",
       "  'reac_vals': [u'85b', u'81b', u'81b'],\n",
       "  'reac_years': [2014, 2016, 2016],\n",
       "  'row': 7,\n",
       "  'start_year': 2017,\n",
       "  'total_units': 15,\n",
       "  'zip': u'15219-5596'},\n",
       " {'TargetPopulation': 'Family',\n",
       "  'assisted_units': 63.0,\n",
       "  'end_year': 2031,\n",
       "  'lat': 40.44309997558594,\n",
       "  'lon': -79.98210144042969,\n",
       "  'reac_vals': [u'92b*', u'76c*', u'76c*'],\n",
       "  'reac_years': [2011, 2016, 2016],\n",
       "  'row': 8,\n",
       "  'start_year': 2001,\n",
       "  'total_units': 63,\n",
       "  'zip': u'15219-4305'},\n",
       " {'TargetPopulation': 'Elderly',\n",
       "  'assisted_units': 100.0,\n",
       "  'end_year': 2032,\n",
       "  'lat': 40.44287872314453,\n",
       "  'lon': -79.98017120361328,\n",
       "  'reac_vals': [u'88b', u'86c', u'86c'],\n",
       "  'reac_years': [2014, 2016, 2016],\n",
       "  'row': 9,\n",
       "  'start_year': 2012,\n",
       "  'total_units': 101,\n",
       "  'zip': u'15219-3381'},\n",
       " {'TargetPopulation': 'Elderly',\n",
       "  'assisted_units': 190.0,\n",
       "  'end_year': 2034,\n",
       "  'lat': 40.4439811706543,\n",
       "  'lon': -79.98695373535156,\n",
       "  'reac_vals': [u'78c', u'84b', u'84b'],\n",
       "  'reac_years': [2013, 2016, 2016],\n",
       "  'row': 10,\n",
       "  'start_year': 2014,\n",
       "  'total_units': 190,\n",
       "  'zip': u'15219-4147'},\n",
       " {'TargetPopulation': 'Elderly',\n",
       "  'assisted_units': 30.0,\n",
       "  'end_year': 2035,\n",
       "  'lat': 40.439659118652344,\n",
       "  'lon': -79.9832992553711,\n",
       "  'reac_vals': [u'72c*', u'65c', u'65c'],\n",
       "  'reac_years': [2015, 2016, 2016],\n",
       "  'row': 11,\n",
       "  'start_year': 1975,\n",
       "  'total_units': 30,\n",
       "  'zip': u'15221-2864'},\n",
       " {'TargetPopulation': 'Family',\n",
       "  'assisted_units': 66.0,\n",
       "  'end_year': 2035,\n",
       "  'lat': 40.44483184814453,\n",
       "  'lon': -79.98571014404297,\n",
       "  'reac_vals': [u'86c', u'93b*', u'93b*'],\n",
       "  'reac_years': [2013, 2016, 2016],\n",
       "  'row': 12,\n",
       "  'start_year': 2005,\n",
       "  'total_units': 66,\n",
       "  'zip': u'15219-3606'},\n",
       " {'TargetPopulation': 'Family',\n",
       "  'assisted_units': 14.0,\n",
       "  'end_year': 2036,\n",
       "  'lat': 40.43968963623047,\n",
       "  'lon': -79.98041534423828,\n",
       "  'reac_vals': [],\n",
       "  'reac_years': [],\n",
       "  'row': 13,\n",
       "  'start_year': 2012,\n",
       "  'total_units': 23,\n",
       "  'zip': u'15219-3336'},\n",
       " {'TargetPopulation': 'Family',\n",
       "  'assisted_units': 14.0,\n",
       "  'end_year': 2036,\n",
       "  'lat': 40.440059661865234,\n",
       "  'lon': -79.98204040527344,\n",
       "  'reac_vals': [],\n",
       "  'reac_years': [],\n",
       "  'row': 14,\n",
       "  'start_year': 2012,\n",
       "  'total_units': 23,\n",
       "  'zip': u'15219-3358'}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_pgh_data_s8[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'TargetPopulation': 'Family',\n",
       "  'assisted_units': 46.0,\n",
       "  'end_year': 2031,\n",
       "  'lat': 40.456630706787109,\n",
       "  'lon': -79.900993347167969,\n",
       "  'row': 92,\n",
       "  'start_year': 2001,\n",
       "  'total_units': 71,\n",
       "  'zip': u'15208-1767'},\n",
       " {'TargetPopulation': 'Family',\n",
       "  'assisted_units': 66.0,\n",
       "  'end_year': 2031,\n",
       "  'lat': 40.454559326171875,\n",
       "  'lon': -79.869346618652344,\n",
       "  'row': 98,\n",
       "  'start_year': 2004,\n",
       "  'total_units': 117,\n",
       "  'zip': u'15221-1079'},\n",
       " {'TargetPopulation': 'Elderly',\n",
       "  'assisted_units': 47.0,\n",
       "  'end_year': 2017,\n",
       "  'lat': 40.413669586181641,\n",
       "  'lon': -79.921562194824219,\n",
       "  'row': 114,\n",
       "  'start_year': 2017,\n",
       "  'total_units': 137,\n",
       "  'zip': u'15217-3231'},\n",
       " {'TargetPopulation': 'Disabled',\n",
       "  'assisted_units': 8.0,\n",
       "  'end_year': 2018,\n",
       "  'lat': 40.496971130371094,\n",
       "  'lon': -79.864547729492188,\n",
       "  'row': 185,\n",
       "  'start_year': 2017,\n",
       "  'total_units': 13,\n",
       "  'zip': u'15238-2932'},\n",
       " {'TargetPopulation': 'Elderly',\n",
       "  'assisted_units': 58.0,\n",
       "  'end_year': 2030,\n",
       "  'lat': 40.440719604492188,\n",
       "  'lon': -80.068206787109375,\n",
       "  'row': 240,\n",
       "  'start_year': 2010,\n",
       "  'total_units': 103,\n",
       "  'zip': u'15205-2771'},\n",
       " {'TargetPopulation': 'Elderly or disabled',\n",
       "  'assisted_units': 18.0,\n",
       "  'end_year': 2032,\n",
       "  'lat': 40.475898742675781,\n",
       "  'lon': -79.78515625,\n",
       "  'row': 363,\n",
       "  'start_year': 2012,\n",
       "  'total_units': 24,\n",
       "  'zip': u'15235-3653'},\n",
       " {'TargetPopulation': 'Family',\n",
       "  'assisted_units': 23.0,\n",
       "  'end_year': 2031,\n",
       "  'lat': 40.350616455078125,\n",
       "  'lon': -79.85736083984375,\n",
       "  'row': 390,\n",
       "  'start_year': 2004,\n",
       "  'total_units': 117,\n",
       "  'zip': u'15132-2301'},\n",
       " {'TargetPopulation': 'Elderly',\n",
       "  'assisted_units': 38.0,\n",
       "  'end_year': 2031,\n",
       "  'lat': 40.350639343261719,\n",
       "  'lon': -79.862083435058594,\n",
       "  'row': 391,\n",
       "  'start_year': 2004,\n",
       "  'total_units': 132,\n",
       "  'zip': u'15132-3034'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_pgh_data_s8_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1975"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earliest_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2051"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2017,2018,2019,2020,2021,2022,2023,2024,2025,2026,2027,2028,2029,2030,2031,2032,2033,2034,2035,2036,2037,2038,2039,2040,2041,2042,2043,2044,2045,2046,2047,2048,2049,2050'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\",\".join(map(str,range(2017,2051)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_pd_csv(parsed_pd_data, start_year, end_year, out_path, out_type):\n",
    "    date_range = range(start_year, end_year+1)\n",
    "    out = open(out_path, 'w')\n",
    "    # Write out header row.  First column doesn't have a column heading, next two are lat, lon, then each year\n",
    "    out.write(\",lat,lon,%s\\n\" % (\",\".join(map(str,date_range))))\n",
    "    \n",
    "    for rec in parsed_pd_data:\n",
    "        start_year = rec['start_year']\n",
    "        end_year = rec['end_year']\n",
    "        out_data=[\"%s (%d-%d)\" %(rec['zip'],start_year,end_year), rec['lat'], rec['lon']]\n",
    "        \n",
    "        for year in date_range:\n",
    "            out_val = 0\n",
    "            # If csv type is:\n",
    "            #  'all' or 'total', use total_units\n",
    "            #  'current', use size for year >= start_year and year< end_year (green)\n",
    "            #  'expiring', use size for year == end_year (yellow)\n",
    "            #  'expired', use size for year > end_year (red)\n",
    "            if(out_type == 'all' or (out_type == 'total' and not 'assisted_units' in rec.keys())):\n",
    "                out_val = rec['total_units']\n",
    "            elif('assisted_units' in rec.keys() and\n",
    "                 ((out_type == 'current' and year >= start_year and year< end_year) or\n",
    "                  (out_type == 'expiring' and year==end_year) or\n",
    "                  (out_type == 'expired' and year>end_year))):\n",
    "                out_val = rec['assisted_units']\n",
    "            else:\n",
    "                # Assume this is a substring match for TargetPopulation \n",
    "                if(out_type in rec['TargetPopulation'].lower()):\n",
    "                    out_val = rec['total_units']\n",
    "            out_data.append(out_val)\n",
    "        # Write total_deaths for country here\n",
    "        out.write('%s\\n' % (\",\".join(map(str,out_data))))\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This uses colormap https://tiles.earthtime.org/colormaps/red-yellow-green.png\n",
    "current_val=1\n",
    "expiring_val=0.5\n",
    "expired_val=0\n",
    "def write_combo_pd_csv(parsed_pd_data, start_year, end_year, out_path):\n",
    "    date_range = range(start_year, end_year+1)\n",
    "    out = open(out_path, 'w')\n",
    "    # Write out header row.  First column doesn't have a column heading, next two are lat, lon, then each year\n",
    "    out.write(\",lat,lon,%s\\n\" % (\",\".join(map(str,date_range))))\n",
    "    \n",
    "    for rec in parsed_pd_data:\n",
    "        start_year = rec['start_year']\n",
    "        end_year = rec['end_year']\n",
    "\n",
    "        #Output two rows, both with the same label.  First row is bubble size.  Second row is 0 - 1 for status\n",
    "        row_label = \"%s (%d-%d)\" %(rec['zip'],start_year,end_year)\n",
    "        units=0\n",
    "        if(not 'assisted_units' in rec.keys()):\n",
    "            units = rec['total_units']\n",
    "        else:\n",
    "            units = rec['assisted_units']\n",
    "        bubble_size = units\n",
    "        \n",
    "        # Bubble size row -- size is constant\n",
    "        size_row=[row_label, rec['lat'], rec['lon']]\n",
    "\n",
    "        for year in date_range:\n",
    "            size_row.append(bubble_size)\n",
    "            \n",
    "        # Bubble color row -- value depends on status\n",
    "        color_row=[row_label, rec['lat'], rec['lon']]\n",
    "\n",
    "        for year in date_range:\n",
    "            color_val=0\n",
    "            if (year >= start_year and year< end_year):\n",
    "                color_val=current_val\n",
    "            elif (year==end_year):\n",
    "                color_val=expiring_val\n",
    "            elif (year>end_year):\n",
    "                color_val=expired_val\n",
    "            color_row.append(color_val)\n",
    "            \n",
    "        # Write both rows for this property out here\n",
    "        out.write('%s\\n' % (\",\".join(map(str,size_row))))\n",
    "        out.write('%s\\n' % (\",\".join(map(str,color_row))))\n",
    "        \n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This uses colormap https://tiles.earthtime.org/colormaps/grey-red-yellow-green.png\n",
    "a_val=1\n",
    "b_val=0.666\n",
    "c_val=0.333\n",
    "unknown_val=0\n",
    "def write_reac_code_pd_csv(parsed_pd_data, start_year, end_year, out_path):\n",
    "    date_range = range(start_year, end_year+1)\n",
    "    out = open(out_path, 'w')\n",
    "    # Write out header row.  First column doesn't have a column heading, next two are lat, lon, then each year\n",
    "    out.write(\",lat,lon,%s\\n\" % (\",\".join(map(str,date_range))))\n",
    "    \n",
    "    for rec in parsed_pd_data:\n",
    "        # If we have no reac scores, skip this one\n",
    "        if(len(rec['reac_years'])==0):\n",
    "            continue\n",
    "            \n",
    "        # Set start_year to be earliest reac_year and end_year to be latest reac_year\n",
    "        start_year = rec['reac_years'][0]\n",
    "        end_year = rec['reac_years'][len(rec['reac_years'])-1]\n",
    "\n",
    "        #Output two rows, both with the same label.  First row is bubble size.  Second row is 0 - 1 for status\n",
    "        row_label = \"%s (%d-%d)\" %(rec['zip'],start_year,end_year)\n",
    "        units=0\n",
    "        if(not 'assisted_units' in rec.keys()):\n",
    "            units = rec['total_units']\n",
    "        else:\n",
    "            units = rec['assisted_units']\n",
    "        bubble_size = units\n",
    "        \n",
    "        # Bubble size row -- size is constant\n",
    "        size_row=[row_label, rec['lat'], rec['lon']]\n",
    "\n",
    "        for year in date_range:\n",
    "            size_row.append(bubble_size)\n",
    "            \n",
    "        # Bubble color row -- value depends on status\n",
    "        color_row=[row_label, rec['lat'], rec['lon']]\n",
    "\n",
    "        # Earliest year will be \n",
    "        reac_i = 0\n",
    "        last_reac_val = rec['reac_vals'][0]\n",
    "        for year in date_range:\n",
    "            color_val=0\n",
    "            # Check if we need to increment reac_i to a later test date\n",
    "            while(year>rec['reac_years'][reac_i] and reac_i<(len(rec['reac_years'])-1)):\n",
    "                last_reac_val = rec['reac_vals'][reac_i]\n",
    "                reac_i=reac_i+1\n",
    "            # If we've caught up to the last reac_year, set last_reac_val to the end one\n",
    "            if(reac_i==(len(rec['reac_years'])-1) and year>=end_year):\n",
    "                last_reac_val = rec['reac_vals'][reac_i]\n",
    "                \n",
    "            # At this point last_reac_val is valid for this year so long as year>=start_year.\n",
    "            # If year < start_year, set color to unknown_val\n",
    "            #if (year < start_year):\n",
    "            #    color_val = unknown_val\n",
    "            #else:\n",
    "            \n",
    "            # Randy doesn't want it to start as grey\n",
    "            if(True):\n",
    "                if('a' in last_reac_val):\n",
    "                    color_val = a_val\n",
    "                elif('b' in last_reac_val):\n",
    "                    color_val = b_val\n",
    "                elif('c' in last_reac_val):\n",
    "                    color_val = c_val\n",
    "                else:\n",
    "                    color_val = unknown_val\n",
    "            color_row.append(color_val)\n",
    "            \n",
    "        # Write both rows for this property out here\n",
    "        out.write('%s\\n' % (\",\".join(map(str,size_row))))\n",
    "        out.write('%s\\n' % (\",\".join(map(str,color_row))))\n",
    "        \n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This uses colormap https://tiles.earthtime.org/colormaps/grey-red-yellow-green.png\n",
    "unknown_val=0\n",
    "def write_reac_val_pd_csv(parsed_pd_data, start_year, end_year, out_path):\n",
    "    date_range = range(start_year, end_year+1)\n",
    "    out = open(out_path, 'w')\n",
    "    \n",
    "    # Setup regular expression for parsing reac score\n",
    "    reac_re = re.compile('(\\d+)([abc])(\\*?)')\n",
    "\n",
    "    # Write out header row.  First column doesn't have a column heading, next two are lat, lon, then each year\n",
    "    out.write(\",lat,lon,%s\\n\" % (\",\".join(map(str,date_range))))\n",
    "    \n",
    "    for rec in parsed_pd_data:\n",
    "        # If we have no reac scores, skip this one\n",
    "        if(len(rec['reac_years'])==0):\n",
    "            continue\n",
    "            \n",
    "        # Set start_year to be earliest reac_year and end_year to be latest reac_year\n",
    "        start_year = rec['reac_years'][0]\n",
    "        end_year = rec['reac_years'][len(rec['reac_years'])-1]\n",
    "\n",
    "        #Output two rows, both with the same label.  First row is bubble size.  Second row is 0 - 1 for status\n",
    "        row_label = \"%s (%d-%d)\" %(rec['zip'],start_year,end_year)\n",
    "        units=0\n",
    "        if(not 'assisted_units' in rec.keys()):\n",
    "            units = rec['total_units']\n",
    "        else:\n",
    "            units = rec['assisted_units']\n",
    "        bubble_size = units\n",
    "        \n",
    "        # Bubble size row -- size is constant\n",
    "        size_row=[row_label, rec['lat'], rec['lon']]\n",
    "\n",
    "        for year in date_range:\n",
    "            size_row.append(bubble_size)\n",
    "            \n",
    "        # Bubble color row -- value depends on status\n",
    "        color_row=[row_label, rec['lat'], rec['lon']]\n",
    "\n",
    "        # Earliest year will be \n",
    "        reac_i = 0\n",
    "        last_reac_val = rec['reac_vals'][0]\n",
    "        for year in date_range:\n",
    "            color_val=0\n",
    "            # Check if we need to increment reac_i to a later test date\n",
    "            while(year>rec['reac_years'][reac_i] and reac_i<(len(rec['reac_years'])-1)):\n",
    "                last_reac_val = rec['reac_vals'][reac_i]\n",
    "                reac_i=reac_i+1\n",
    "            # If we've caught up to the last reac_year, set last_reac_val to the end one\n",
    "            if(reac_i==(len(rec['reac_years'])-1) and year>=end_year):\n",
    "                last_reac_val = rec['reac_vals'][reac_i]\n",
    "                \n",
    "            # At this point last_reac_val is valid for this year so long as year>=start_year.\n",
    "            # If year < start_year, set color to unknown_val\n",
    "            #if (year < start_year):\n",
    "            #    color_val = unknown_val\n",
    "            #else:\n",
    "            # Randy doesn't want it to start as grey\n",
    "            if(True):\n",
    "                # Strip the number from the front of the reac string\n",
    "                m = reac_re.match(last_reac_val)\n",
    "                # The number will be in the first group\n",
    "                color_val = int(m.group(1))\n",
    "            color_row.append(color_val)\n",
    "            \n",
    "        # Write both rows for this property out here\n",
    "        out.write('%s\\n' % (\",\".join(map(str,size_row))))\n",
    "        out.write('%s\\n' % (\",\".join(map(str,color_row))))\n",
    "        \n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'94'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reac_str = \"94c*\"\n",
    "p = re.compile('(\\d+)([abc])(\\*?)')\n",
    "m = p.match(reac_str)\n",
    "m.groups()\n",
    "m.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgh_path = \"preservationdatabase/Active and Inconclusive Properties Pgh.xlsx\"\n",
    "# Removed \"S8_2\" because S8_2_AssistedUnits1, S8_2_StartTime1, S8_2_EndTime1 don't follow the pattern\n",
    "#for program in [\"S8_1\",\"S202_1\",\"S202_2\",\"S236_1\",\"S236_2\",\"FHA_1\",\"FHA_2\",\"LIHTC_1\",\"LIHTC_2\"]:\n",
    "for program_info in [{'name':'S8_m','programs':[\"S8_1\",\"S8_2\"]},\n",
    "                     {'name':'S202_m','programs':[\"S202_1\",\"S202_2\"]},\n",
    "                     {'name':'S236_m','programs':[\"S236_1\",\"S236_2\"]},\n",
    "                     {'name':'FHA_m','programs':[\"FHA_1\",\"FHA_2\"]},\n",
    "                     {'name':'LIHTC_m','programs':[\"LIHTC_1\",\"LIHTC_2\"]}]:\n",
    "    program = program_info['name']\n",
    "    program_arr = program_info['programs']\n",
    "    parsed_pgh_data = process_preservationdatabase_xls(pgh_path,program_arr)\n",
    "\n",
    "    #write_pd_csv(parsed_pgh_data, 2017,2051,\"preservationdatabase/pgh_current_%s_2018.csv\"%(program),'current')\n",
    "    #write_pd_csv(parsed_pgh_data, 2017,2051,\"preservationdatabase/pgh_expiring_%s_2018.csv\"%(program),'expiring')\n",
    "    #write_pd_csv(parsed_pgh_data, 2017,2051,\"preservationdatabase/pgh_expired_%s_2018.csv\"%(program),'expired')\n",
    "    #write_pd_csv(parsed_pgh_data, 2017,2051,\"preservationdatabase/pgh_total_%s_2018.csv\"%(program),'total')\n",
    "    write_combo_pd_csv(parsed_pgh_data, 2017,2051,\"preservationdatabase/pgh_combo_%s_2018.csv\"%(program))\n",
    "    write_reac_code_pd_csv(parsed_pgh_data, 2003,2018,\"preservationdatabase/pgh_reac_code_%s_2018.csv\"%(program))\n",
    "    write_reac_val_pd_csv(parsed_pgh_data, 2003,2018,\"preservationdatabase/pgh_reac_val_%s_2018.csv\"%(program))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_pgh_data = process_preservationdatabase_xls(pgh_path,[])\n",
    "write_pd_csv(parsed_pgh_data, 2017,2051,\"preservationdatabase/pgh_all_2018.csv\",'all')\n",
    "write_pd_csv(parsed_pgh_data, 2017,2051,\"preservationdatabase/pgh_elderly_2018.csv\",'elderly')\n",
    "write_pd_csv(parsed_pgh_data, 2017,2051,\"preservationdatabase/pgh_family_2018.csv\",'family')\n",
    "write_pd_csv(parsed_pgh_data, 2017,2051,\"preservationdatabase/pgh_disabled_2018.csv\",'disabled')\n",
    "write_pd_csv(parsed_pgh_data, 2017,2051,\"preservationdatabase/pgh_mixed_2018.csv\",'mixed')\n",
    "write_reac_code_pd_csv(parsed_pgh_data, 2003,2018,\"preservationdatabase/pgh_reac_code_all_2018.csv\")\n",
    "write_reac_val_pd_csv(parsed_pgh_data, 2003,2018,\"preservationdatabase/pgh_reac_val_all_2018.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "439"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parsed_pgh_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://tiles.earthtime.org/preservationdatabase/pgh_current_S8_m_2018.csv\n",
      "https://tiles.earthtime.org/preservationdatabase/pgh_expiring_S8_m_2018.csv\n",
      "https://tiles.earthtime.org/preservationdatabase/pgh_expired_S8_m_2018.csv\n",
      "https://tiles.earthtime.org/preservationdatabase/pgh_total_S8_m_2018.csv\n",
      "https://tiles.earthtime.org/preservationdatabase/pgh_current_S202_m_2018.csv\n",
      "https://tiles.earthtime.org/preservationdatabase/pgh_expiring_S202_m_2018.csv\n",
      "https://tiles.earthtime.org/preservationdatabase/pgh_expired_S202_m_2018.csv\n",
      "https://tiles.earthtime.org/preservationdatabase/pgh_total_S202_m_2018.csv\n",
      "https://tiles.earthtime.org/preservationdatabase/pgh_current_S236_m_2018.csv\n",
      "https://tiles.earthtime.org/preservationdatabase/pgh_expiring_S236_m_2018.csv\n",
      "https://tiles.earthtime.org/preservationdatabase/pgh_expired_S236_m_2018.csv\n",
      "https://tiles.earthtime.org/preservationdatabase/pgh_total_S236_m_2018.csv\n",
      "https://tiles.earthtime.org/preservationdatabase/pgh_current_FHA_m_2018.csv\n",
      "https://tiles.earthtime.org/preservationdatabase/pgh_expiring_FHA_m_2018.csv\n",
      "https://tiles.earthtime.org/preservationdatabase/pgh_expired_FHA_m_2018.csv\n",
      "https://tiles.earthtime.org/preservationdatabase/pgh_total_FHA_m_2018.csv\n",
      "https://tiles.earthtime.org/preservationdatabase/pgh_current_LIHTC_m_2018.csv\n",
      "https://tiles.earthtime.org/preservationdatabase/pgh_expiring_LIHTC_m_2018.csv\n",
      "https://tiles.earthtime.org/preservationdatabase/pgh_expired_LIHTC_m_2018.csv\n",
      "https://tiles.earthtime.org/preservationdatabase/pgh_total_LIHTC_m_2018.csv\n"
     ]
    }
   ],
   "source": [
    "for program_info in [{'name':'S8_m','programs':[\"S8_1\",\"S8_2\"]},\n",
    "                     {'name':'S202_m','programs':[\"S202_1\",\"S202_2\"]},\n",
    "                     {'name':'S236_m','programs':[\"S236_1\",\"S236_2\"]},\n",
    "                     {'name':'FHA_m','programs':[\"FHA_1\",\"FHA_2\"]},\n",
    "                     {'name':'LIHTC_m','programs':[\"LIHTC_1\",\"LIHTC_2\"]}]:\n",
    "    program = program_info['name']\n",
    "    print \"https://tiles.earthtime.org/preservationdatabase/pgh_current_%s_2018.csv\"%(program)\n",
    "    print \"https://tiles.earthtime.org/preservationdatabase/pgh_expiring_%s_2018.csv\"%(program)\n",
    "    print \"https://tiles.earthtime.org/preservationdatabase/pgh_expired_%s_2018.csv\"%(program)\n",
    "    print \"https://tiles.earthtime.org/preservationdatabase/pgh_total_%s_2018.csv\"%(program)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download File Templates for 5-year data\n",
    "\n",
    "5-year data is a 5-year average, ending in the named year.\n",
    "So the recently released ACS2016-5year actually is from 2012-2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capture/ACS2009_5year/2009_5yr_Summary_FileTemplates.zip already downloaded\n",
      "capture/ACS2009_5year/2009_5yr_Summary_FileTemplates.zip already unzipped\n",
      "capture/ACS2010_5year/2010_5yr_Summary_FileTemplates.zip already downloaded\n",
      "capture/ACS2010_5year/2010_5yr_Summary_FileTemplates.zip already unzipped\n",
      "capture/ACS2011_5year/2011_5yr_Summary_FileTemplates.zip already downloaded\n",
      "capture/ACS2011_5year/2011_5yr_Summary_FileTemplates.zip already unzipped\n",
      "capture/ACS2012_5year/2012_5yr_Summary_FileTemplates.zip already downloaded\n",
      "capture/ACS2012_5year/2012_5yr_Summary_FileTemplates.zip already unzipped\n",
      "capture/ACS2013_5year/2013_5yr_Summary_FileTemplates.zip already downloaded\n",
      "capture/ACS2013_5year/2013_5yr_Summary_FileTemplates.zip already unzipped\n",
      "capture/ACS2014_5year/2014_5yr_Summary_FileTemplates.zip already downloaded\n",
      "capture/ACS2014_5year/2014_5yr_Summary_FileTemplates.zip already unzipped\n",
      "capture/ACS2015_5year/2015_5yr_Summary_FileTemplates.zip already downloaded\n",
      "capture/ACS2015_5year/2015_5yr_Summary_FileTemplates.zip already unzipped\n",
      "capture/ACS2016_5year/2016_5yr_Summary_FileTemplates.zip already downloaded\n",
      "capture/ACS2016_5year/2016_5yr_Summary_FileTemplates.zip already unzipped\n"
     ]
    }
   ],
   "source": [
    "#src = 'https://www2.census.gov/programs-surveys/acs/summary_file/2015/data/2015_1yr_Summary_FileTemplates.zip'\n",
    "#dest = 'capture/ACS2015_1year/2015_1yr_Summary_FileTemplates.zip'\n",
    "#download_file(src, dest)\n",
    "#templates = unzip_file(dest)\n",
    "\n",
    "def download_file_templates(year):\n",
    "    src = 'https://www2.census.gov/programs-surveys/acs/summary_file/{year}/data/{year}_5yr_Summary_FileTemplates.zip'.format(**locals())\n",
    "\n",
    "    # Special-case 2010\n",
    "    src = src.replace('2010_5yr_Summary_File', '2010_5yr_SummaryFile')\n",
    "    \n",
    "    dest = 'capture/ACS{year}_5year/{year}_5yr_Summary_FileTemplates.zip'.format(**locals())\n",
    "    download_file(src, dest)\n",
    "    templates = unzip_file(dest)\n",
    "    \n",
    "for year in range(2009, 2017):\n",
    "    download_file_templates(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls -l capture/ACS2015_1year/2015_1yr_Summary_FileTemplates/Templates | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download ACS2015 5-year data (tract and block group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "process_year=2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capture/ACS2009_5year/Tracts_Block_Groups_Only.zip already exists, skipping\n",
      "capture/ACS2010_5year/Tracts_Block_Groups_Only.zip already exists, skipping\n",
      "capture/ACS2011_5year/Tracts_Block_Groups_Only.tar.gz already exists, skipping\n",
      "capture/ACS2012_5year/Tracts_Block_Groups_Only.tar.gz already exists, skipping\n",
      "capture/ACS2013_5year/Tracts_Block_Groups_Only.tar.gz already exists, skipping\n",
      "capture/ACS2014_5year/Tracts_Block_Groups_Only.tar.gz already exists, skipping\n",
      "capture/ACS2015_5year/Tracts_Block_Groups_Only.tar.gz already exists, skipping\n",
      "capture/ACS2016_5year/Tracts_Block_Groups_Only.tar.gz already exists, skipping\n"
     ]
    }
   ],
   "source": [
    "def download_data(year):\n",
    "    filename = 'Tracts_Block_Groups_Only'\n",
    "    if year < 2011:\n",
    "        filename += '.zip'\n",
    "    else:\n",
    "        filename += '.tar.gz'\n",
    "    src = 'https://www2.census.gov/programs-surveys/acs/summary_file/{year}/data/5_year_entire_sf/{filename}'.format(**locals())\n",
    "    dest = 'capture/ACS{year}_5year/{filename}'.format(**locals())\n",
    "\n",
    "    if os.path.exists(dest):\n",
    "        print '{dest} already exists, skipping'.format(**locals())\n",
    "    else:\n",
    "        try:\n",
    "            os.unlink(filename)\n",
    "        except OSError:\n",
    "            pass\n",
    "        cmd = '/usr/bin/curl'\n",
    "        cmd += \" -H 'User-Agent:Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'\"\n",
    "        cmd += ' {src}'.format(**locals())\n",
    "        cmd += ' >{dest}'.format(**locals())\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(dest))\n",
    "        except OSError:\n",
    "            pass\n",
    "        print cmd\n",
    "        subprocess_check(cmd)\n",
    "        print 'Downloaded to {dest}'.format(**locals())\n",
    "\n",
    "for year in range(2009, 2017):\n",
    "    download_data(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 rsargent rsargent 2806502508 Oct  5 07:52 capture/ACS2009_5year/Tracts_Block_Groups_Only.zip\r\n",
      "-rw-rw-r-- 1 rsargent rsargent 3369803296 Oct  5 07:59 capture/ACS2010_5year/Tracts_Block_Groups_Only.zip\r\n",
      "-rw-rw-r-- 1 rsargent rsargent 3297054880 Oct  5 08:12 capture/ACS2011_5year/Tracts_Block_Groups_Only.tar.gz\r\n",
      "-rw-rw-r-- 1 rsargent rsargent 3651813394 Oct  5 07:33 capture/ACS2012_5year/Tracts_Block_Groups_Only.tar.gz\r\n",
      "-rw-rw-r-- 1 rsargent rsargent 3769295680 Oct  5 07:45 capture/ACS2013_5year/Tracts_Block_Groups_Only.tar.gz\r\n",
      "-rw-rw-r-- 1 rsargent rsargent 3757945352 Oct  5 07:59 capture/ACS2014_5year/Tracts_Block_Groups_Only.tar.gz\r\n",
      "-rw-rw-r-- 1 rsargent rsargent 3747109902 Dec  2  2016 capture/ACS2015_5year/Tracts_Block_Groups_Only.tar.gz\r\n",
      "-rw-rw-r-- 1 rsargent rsargent 3780352044 Feb 14 15:00 capture/ACS2016_5year/Tracts_Block_Groups_Only.tar.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l capture/ACS*/Tracts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !mkdir -p capture/ACS2005_5year\n",
    "# !mv  capture/ACS2005_5year\n",
    "#\n",
    "# !cd capture/ACS2005_5year; tar xvfz Tracts_Block_Groups_Only.tar.gz >/dev/null\n",
    "#\n",
    "# !wget --header=\"User-Agent: Mozilla/5.0 (Windows NT 6.0) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.97 Safari/537.11\" https://www2.census.gov/programs-surveys/acs/summary_file/2015/data/5_year_entire_sf/2015_ACS_Geography_Files.zip\n",
    "#\n",
    "# !mv 2015_ACS_Geography_Files.zip capture/ACS2005_5year\n",
    "# \n",
    "# unzip_file('capture/ACS2005_5year/2015_ACS_Geography_Files.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_geography_data(year, force_regenerate=False):\n",
    "    fname = (\"{year}_ACS_Geography_Files.zip\").format(**locals())\n",
    "    cdir = (\"capture/ACS{year}_5year\").format(**locals())\n",
    "    fpath = (\"{cdir}/{fname}\").format(**locals())\n",
    "    \n",
    "    if os.path.exists(fpath) and not force_regenerate:\n",
    "        print '{fpath} already exists, skipping'.format(**locals())\n",
    "        return\n",
    "    \n",
    "    url_template = \"https://www2.census.gov/programs-surveys/acs/summary_file/{year}/data/5_year_entire_sf/{fname}\"\n",
    "    url = url_template.format(**locals())\n",
    "    !wget --header=\"User-Agent: Mozilla/5.0 (Windows NT 6.0) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.97 Safari/537.11\" $url\n",
    "    \n",
    "    !mv $fname $cdir\n",
    "    unzip_file(fpath)\n",
    "    print \"Downloaded %s to %s\" % (fname,fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-02-28 22:16:08--  https://www2.census.gov/programs-surveys/acs/summary_file/2009/data/5_year_entire_sf/2009_ACS_Geography_Files.zip\n",
      "Resolving www2.census.gov (www2.census.gov)... 23.36.91.141, 2600:1408:7:291::208c, 2600:1408:7:2a5::208c\n",
      "Connecting to www2.census.gov (www2.census.gov)|23.36.91.141|:443... connected.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2018-02-28 22:16:09 ERROR 404: Not Found.\n",
      "\n",
      "mv: cannot stat '2009_ACS_Geography_Files.zip': No such file or directory\n",
      "Unzipping capture/ACS2009_5year/2009_ACS_Geography_Files.zip into capture/ACS2009_5year/2009_ACS_Geography_Files.tmp\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Call to subprocess_check failed with return code 9\nStandard error:\nunzip:  cannot find or open capture/ACS2009_5year/2009_ACS_Geography_Files.zip, capture/ACS2009_5year/2009_ACS_Geography_Files.zip.zip or capture/ACS2009_5year/2009_ACS_Geography_Files.zip.ZIP.\nStandard out:\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-92b0ec7c920e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdownload_geography_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_year\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-501fcfb5e3f8>\u001b[0m in \u001b[0;36mdownload_geography_data\u001b[0;34m(year, force_regenerate)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'mv $fname $cdir'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0munzip_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Downloaded %s to %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36munzip_file\u001b[0;34m(filename)\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36msubprocess_check\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Call to subprocess_check failed with return code 9\nStandard error:\nunzip:  cannot find or open capture/ACS2009_5year/2009_ACS_Geography_Files.zip, capture/ACS2009_5year/2009_ACS_Geography_Files.zip.zip or capture/ACS2009_5year/2009_ACS_Geography_Files.zip.ZIP.\nStandard out:\n"
     ]
    }
   ],
   "source": [
    "download_geography_data(process_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read CSV utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_acs_5year_template(year, seqno):\n",
    "    for template in ['capture/ACS{year}_5year/{year}_5yr_Summary_FileTemplates/Seq{seqno}.xls',\n",
    "                     'capture/ACS{year}_5year/{year}_5yr_Summary_FileTemplates/{year}_5yr_Templates/Seq{seqno}.xls',\n",
    "                     'capture/ACS{year}_5year/{year}_5yr_Summary_FileTemplates/seq/Seq{seqno}.xls',\n",
    "                     'capture/ACS{year}_5year/{year}_5yr_Summary_FileTemplates/templates/Seq{seqno}.xls',\n",
    "                     'capture/ACS{year}_5year/{year}_5yr_Summary_FileTemplates/Seq%04d.xls'%(seqno)]:\n",
    "        path = template.format(**locals())\n",
    "        #print 'Checking for {path}'.format(**locals())\n",
    "        if os.path.exists(path):\n",
    "            return pandas.read_excel(path)\n",
    "    #print 'yo could not find {year}:{seqno}'.format(**locals())\n",
    "    return None\n",
    "\n",
    "def find_acs_5year_data(year, state, seqno):\n",
    "    fname = 'e%d5%s%04d000.txt' % (year, state, seqno)\n",
    "    for template in ['capture/ACS{year}_5year/group2/{fname}',\n",
    "                     'capture/ACS{year}_5year/data/tab4/sumfile/prod/2012thru2016/group2/{fname}',\n",
    "                     'capture/ACS{year}_5year/tab4/sumfile/prod/2010thru2014/group2/{fname}',\n",
    "                     'capture/ACS{year}_5year/tab4/sumfile/prod/2008thru2012/group2/{fname}',\n",
    "                     'capture/ACS{year}_5year/tab4/sumfile/prod/2006thru2010/group2/{fname}']:\n",
    "        path = template.format(**locals())\n",
    "        #print 'Checking for {path}'.format(**locals())\n",
    "        if os.path.exists(path):\n",
    "            return path\n",
    "    print 'Could not find {year}:{seqno} file {fname}'.format(**locals())\n",
    "    return None\n",
    "\n",
    "# Combine template header and data into pandas frame\n",
    "def read_acs_5year_data(year, state, seqno):\n",
    "    header = read_acs_5year_template(year, seqno)\n",
    "    data_fname = find_acs_5year_data(year, state, seqno)\n",
    "    if not data_fname:\n",
    "        return None\n",
    "    else:\n",
    "        data = pandas.read_csv(data_fname,\n",
    "                               index_col=False,\n",
    "                               dtype={'FILEID':numpy.str,\n",
    "                                      'FILETYPE':numpy.str,\n",
    "                                      'STUSAB':numpy.str,\n",
    "                                      'CHARITER':numpy.str,\n",
    "                                      'SEQUENCE':numpy.str,\n",
    "                                      'LOGRECNO':numpy.str},\n",
    "                               header=None,\n",
    "                               names=header.columns.values)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FILEID</th>\n",
       "      <th>FILETYPE</th>\n",
       "      <th>STUSAB</th>\n",
       "      <th>CHARITER</th>\n",
       "      <th>SEQUENCE</th>\n",
       "      <th>LOGRECNO</th>\n",
       "      <th>B07401_001</th>\n",
       "      <th>B07401_002</th>\n",
       "      <th>B07401_003</th>\n",
       "      <th>B07401_004</th>\n",
       "      <th>...</th>\n",
       "      <th>B07409_021</th>\n",
       "      <th>B07409_022</th>\n",
       "      <th>B07409_023</th>\n",
       "      <th>B07409_024</th>\n",
       "      <th>B07409_025</th>\n",
       "      <th>B07409_026</th>\n",
       "      <th>B07409_027</th>\n",
       "      <th>B07409_028</th>\n",
       "      <th>B07409_029</th>\n",
       "      <th>B07409_030</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FILEID</td>\n",
       "      <td>FILETYPE</td>\n",
       "      <td>STUSAB</td>\n",
       "      <td>CHARITER</td>\n",
       "      <td>SEQUENCE</td>\n",
       "      <td>LOGRECNO</td>\n",
       "      <td>Population 1 year and over in the United States</td>\n",
       "      <td>Population 1 year and over in the United States% 1 to 4 years</td>\n",
       "      <td>Population 1 year and over in the United States% 5 to 17 years</td>\n",
       "      <td>Population 1 year and over in the United States% 18 and 19 years</td>\n",
       "      <td>...</td>\n",
       "      <td>Population 25 years and over in the United States% Moved to different county within same state:% High school graduate (includes equivalency)</td>\n",
       "      <td>Population 25 years and over in the United States% Moved to different county within same state:% Some college or associate's degree</td>\n",
       "      <td>Population 25 years and over in the United States% Moved to different county within same state:% Bachelor's degree</td>\n",
       "      <td>Population 25 years and over in the United States% Moved to different county within same state:% Graduate or professional degree</td>\n",
       "      <td>Population 25 years and over in the United States% Moved to different state:</td>\n",
       "      <td>Population 25 years and over in the United States% Moved to different state:% Less than high school graduate</td>\n",
       "      <td>Population 25 years and over in the United States% Moved to different state:% High school graduate (includes equivalency)</td>\n",
       "      <td>Population 25 years and over in the United States% Moved to different state:% Some college or associate's degree</td>\n",
       "      <td>Population 25 years and over in the United States% Moved to different state:% Bachelor's degree</td>\n",
       "      <td>Population 25 years and over in the United States% Moved to different state:% Graduate or professional degree</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  236 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   FILEID  FILETYPE  STUSAB  CHARITER  SEQUENCE  LOGRECNO  \\\n",
       "0  FILEID  FILETYPE  STUSAB  CHARITER  SEQUENCE  LOGRECNO   \n",
       "\n",
       "                                        B07401_001  \\\n",
       "0  Population 1 year and over in the United States   \n",
       "\n",
       "                                                      B07401_002  \\\n",
       "0  Population 1 year and over in the United States% 1 to 4 years   \n",
       "\n",
       "                                                       B07401_003  \\\n",
       "0  Population 1 year and over in the United States% 5 to 17 years   \n",
       "\n",
       "                                                         B07401_004  \\\n",
       "0  Population 1 year and over in the United States% 18 and 19 years   \n",
       "\n",
       "                                                       ...                                                        \\\n",
       "0                                                      ...                                                         \n",
       "\n",
       "                                                                                                                                     B07409_021  \\\n",
       "0  Population 25 years and over in the United States% Moved to different county within same state:% High school graduate (includes equivalency)   \n",
       "\n",
       "                                                                                                                            B07409_022  \\\n",
       "0  Population 25 years and over in the United States% Moved to different county within same state:% Some college or associate's degree   \n",
       "\n",
       "                                                                                                           B07409_023  \\\n",
       "0  Population 25 years and over in the United States% Moved to different county within same state:% Bachelor's degree   \n",
       "\n",
       "                                                                                                                         B07409_024  \\\n",
       "0  Population 25 years and over in the United States% Moved to different county within same state:% Graduate or professional degree   \n",
       "\n",
       "                                                                     B07409_025  \\\n",
       "0  Population 25 years and over in the United States% Moved to different state:   \n",
       "\n",
       "                                                                                                     B07409_026  \\\n",
       "0  Population 25 years and over in the United States% Moved to different state:% Less than high school graduate   \n",
       "\n",
       "                                                                                                                  B07409_027  \\\n",
       "0  Population 25 years and over in the United States% Moved to different state:% High school graduate (includes equivalency)   \n",
       "\n",
       "                                                                                                         B07409_028  \\\n",
       "0  Population 25 years and over in the United States% Moved to different state:% Some college or associate's degree   \n",
       "\n",
       "                                                                                        B07409_029  \\\n",
       "0  Population 25 years and over in the United States% Moved to different state:% Bachelor's degree   \n",
       "\n",
       "                                                                                                      B07409_030  \n",
       "0  Population 25 years and over in the United States% Moved to different state:% Graduate or professional degree  \n",
       "\n",
       "[1 rows x 236 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_acs_5year_template(process_year, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FILEID</th>\n",
       "      <th>FILETYPE</th>\n",
       "      <th>STUSAB</th>\n",
       "      <th>CHARITER</th>\n",
       "      <th>SEQUENCE</th>\n",
       "      <th>LOGRECNO</th>\n",
       "      <th>B07401_001</th>\n",
       "      <th>B07401_002</th>\n",
       "      <th>B07401_003</th>\n",
       "      <th>B07401_004</th>\n",
       "      <th>...</th>\n",
       "      <th>B07409_021</th>\n",
       "      <th>B07409_022</th>\n",
       "      <th>B07409_023</th>\n",
       "      <th>B07409_024</th>\n",
       "      <th>B07409_025</th>\n",
       "      <th>B07409_026</th>\n",
       "      <th>B07409_027</th>\n",
       "      <th>B07409_028</th>\n",
       "      <th>B07409_029</th>\n",
       "      <th>B07409_030</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows  236 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [FILEID, FILETYPE, STUSAB, CHARITER, SEQUENCE, LOGRECNO, B07401_001, B07401_002, B07401_003, B07401_004, B07401_005, B07401_006, B07401_007, B07401_008, B07401_009, B07401_010, B07401_011, B07401_012, B07401_013, B07401_014, B07401_015, B07401_016, B07401_017, B07401_018, B07401_019, B07401_020, B07401_021, B07401_022, B07401_023, B07401_024, B07401_025, B07401_026, B07401_027, B07401_028, B07401_029, B07401_030, B07401_031, B07401_032, B07401_033, B07401_034, B07401_035, B07401_036, B07401_037, B07401_038, B07401_039, B07401_040, B07401_041, B07401_042, B07401_043, B07401_044, B07401_045, B07401_046, B07401_047, B07401_048, B07401_049, B07401_050, B07401_051, B07401_052, B07401_053, B07401_054, B07401_055, B07401_056, B07401_057, B07401_058, B07401_059, B07401_060, B07401_061, B07401_062, B07401_063, B07401_064, B07401_065, B07401_066, B07401_067, B07401_068, B07401_069, B07401_070, B07401_071, B07401_072, B07401_073, B07401_074, B07401_075, B07401_076, B07401_077, B07401_078, B07401_079, B07401_080, B07402_001, B07402_002, B07402_003, B07402_004, B07402_005, B07403_001, B07403_002, B07403_003, B07403_004, B07403_005, B07403_006, B07403_007, B07403_008, B07403_009, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 236 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_acs_5year_data(process_year,'pa', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write ACSYYYY 5-year description.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check if dataset is already defined.  If not, define it as a map, otherwise, leave it alone\n",
    "try:\n",
    "  dataset\n",
    "except NameError:\n",
    "  dataset = {}\n",
    "\n",
    "column_dir = 'columncache'\n",
    "\n",
    "def write_acs_5year_description(year, force_regenerate=False):\n",
    "    dataset[year] = 'acs{year}_5year_tract2010'.format(**locals())\n",
    "    description_path = column_dir + '/' + dataset[year] + '/description.html'\n",
    "\n",
    "    if os.path.exists(description_path) and not force_regenerate:\n",
    "        print '{description_path} already exists, skipping'.format(**locals())\n",
    "        return\n",
    "\n",
    "    table_rows = []\n",
    "\n",
    "    for seqno in range(1, 1000):\n",
    "        template = read_acs_5year_template(year, seqno)\n",
    "        if template is None:\n",
    "            break\n",
    "        for col in range(6, template.shape[1]):\n",
    "            colname = template.columns.values[col]\n",
    "            description = template.iloc[0,col]\n",
    "            try:\n",
    "                description = description.replace(':', '')\n",
    "                description = re.sub(r'\\s*%\\s*', ' &mdash; ', description)\n",
    "            except:\n",
    "                print \"%d:%d col %d description = '%s', using '%s' instead\" % (year, seqno, col, description,colname)\n",
    "                description = colname\n",
    "            # format can't handle array reference, so put dataset[year] in a flat variable for the format to work\n",
    "            dataset_var = dataset[year]\n",
    "            table_rows.append(u'<tr><td>{dataset_var}.{colname}</td><td>{description}</td></tr>\\n'.format(**locals()))\n",
    "\n",
    "    html = '<table>' + ''.join(table_rows) + '</table>'\n",
    "\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(description_path))\n",
    "    except:\n",
    "        pass\n",
    "    open(description_path, 'w').write(html.encode('utf8'))\n",
    "    print 'Wrote %d column names and descriptions to %s' % (len(table_rows), description_path)\n",
    "    print 'Check it out at http://dotmaptiles.createlab.org/data/acs{year}_5year_tract2010'.format(**locals())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009:57 col 128 description = 'nan', using 'B19080_005' instead\n",
      "2009:57 col 134 description = 'nan', using 'B19081_006' instead\n",
      "2009:57 col 140 description = 'nan', using 'B19082_006' instead\n",
      "2009:96 col 49 description = 'nan', using 'B25005_002' instead\n",
      "2009:105 col 6 description = 'nan', using 'B98001_001' instead\n",
      "2009:105 col 7 description = 'nan', using 'B98001_002' instead\n",
      "2009:105 col 8 description = 'nan', using 'B98002_001' instead\n",
      "2009:105 col 9 description = 'nan', using 'B98002_002' instead\n",
      "Wrote 21207 column names and descriptions to columncache/acs2009_5year_tract2010/description.html\n",
      "Check it out at http://dotmaptiles.createlab.org/data/acs2009_5year_tract2010\n"
     ]
    }
   ],
   "source": [
    "write_acs_5year_description(process_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create ACS2015 block-level population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read 2010 block geoids and 2010 block populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block_populations has 308745538 total people\n"
     ]
    }
   ],
   "source": [
    "block_populations = numpy.load('columncache/census2010_block2010/p001001.numpy')\n",
    "print 'block_populations has', sum(block_populations), 'total people'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11078297 blocks\n"
     ]
    }
   ],
   "source": [
    "# block_geoids_2010 = [row[0] for row in query_psql(\"SELECT geoid2010 FROM sf1_2010_block_p001 order by blockidx2010\")]\n",
    "block_geoids_2010 = json.load(open('block_geoids_2010.json'))\n",
    "print 'There are', len(block_geoids_2010), 'blocks'\n",
    "\n",
    "assert(len(block_geoids_2010) + 1 == len(block_populations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute 2010 population by tract and block indices from tract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 73057 tracts\n",
      "tract_populations has 308745538 people\n"
     ]
    }
   ],
   "source": [
    "tract_populations = {}\n",
    "tract_block_indexes = {}\n",
    "\n",
    "for block_index_minus_one, block_geoid in enumerate(block_geoids_2010):\n",
    "    block_index = block_index_minus_one + 1\n",
    "    tract_name = block_geoid[0:11] # SSCCCTTTTTT\n",
    "    if tract_name not in tract_populations:\n",
    "        tract_populations[tract_name] = 0\n",
    "        tract_block_indexes[tract_name] = []\n",
    "    tract_populations[tract_name] += block_populations[block_index]\n",
    "    tract_block_indexes[tract_name].append(block_index)\n",
    "\n",
    "print 'There are', len(tract_populations), 'tracts'\n",
    "print 'tract_populations has', sum(tract_populations.values()), 'people'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map tract identifiers to LOGRECNO using geography file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tract_to_logrecno_year=None\n",
    "tract_to_logrecno = {}\n",
    "\n",
    "def compute_tract_to_logrecno(state, year):\n",
    "    global tract_to_logrecno_year\n",
    "    tract_to_logrecno_year=year\n",
    "    \n",
    "    # In the case of 2009, use the 2010 geography files\n",
    "    geo_file_year = year\n",
    "    if(geo_file_year == 2009):\n",
    "        geo_file_year = 2010\n",
    "        \n",
    "    for template in [\"capture/ACS{geo_file_year}_5year/{geo_file_year}_ACS_Geography_Files/g{geo_file_year}5{state}.csv\",\n",
    "                     \"capture/ACS{geo_file_year}_5year/{geo_file_year}_ACS_Geography_Files/geo/g{geo_file_year}5{state}.csv\",\n",
    "                     \"capture/ACS{geo_file_year}_5year/{geo_file_year}_ACS_Geography_Files/tab4/sumfile/prod/2009thru2013/geo/g{geo_file_year}5{state}.csv\",\n",
    "                     \"capture/ACS{geo_file_year}_5year/{geo_file_year}_ACS_Geography_Files/geog/g{geo_file_year}5{state}.csv\"]:\n",
    "        csv_path = template.format(**locals())\n",
    "        if os.path.exists(csv_path):\n",
    "            geography = pandas.read_csv(csv_path,\n",
    "                                        dtype=numpy.str,\n",
    "                                        index_col=False,\n",
    "                                        header=None,\n",
    "                                        keep_default_na=False,\n",
    "                                        na_values=[])\n",
    "\n",
    "            nrows = geography.shape[0]\n",
    "            print 'State {state} has {nrows} geography rows'.format(**locals())\n",
    "    \n",
    "            ntracts = 0\n",
    "            tract_to_logrecno[state] = {}\n",
    "    \n",
    "            for r in range(0, geography.shape[0]):\n",
    "                aggregation_level = geography.iloc[r, 2]\n",
    "                if aggregation_level == '140': # census tract\n",
    "                    tract_identifier = geography.iloc[r, 48][7:]\n",
    "                    logrecno = geography.iloc[r, 4]\n",
    "                    tract_to_logrecno[state][tract_identifier] = logrecno\n",
    "    \n",
    "            print 'Found %d tracts for state %s in year %d' % (len(tract_to_logrecno[state]), state, year)\n",
    "            return\n",
    "\n",
    "    print '{csv_path} missing, call download_geography_data({geo_file_year}), skipping {state},{geo_file_year}'.format(**locals())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State ak has 4193 geography rows\n",
      "Found 167 tracts for state ak in year 2009\n",
      "State al has 11466 geography rows\n",
      "Found 1181 tracts for state al in year 2009\n",
      "State ar has 12182 geography rows\n",
      "Found 686 tracts for state ar in year 2009\n",
      "State az has 11173 geography rows\n",
      "Found 1526 tracts for state az in year 2009\n",
      "State ca has 52857 geography rows\n",
      "Found 8057 tracts for state ca in year 2009\n",
      "State co has 10108 geography rows\n",
      "Found 1249 tracts for state co in year 2009\n",
      "State ct has 6401 geography rows\n",
      "Found 833 tracts for state ct in year 2009\n",
      "State dc has 857 geography rows\n",
      "Found 179 tracts for state dc in year 2009\n",
      "State de has 1714 geography rows\n",
      "Found 218 tracts for state de in year 2009\n",
      "State fl has 28273 geography rows\n",
      "Found 4245 tracts for state fl in year 2009\n",
      "State ga has 16360 geography rows\n",
      "Found 1969 tracts for state ga in year 2009\n",
      "State hi has 3120 geography rows\n",
      "Found 351 tracts for state hi in year 2009\n",
      "State ia has 16074 geography rows\n",
      "Found 825 tracts for state ia in year 2009\n",
      "State id has 3694 geography rows\n",
      "Found 298 tracts for state id in year 2009\n",
      "State il has 33251 geography rows\n",
      "Found 3123 tracts for state il in year 2009\n",
      "State in has 16555 geography rows\n",
      "Found 1511 tracts for state in in year 2009\n",
      "State ks has 12884 geography rows\n",
      "Found 770 tracts for state ks in year 2009\n",
      "State ky has 10596 geography rows\n",
      "Found 1115 tracts for state ky in year 2009\n",
      "State la has 12587 geography rows\n",
      "Found 1148 tracts for state la in year 2009\n",
      "State ma has 11869 geography rows\n",
      "Found 1478 tracts for state ma in year 2009\n",
      "State md has 11272 geography rows\n",
      "Found 1406 tracts for state md in year 2009\n",
      "State me has 4999 geography rows\n",
      "Found 358 tracts for state me in year 2009\n",
      "State mi has 23401 geography rows\n",
      "Found 2813 tracts for state mi in year 2009\n",
      "State mn has 19659 geography rows\n",
      "Found 1338 tracts for state mn in year 2009\n",
      "State mo has 19801 geography rows\n",
      "Found 1393 tracts for state mo in year 2009\n",
      "State ms has 9187 geography rows\n",
      "Found 664 tracts for state ms in year 2009\n",
      "State mt has 4714 geography rows\n",
      "Found 271 tracts for state mt in year 2009\n",
      "State nc has 21777 geography rows\n",
      "Found 2195 tracts for state nc in year 2009\n",
      "State nd has 8092 geography rows\n",
      "Found 205 tracts for state nd in year 2009\n",
      "State ne has 9796 geography rows\n",
      "Found 532 tracts for state ne in year 2009\n",
      "State nh has 3568 geography rows\n",
      "Found 295 tracts for state nh in year 2009\n",
      "State nj has 14388 geography rows\n",
      "Found 2010 tracts for state nj in year 2009\n",
      "State nm has 5991 geography rows\n",
      "Found 499 tracts for state nm in year 2009\n",
      "State nv has 4695 geography rows\n",
      "Found 687 tracts for state nv in year 2009\n",
      "State ny has 34946 geography rows\n",
      "Found 4919 tracts for state ny in year 2009\n",
      "State oh has 28188 geography rows\n",
      "Found 2952 tracts for state oh in year 2009\n",
      "State ok has 11933 geography rows\n",
      "Found 1046 tracts for state ok in year 2009\n",
      "State or has 7807 geography rows\n",
      "Found 834 tracts for state or in year 2009\n",
      "State pa has 29951 geography rows\n",
      "Found 3218 tracts for state pa in year 2009\n",
      "State ri has 1972 geography rows\n",
      "Found 244 tracts for state ri in year 2009\n",
      "State sc has 9356 geography rows\n",
      "Found 1103 tracts for state sc in year 2009\n",
      "State sd has 7045 geography rows\n",
      "Found 222 tracts for state sd in year 2009\n",
      "State tn has 15296 geography rows\n",
      "Found 1497 tracts for state tn in year 2009\n",
      "State tx has 42248 geography rows\n",
      "Found 5265 tracts for state tx in year 2009\n",
      "State ut has 5563 geography rows\n",
      "Found 588 tracts for state ut in year 2009\n",
      "State va has 15731 geography rows\n",
      "Found 1907 tracts for state va in year 2009\n",
      "State vt has 2856 geography rows\n",
      "Found 184 tracts for state vt in year 2009\n",
      "State wa has 12985 geography rows\n",
      "Found 1458 tracts for state wa in year 2009\n",
      "State wi has 18050 geography rows\n",
      "Found 1409 tracts for state wi in year 2009\n",
      "State wv has 6351 geography rows\n",
      "Found 484 tracts for state wv in year 2009\n",
      "State wy has 2214 geography rows\n",
      "Found 132 tracts for state wy in year 2009\n"
     ]
    }
   ],
   "source": [
    "for state in state_names:\n",
    "    compute_tract_to_logrecno(state, process_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate and write columns for data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AW 2/15/18: Randy believes this version is older than the one below.  I discovered this after putting in some work to generalize it to a \n",
    "# parameterized year.  The current version doesn't work.\n",
    "# TODO: can we do this with a data frame then write out columns?\n",
    "\n",
    "# def interpolate_acs_file(year, state, seq):\n",
    "#     print 'Reading %s:%d for %d' % (state, seq, year)\n",
    "#     data = read_acs_5year_data(year, state, seq)\n",
    "\n",
    "#     print 'Mapping locrecno to row'\n",
    "#     logrecnos = data['LOGRECNO']\n",
    "\n",
    "#     logrecno_to_row = {}\n",
    "\n",
    "#     for r, logrecno in enumerate(logrecnos):\n",
    "#         logrecno_to_row[logrecno] = r\n",
    "    \n",
    "#     col_names = data.columns.values[6:]\n",
    "#     print 'Iterating across %d columns' % len(col_names)\n",
    "#     for col_name in col_names:\n",
    "#         input_col = data[col_name]\n",
    "#         output_col_path = column_dir + '/' + dataset + '/' + col_name + '.float32'\n",
    "#         if os.path.exists(output_col_path):\n",
    "#             print '%s already exists, skipping' % output_col_path\n",
    "#             continue\n",
    "\n",
    "#         output_col = numpy.zeros(block_populations.size, dtype=numpy.float32)\n",
    "\n",
    "#         for tract in sorted(tract_to_logrecno[state].keys()):\n",
    "#             input_pop = input_col[logrecno_to_row[tract_to_logrecno[state][tract]]]\n",
    "#             if not isinstance(input_pop, numbers.Number):\n",
    "#                 if input_pop == '.':\n",
    "#                     input_pop = 0\n",
    "#                 else:\n",
    "#                     try:\n",
    "#                         input_pop = float(input_pop)\n",
    "#                     except:\n",
    "#                         print 'That population is'\n",
    "#                         print input_pop\n",
    "#                         print type(input_pop)\n",
    "#                         print '>%s<' % input_pop\n",
    "#                         input_pop = 0\n",
    "#             if not tract in tract_block_indexes:\n",
    "#                 print 'missing tract {tract} from tract_block_indexes'.format(**locals())\n",
    "#             else:\n",
    "#                 for block_index in tract_block_indexes[tract]:\n",
    "#                     if block_populations[block_index]:\n",
    "#                         output_col[block_index] = input_pop * float(block_populations[block_index]) / tract_populations[tract]\n",
    "            \n",
    "#         output_col.tofile(output_col_path + '.tmp')\n",
    "#         os.rename(output_col_path + '.tmp', output_col_path)\n",
    "#         print 'Created %s' % output_col_path\n",
    "\n",
    "# for seq in range(97, 2000):\n",
    "#     interpolate_acs_file(year, 'pa', seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: can we do this with a data frame then write out columns?\n",
    "\n",
    "def interpolate_acs_file(year, seq):\n",
    "    global tract_to_logrecno_year\n",
    "    sys.stdout.write(\"interpolating %d:%d\\n\" % (year, seq))\n",
    "    \n",
    "   # Make sure dataset[year] already exists.  If not, prompt to run write_acs_5year_description(year)\n",
    "    try:\n",
    "        dataset[year]\n",
    "    except:\n",
    "        print \"dataset[%d] not defined.  Call write_acs_5year_description(%d) first.\" % (year, year)\n",
    "        return None\n",
    "\n",
    "\n",
    "    # Make sure tract_to_logrecno_year already exists and matches year.  If not, prompt to run compute_tract_to_logrecno(state, %d)\n",
    "    try:\n",
    "        tract_to_logrecno_year\n",
    "    except:\n",
    "        print \"tract_to_logrecno_year not defined.  Call compute_tract_to_logrecno(state, %d) first.\" % (year)\n",
    "        return None\n",
    "\n",
    "    if tract_to_logrecno_year != year:\n",
    "        print \"tract_to_logrecno_year doesn't match.  Call compute_tract_to_logrecno(state, %d) first.\" % (year)\n",
    "        return None\n",
    "    \n",
    "    output_cols = {}\n",
    "    missing_tracts = {}\n",
    "    num_nans=0\n",
    "    for state in state_names:\n",
    "        data = read_acs_5year_data(year, state, seq)\n",
    "    \n",
    "        logrecnos = data['LOGRECNO']\n",
    "\n",
    "        logrecno_to_row = {}\n",
    "\n",
    "        col_names = data.columns.values[6:]\n",
    "        sys.stdout.write('%s:%d %d has %d columns\\n' % (state, seq, year, len(col_names)))\n",
    "        assert len(col_names) < 500   # sanity check to avoid demanding too much RAM on hal15\n",
    "\n",
    "        if state == state_names[0]:\n",
    "            missing = 0\n",
    "            # First state.  Now that we know the col names, let's see if the output files all already exist\n",
    "            for col_name in col_names:\n",
    "                output_col_path = column_dir + '/' + dataset[year] + '/' + col_name + '.float32'\n",
    "                if not os.path.exists(output_col_path):\n",
    "                    missing += 1\n",
    "            if missing == 0:\n",
    "                sys.stdout.write(\"All %d columns for sequence %d already exist, skipping\\n\" % (len(col_names), seq))\n",
    "                return\n",
    "        \n",
    "        for r, logrecno in enumerate(logrecnos):\n",
    "            logrecno_to_row[logrecno] = r\n",
    "    \n",
    "        for col_name in col_names:\n",
    "            input_col = data[col_name]\n",
    "                \n",
    "            if not col_name in output_cols:\n",
    "                output_cols[col_name] = numpy.zeros(block_populations.size, dtype=numpy.float32)\n",
    "            output_col = output_cols[col_name]\n",
    "\n",
    "            for tract in sorted(tract_to_logrecno[state].keys()):\n",
    "                input_pop = input_col[logrecno_to_row[tract_to_logrecno[state][tract]]]\n",
    "                if not isinstance(input_pop, numbers.Number):\n",
    "                    if input_pop == '.':\n",
    "                        input_pop = 0\n",
    "                    else:\n",
    "                        try:\n",
    "                            input_pop = float(input_pop)\n",
    "                        except:\n",
    "                            print 'That population is'\n",
    "                            print input_pop\n",
    "                            print type(input_pop)\n",
    "                            print '>%s<' % input_pop\n",
    "                            input_pop = 0\n",
    "                            \n",
    "                if math.isnan(input_pop):\n",
    "                    #sys.stdout.write('Warning, %s:%d Tract %s is nan\\n' % (state, seq, tract))\n",
    "                    num_nans=num_nans+1\n",
    "\n",
    "                if not tract in tract_block_indexes:\n",
    "                    missing_tracts[tract] = True\n",
    "                else:\n",
    "                    for block_index in tract_block_indexes[tract]:\n",
    "                        if block_populations[block_index]:\n",
    "                            output_col[block_index] = input_pop * float(block_populations[block_index]) / tract_populations[tract]\n",
    "            \n",
    "    sys.stdout.write('Seq %d missing tracts: %s\\n' % (seq, sorted(missing_tracts.keys())))\n",
    "        \n",
    "    if num_nans>0:\n",
    "        sys.stdout.write('Seq %d contains %d nans' % (seq,num_nans))\n",
    "        \n",
    "    for col_name in sorted(output_cols.keys()):\n",
    "        output_col_path = column_dir + '/' + dataset[year] + '/' + col_name + '.float32'\n",
    "        output_cols[col_name].tofile(output_col_path + '.tmp')\n",
    "        os.rename(output_col_path + '.tmp', output_col_path)\n",
    "        sys.stdout.write('Created %s with sum %f\\n' % (output_col_path, output_cols[col_name].sum()))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = read_acs_5year_data(2009, 'ak', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'B07401_001', u'B07401_002', u'B07401_003', u'B07401_004',\n",
       "       u'B07401_005', u'B07401_006', u'B07401_007', u'B07401_008',\n",
       "       u'B07401_009', u'B07401_010', u'B07401_011', u'B07401_012',\n",
       "       u'B07401_013', u'B07401_014', u'B07401_015', u'B07401_016',\n",
       "       u'B07401_017', u'B07401_018', u'B07401_019', u'B07401_020',\n",
       "       u'B07401_021', u'B07401_022', u'B07401_023', u'B07401_024',\n",
       "       u'B07401_025', u'B07401_026', u'B07401_027', u'B07401_028',\n",
       "       u'B07401_029', u'B07401_030', u'B07401_031', u'B07401_032',\n",
       "       u'B07401_033', u'B07401_034', u'B07401_035', u'B07401_036',\n",
       "       u'B07401_037', u'B07401_038', u'B07401_039', u'B07401_040',\n",
       "       u'B07401_041', u'B07401_042', u'B07401_043', u'B07401_044',\n",
       "       u'B07401_045', u'B07401_046', u'B07401_047', u'B07401_048',\n",
       "       u'B07401_049', u'B07401_050', u'B07401_051', u'B07401_052',\n",
       "       u'B07401_053', u'B07401_054', u'B07401_055', u'B07401_056',\n",
       "       u'B07401_057', u'B07401_058', u'B07401_059', u'B07401_060',\n",
       "       u'B07401_061', u'B07401_062', u'B07401_063', u'B07401_064',\n",
       "       u'B07401_065', u'B07401_066', u'B07401_067', u'B07401_068',\n",
       "       u'B07401_069', u'B07401_070', u'B07401_071', u'B07401_072',\n",
       "       u'B07401_073', u'B07401_074', u'B07401_075', u'B07401_076',\n",
       "       u'B07401_077', u'B07401_078', u'B07401_079', u'B07401_080',\n",
       "       u'B07402_001', u'B07402_002', u'B07402_003', u'B07402_004',\n",
       "       u'B07402_005', u'B07403_001', u'B07403_002', u'B07403_003',\n",
       "       u'B07403_004', u'B07403_005', u'B07403_006', u'B07403_007',\n",
       "       u'B07403_008', u'B07403_009', u'B07403_010', u'B07403_011',\n",
       "       u'B07403_012', u'B07403_013', u'B07403_014', u'B07403_015',\n",
       "       u'B07404A_001', u'B07404A_002', u'B07404A_003', u'B07404A_004',\n",
       "       u'B07404A_005', u'B07404B_001', u'B07404B_002', u'B07404B_003',\n",
       "       u'B07404B_004', u'B07404B_005', u'B07404C_001', u'B07404C_002',\n",
       "       u'B07404C_003', u'B07404C_004', u'B07404C_005', u'B07404D_001',\n",
       "       u'B07404D_002', u'B07404D_003', u'B07404D_004', u'B07404D_005',\n",
       "       u'B07404E_001', u'B07404E_002', u'B07404E_003', u'B07404E_004',\n",
       "       u'B07404E_005', u'B07404F_001', u'B07404F_002', u'B07404F_003',\n",
       "       u'B07404F_004', u'B07404F_005', u'B07404G_001', u'B07404G_002',\n",
       "       u'B07404G_003', u'B07404G_004', u'B07404G_005', u'B07404H_001',\n",
       "       u'B07404H_002', u'B07404H_003', u'B07404H_004', u'B07404H_005',\n",
       "       u'B07404I_001', u'B07404I_002', u'B07404I_003', u'B07404I_004',\n",
       "       u'B07404I_005', u'B07407_001', u'B07407_002', u'B07407_003',\n",
       "       u'B07407_004', u'B07407_005', u'B07407_006', u'B07407_007',\n",
       "       u'B07407_008', u'B07407_009', u'B07407_010', u'B07407_011',\n",
       "       u'B07407_012', u'B07407_013', u'B07407_014', u'B07407_015',\n",
       "       u'B07407_016', u'B07407_017', u'B07407_018', u'B07407_019',\n",
       "       u'B07407_020', u'B07407_021', u'B07407_022', u'B07407_023',\n",
       "       u'B07407_024', u'B07407_025', u'B07408_001', u'B07408_002',\n",
       "       u'B07408_003', u'B07408_004', u'B07408_005', u'B07408_006',\n",
       "       u'B07408_007', u'B07408_008', u'B07408_009', u'B07408_010',\n",
       "       u'B07408_011', u'B07408_012', u'B07408_013', u'B07408_014',\n",
       "       u'B07408_015', u'B07408_016', u'B07408_017', u'B07408_018',\n",
       "       u'B07408_019', u'B07408_020', u'B07408_021', u'B07408_022',\n",
       "       u'B07408_023', u'B07408_024', u'B07408_025', u'B07408_026',\n",
       "       u'B07408_027', u'B07408_028', u'B07408_029', u'B07408_030',\n",
       "       u'B07409_001', u'B07409_002', u'B07409_003', u'B07409_004',\n",
       "       u'B07409_005', u'B07409_006', u'B07409_007', u'B07409_008',\n",
       "       u'B07409_009', u'B07409_010', u'B07409_011', u'B07409_012',\n",
       "       u'B07409_013', u'B07409_014', u'B07409_015', u'B07409_016',\n",
       "       u'B07409_017', u'B07409_018', u'B07409_019', u'B07409_020',\n",
       "       u'B07409_021', u'B07409_022', u'B07409_023', u'B07409_024',\n",
       "       u'B07409_025', u'B07409_026', u'B07409_027', u'B07409_028',\n",
       "       u'B07409_029', u'B07409_030'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logrecnos = data['LOGRECNO']\n",
    "col_names = data.columns.values[6:]\n",
    "col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: LOGRECNO, dtype: object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logrecnos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logrecno_to_row' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-40ddcb169e92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtract\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtract_to_logrecno\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ak'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"%s %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtract_to_logrecno\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ak'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtract\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogrecno_to_row\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtract_to_logrecno\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ak'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtract\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'logrecno_to_row' is not defined"
     ]
    }
   ],
   "source": [
    "for tract in sorted(tract_to_logrecno['ak'].keys()):\n",
    "    print \"%s %s\" % (tract_to_logrecno['ak'][tract], logrecno_to_row[tract_to_logrecno['ak'][tract]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interpolating 2009:1\n",
      "ak:1 2009 has 230 columns\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'0001037'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-2a1197a641c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minterpolate_acs_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_year\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-1be0d8d8351e>\u001b[0m in \u001b[0;36minterpolate_acs_file\u001b[0;34m(year, seq)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtract\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtract_to_logrecno\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0minput_pop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_col\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlogrecno_to_row\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtract_to_logrecno\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtract\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_pop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0minput_pop\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '0001037'"
     ]
    }
   ],
   "source": [
    "interpolate_acs_file(process_year, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interpolating 2010:1\n",
      "interpolating 2010:3\n",
      "interpolating 2010:2\n",
      "interpolating 2010:4\n",
      "ak:1 2010 has 230 columns\n",
      "All 230 columns for sequence 1 already exist, skipping\n",
      "ak:3 2010 has 237 columns\n",
      "interpolating 2010:5\n",
      "ak:5 2010 has 175 columns\n",
      "ak:2 2010 has 95 columns\n",
      "ak:4 2010 has 217 columns\n",
      "al:2 2010 has 95 columns\n",
      "al:5 2010 has 175 columns\n",
      "al:4 2010 has 217 columns\n",
      "al:3 2010 has 237 columns\n",
      "ar:2 2010 has 95 columns\n",
      "az:2 2010 has 95 columns\n"
     ]
    }
   ],
   "source": [
    "# 4 seems conservative on a 64GB machine\n",
    "pool = SimpleProcessPoolExecutor(4)\n",
    "\n",
    "for seq in range(1, 1000):\n",
    "    pool.submit(interpolate_acs_file, process_year, seq)\n",
    "\n",
    "pool.shutdown()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2016"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for capture/ACS2015_5year/group2/e20155ak0001000.txt\n"
     ]
    }
   ],
   "source": [
    "data = read_acs_5year_data(2015, 'ak', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logrecnos = data['LOGRECNO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0000617'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tract_to_logrecno['ak']['02198000300']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(logrecnos)):\n",
    "    if(logrecnos[i]=='0000617'):\n",
    "        print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'B00001_001', u'B00002_001'], dtype=object)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names = data.columns.values[6:]\n",
    "col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(tract_block_indexes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls -l columncache/acs2015_5year_tract2010/B08006_002.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=numpy.memmap('columncache/acs2015_5year_tract2010/B00001_001.float32', dtype=numpy.float32, mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memmap(nan, dtype=float32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memmap([ 0.        ,  7.81642246,  0.        , ...,  1.24807394,\n",
       "        0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
